## Funções Composicionais e a Mitigação da Maldição da Dimensionalidade

### Introdução
Em capítulos anteriores, foram estabelecidas taxas de convergência para a aproximação de funções $f: [0,1]^d \rightarrow \mathbb{R}$ por redes neurais. O Teorema 7.7, por exemplo, fornece um limite de erro de $O(N^{-(k+s)/d})$ em termos do tamanho da rede $N$, onde $k$ e $s$ descrevem a suavidade de $f$ [^1]. No entanto, essa dependência exponencial na dimensão $d$ é conhecida como a "maldição da dimensionalidade" [^1]. Este capítulo explora cenários onde essa maldição pode ser mitigada, focando em funções com uma *estrutura composicional* específica [^1]. Funções composicionais são construídas através da composição e combinações lineares de subfunções simples de baixa dimensão [^1].

### Conceitos Fundamentais
Uma das maneiras de atenuar a maldição da dimensionalidade é considerar funções com uma estrutura composicional específica [^1].  Concretamente, essas funções são construídas por composições e combinações lineares de subfunções simples de baixa dimensão. Nestes casos, a maldição da dimensionalidade está presente, mas apenas através da dimensão de entrada das subfunções [^1].

Para formalizar essa ideia, considere um grafo acíclico direcionado $G$ com $M$ vértices $\eta_1, ..., \eta_M$ que satisfazem as seguintes propriedades [^1]:

*   Exatamente $d$ vértices, $\eta_1, ..., \eta_d$, não possuem arestas de entrada.
*   Cada vértice possui no máximo $m \in \mathbb{N}$ arestas de entrada.
*   Exatamente um vértice, $\eta_M$, não possui arestas de saída.

Com cada vértice $\eta_j$ para $j > d$, associamos uma função $f_j : \mathbb{R}^{d_j} \rightarrow \mathbb{R}$ [^1]. Aqui, $d_j$ denota a cardinalidade do conjunto $S_j$, que é definido como o conjunto de índices $i$ correspondentes aos vértices $\eta_i$ para os quais existe uma aresta de $\eta_i$ para $\eta_j$ [^1]. Sem perda de generalidade, assumimos que $m > d_j = |S_j| \geq 1$ para todo $j > d$ [^1]. Finalmente, definimos [^1]:

$$F_j := x_j \quad \text{para todo } j \leq d \qquad (8.2.1a)$$

$$F_j := f_j((F_i)_{i \in S_j}) \quad \text{para todo } j > d. \qquad (8.2.1b)$$

Então, $F_M(x_1, ..., x_d)$ é uma função de $\mathbb{R}^d \rightarrow \mathbb{R}$ [^1]. Assumindo que

$$||f_j||_{C^{k,s}(\mathbb{R}^{d_j})} \leq 1 \quad \text{para todo } j = d+1, ..., M, \qquad (8.2.2)$$

denotamos o conjunto de todas as funções do tipo $F_M$ por $F^{k,s}(m, d, M)$ [^1]. A Figura 8.1 (presente no texto original) mostra possíveis grafos de tais funções [^1].

Claramente, para $s = 0$, $F^{k,0}(m, d, M) \subseteq C^k(\mathbb{R}^d)$, já que a composição de funções em $C^k$ pertence novamente a $C^k$ [^1]. Uma aplicação direta do Teorema 7.7 permite aproximar $F_1 \in F^k(m, d, M)$ com uma rede neural de tamanho $O(N \log(N))$ e erro $O(N^{-k/d})$ [^1]. Como cada $f_j$ depende apenas de $m$ variáveis, intuitivamente esperamos uma convergência de erro do tipo $O(N^{-k/m})$ com a constante dependendo de alguma forma do número $M$ de vértices [^1].

O Proposition 8.5 quantifica essa intuição, demonstrando que é possível construir uma rede neural ReLU $\widehat{F_M}$ que aproxima $F_M \in F^{k,s}(m, d, M)$ com um erro de aproximação de $N^{-\frac{k+s}{m}}$, onde $N$ representa o tamanho da rede [^1]. O tamanho da rede e a profundidade crescem logaritmicamente com $N$ [^1].

**Proposition 8.5.** Seja $k, m, d, M \in \mathbb{N}$ e $s > 0$ [^1]. Seja $F_M \in F^{k,s}(m, d, M)$ [^1]. Então existe uma constante $C = C(m, k + s, M)$ tal que para todo $N \in \mathbb{N}$ existe uma rede neural ReLU $\widehat{F_M}$ tal que [^1]:

$$size(\widehat{F_M}) \leq CN \log(N), \quad depth(\widehat{F_M}) \leq C \log(N)$$

$$ \sup_{x \in [0,1]^d} |F_M(x) - \widehat{F_M}(x)| \leq N^{-\frac{k+s}{m}}$$\n

A prova deste resultado envolve a construção de aproximações $\hat{f}_j$ para cada $f_j$ e a propagação dessas aproximações através da estrutura composicional [^1].

### Conclusão
A estrutura composicional de funções oferece uma maneira de mitigar a maldição da dimensionalidade [^1]. Ao restringir a complexidade das interações entre variáveis, é possível alcançar taxas de convergência que dependem da dimensão das subfunções elementares, em vez da dimensão total do espaço de entrada [^1]. Isso é particularmente relevante em aplicações onde as funções de interesse exibem essa propriedade, como em redes de sensores ou na modelagem de fenômenos físicos com interações locais [^1].

### Referências
[^1]: Capítulo 8 do texto fornecido.
<!-- END -->