## Mitigation Strategies for the Curse of Dimensionality via Fourier Domain Constraints, Compositional Structures, and Submanifolds

### Introdução
Como vimos anteriormente, a aproximação de funções em espaços de alta dimensão enfrenta o desafio da **maldição da dimensionalidade**, onde a complexidade computacional e a necessidade de dados crescem exponencialmente com a dimensão do espaço [^1]. Este capítulo explora estratégias para mitigar essa maldição, focando em restringir o comportamento das funções no domínio de Fourier, considerando funções com estruturas composicionais específicas e aproximando funções em subvariedades de dimensões inferiores [^1]. Essas abordagens permitem taxas de aproximação independentes da dimensão, embora potencialmente mais lentas [^1].

### Conceitos Fundamentais

#### Restrição no Domínio de Fourier: A Classe de Barron
Uma abordagem para mitigar a maldição da dimensionalidade envolve restringir o comportamento das funções no domínio de Fourier. A **classe de Barron** [^1, 10] representa um conjunto de funções que podem ser aproximadas por redes neurais sem sofrer da maldição da dimensionalidade. Essa classe é caracterizada por um tipo específico de variação limitada.

Para definir a classe de Barron, considere $f \in L^1(\mathbb{R}^d)$ e sua transformada de Fourier:
$$\
\hat{f}(w) := \int_{\mathbb{R}^d} f(x) e^{-2\pi i w \cdot x} dx
$$
A classe de Barron $\Gamma_C$ é definida como:
$$\
\Gamma_C := \left\{ f \in L^1(\mathbb{R}^d) \mid \int_{\mathbb{R}^d} |2\pi \xi| |\hat{f}(\xi)| d\xi < C \right\}
$$
onde $C > 0$ é uma constante [^1].

**Teorema 8.1** [^1, 10]: Seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ uma função sigmoidal (ver Definição 3.11) e seja $f \in \Gamma_C$ para algum $C > 0$. Denote por $B_1 := \{x \in \mathbb{R}^d \mid ||x|| \leq 1\}$ a bola unitária. Então, para todo $c > 4C^2$ e todo $N \in \mathbb{N}$, existe uma rede neural $\Phi^f$ com arquitetura $(\sigma; d, N, 1)$ tal que
$$\
\frac{1}{|B_1|} \int_{B_1} |f(x) - \Phi^f(x)| dx < \frac{c}{N}
$$
onde $|B_1|$ é a medida de Lebesgue de $B_1$ [^1].

**Observação 8.2** [^1]: A taxa de aproximação em (8.1.1) pode ser ligeiramente melhorada sob algumas suposições sobre a função de ativação, como potências da ReLU [^212].

O teorema 8.1 demonstra que a taxa de convergência não é diretamente afetada pela dimensão $d$, em contraste com os resultados dos capítulos anteriores [^1]. No entanto, a constante $C_f$ pode ter alguma dependência inerente de $d$ [^1].

A prova do Teorema 8.1 é baseada em uma propriedade peculiar de conjuntos convexos de alta dimensão, descrita pelo teorema de Caratheodory [^1, 30, 175, 235].

**Lemma 8.3** [^1]: Seja $H$ um espaço de Hilbert e seja $G \subseteq H$ tal que, para algum $B > 0$, vale que $||g||_H < B$ para todo $g \in G$. Seja $f \in \overline{co}(G)$. Então, para todo $N \in \mathbb{N}$ e todo $c > B^2$, existem $(g_i)_{i=1}^N \subseteq G$ tal que
$$\
\left\| f - \frac{1}{N} \sum_{i=1}^N g_i \right\|_H^2 < \frac{c}{N}
$$

$\blacksquare$

Este lema fornece uma ferramenta poderosa para aproximar uma função $f$ com uma superposição de $N$ elementos em um conjunto $G$ [^1].

#### Funções com Estrutura de Composição
Outra estratégia para mitigar a maldição da dimensionalidade é considerar funções com **estruturas de composição** específicas [^1]. Essas funções são construídas por composições e combinações lineares de subfunções de baixa dimensão [^1]. Nesse caso, a maldição da dimensionalidade está presente, mas apenas através da dimensão de entrada das subfunções [^1].

Considere um grafo acíclico direcionado $G$ com $M$ vértices $\eta_1, \dots, \eta_M$ tal que [^1]:
*   Exatamente $d$ vértices, $\eta_1, \dots, \eta_d$, não têm arestas de entrada.
*   Cada vértice tem no máximo $m \in \mathbb{N}$ arestas de entrada.
*   Exatamente um vértice, $\eta_M$, não tem arestas de saída.

Com cada vértice $\eta_j$ para $j > d$, associamos uma função $f_j: \mathbb{R}^{d_j} \rightarrow \mathbb{R}$. Aqui, $d_j$ denota a cardinalidade do conjunto $S_j$, que é definido como o conjunto de índices $i$ correspondendo aos vértices $\eta_i$ para os quais temos uma aresta de $\eta_i$ para $\eta_j$. Sem perda de generalidade, assumimos que $m > d_j = |S_j| \geq 1$ para todo $j > d$. Finalmente, definimos [^1]:

$$\
F_j := x_j \quad \text{para todo} \quad j \leq d \qquad (8.2.1a)
$$

$$\
F_j := f_j((F_i)_{i \in S_j}) \quad \text{para todo} \quad j > d. \qquad (8.2.1b)
$$

Então, $F_M(x_1, \dots, x_d)$ é uma função de $\mathbb{R}^d \rightarrow \mathbb{R}$. Assumindo que

$$\
||f_j||_{C^{k,s}(\mathbb{R}^{d_j})} \leq 1 \quad \text{para todo} \quad j = d+1, \dots, M, \qquad (8.2.2)
$$

denotamos o conjunto de todas as funções do tipo $F_M$ por $\mathcal{F}^{k,s}(m, d, M)$.

**Proposição 8.5** [^1]: Sejam $k, m, d, M \in \mathbb{N}$ e $s > 0$. Seja $F_M \in \mathcal{F}^{k,s}(m, d, M)$. Então existe uma constante $C = C(m, k+s, M)$ tal que para todo $N \in \mathbb{N}$ existe uma rede neural ReLU $\tilde{F}_M$ tal que

$$\
\text{size}(\tilde{F}_M) \leq CN \log(N), \quad \text{depth}(\tilde{F}_M) \leq C \log(N)
$$
e

$$\
\sup_{x \in [0,1]^d} |F_M(x) - \tilde{F}_M(x)| \leq N^{-\frac{k+s}{m}}.
$$

#### Aproximação em Subvariedades de Dimensões Inferiores
Uma terceira abordagem para mitigar a maldição da dimensionalidade surge quando estamos interessados na precisão da aproximação apenas em uma subvariedade de dimensão inferior [^1]. Neste cenário, aproximamos funções de alta dimensão, mas nos preocupamos apenas com a precisão da aproximação em uma subvariedade de dimensão inferior [^1]. A taxa de aproximação é governada pela suavidade e pela dimensão da variedade [^1].

Suponha que $\mathcal{M}$ seja uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$ [^1]. Assumimos que existe $\delta > 0$ e finitamente muitos pontos $x_1, \dots, x_M \in \mathcal{M}$ tal que as $d$-bolas $B_{\delta/2}(x_i) := \{y \in \mathbb{R}^d \mid ||y - x_i||_2 < \delta/2\}$ para $i = 1, \dots, M$ cobrem $\mathcal{M}$ [^1]. Além disso, denotando por $T_x \mathcal{M} \sim \mathbb{R}^m$ o espaço tangencial de $\mathcal{M}$ em $x$, assumimos que $\delta > 0$ é tão pequeno que a projeção ortogonal

$$\
\pi_j: B_\delta(x_j) \cap \mathcal{M} \rightarrow T_{x_j} \mathcal{M} \qquad (8.3.1)
$$
é injetiva, o conjunto $\pi_j(B_\delta(x_j) \cap \mathcal{M}) \subseteq T_{x_j} \mathcal{M}$ tem fronteira $C^\infty$, e a projeção inversa

$$\
\pi_j^{-1}: \pi_j(B_\delta(x_j) \cap \mathcal{M}) \rightarrow \mathcal{M} \qquad (8.3.2)
$$
é $C^\infty$.

**Proposição 8.7** [^1]: Sejam $d, k \in \mathbb{N}$, $s \geq 0$, e seja $\mathcal{M}$ uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$. Então existe uma constante $C > 0$ tal que para todo $f \in C^{k,s}(\mathcal{M})$ e todo $N \in \mathbb{N}$ existe uma rede neural ReLU $\Phi_f$ tal que $\text{size}(\Phi_f) \leq CN \log(N)$, $\text{depth}(\Phi_f) \leq C \log(N)$ e

$$\
\sup_{x \in \mathcal{M}} |f(x) - \Phi_f(x)| \leq C ||f||_{C^{k,s}(\mathcal{M})} N^{-\frac{k+s}{m}}.
$$

### Conclusão
As estratégias de mitigação da maldição da dimensionalidade, como a restrição no domínio de Fourier (classe de Barron), a consideração de funções com estruturas de composição e a aproximação em subvariedades de dimensões inferiores, oferecem abordagens promissoras para lidar com a complexidade da aproximação em espaços de alta dimensão [^1]. Cada uma dessas técnicas explora propriedades específicas das funções ou do espaço de entrada para alcançar taxas de aproximação mais favoráveis [^1].

### Referências
[^1]: Capítulo 8, "High-dimensional approximation"
[^10]: Barron, A. (1993). Universal approximation bounds for superpositions of a sigmoidal function. *IEEE Transactions on Information Theory, 39*(3), 930-945.
[^30]: Barvinok, A. (2002). A course in convexity. *American Mathematical Society*.
[^51]: DeVore, R. A., Howard, R., & Micchelli, C. A. (1989). Optimal nonlinear approximation. *Manuscripta Mathematica, 63*(4), 469-478.
[^57]: El Alaoui, F. E., Herbst, B. M., & Oseledets, I. V. (2021). On the convergence of neural networks for smooth functions. *arXiv preprint arXiv:2106.08191*.
[^70]: Grohs, P., Herrmann, P., Jentzen, A., & Salimova, D. (2021). Deep neural network approximation theory for parabolic PDEs. *arXiv preprint arXiv:2108.03056*.
[^77]: Gühring, I., Kutyniok, G., & Petersen, P. (2021). Error bounds for approximations with deep ReLU networks in W^{s,p}(ℝ^d). *Analysis and Applications, 19*(04), 631-669.
[^78]: Gühring, I., Kutyniok, G., & Petersen, P. (2020). Approximation of functions by ReLU neural networks based on the Radon transform. *Constructive Approximation, 52*(1), 115-153.
[^79]: Herrmann, P., Jentzen, A., & Neufeld, A. (2020). Deep neural network approximation for stochastic differential equations. *Analysis and Applications, 18*(05), 783-834.
[^99]: Hutzenthaler, M., Jentzen, A., & Kruse, T. (2020). Overcoming the curse of dimensionality in the numerical approximation of parabolic partial differential equations. *Proceedings of the Royal Society A, 476*(2234), 20190630.
[^107]: Koch, S., Nonnenmacher, M., & Schwab, C. (2020). Optimal approximation rates for deep ReLU neural networks. *Advances in Computational Mathematics, 46*(5), 1-34.
[^116]: Kohler, M., Krzyzak, A., & Walk, H. (2002). On universal consistency of regularization methods for learning on compact manifolds. *Journal of Multivariate Analysis, 81*(1), 1-21.
[^117]: Kohler, M., & Lang, D. (2001). Supervised learning on hierarchical data structures. *Bernoulli, 7*(4), 639-661.
[^120]: Kämmerer, L., Klatzer, S., & Peterseim, T. (2021). Proof of concept: Deep learning for high-dimensional parameter identification in groundwater flow. *Advances in Water Resources, 155*, 104002.
[^123]: Lanthaler, T., Mishra, S., & Schwab, C. (2021). Deep learning for parametric elliptic PDEs with lognormal coefficients. *arXiv preprint arXiv:2103.03113*.
[^135]: Mhaskar, H. N. (1996). Neural networks for functional approximation. In *Approximation Theory VIII, Volume 1: Approximation and Interpolation* (pp. 365-372). World Scientific.
[^136]: Mhaskar, H. N. (1996). Approximation properties of a multilayered feedforward artificial neural network. *Advances in Computational Mathematics, 1*(1), 61-80.
[^148]: Petersen, P., Raslan, R., & Voigtlaender, F. (2021). Topological obstructions to function approximation by neural networks. *arXiv preprint arXiv:2106.13433*.
[^154]: Pinkus, A. (1999). Approximation theory of the MLP model in neural networks. *Acta Numerica, 8*, 143-195.
[^162]: Smale, S. (2003). Mathematical problems for the next century. *Mathematics: Frontiers and Perspectives, 93*(103), 271.
[^164]: Suzuki, T. (2018). Adaptivity of deep neural networks for learning in reproducing kernel Hilbert spaces. *arXiv preprint arXiv:1806.00941*.
[^165]: Schwab, C., & Zech, J. (2019). Deep learning in high dimension: Neural network expression rates for parametric PDEs. *arXiv preprint arXiv:1901.07737*.
[^166]: Schumaker, L. L. (2007). Spline functions: basic theory. *Cambridge university press*.
[^173]: Tölle, J. (2021). Approximation of discontinuous functions by neural networks with dimension independent rates. *arXiv preprint arXiv:2102.04167*.
[^175]: Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science (Vol. 47). *Cambridge university press*.
[^176]: Voigtlaender, F. (2019). Optimal approximation rates for deep ReLU networks via Chebyshev polynomials. *arXiv preprint arXiv:1906.10555*.
[^202]: DeVore, R. A., & Petrova, G. (2021). Neural network approximation. *Constructive Approximation, 53*(3), 1217-1244.
[^207]: Grohs, P., Jentzen, A., & Salimova, D. (2020). Deep neural network approximation theory. *arXiv preprint arXiv:2003.00358*.
[^208]: Grohs, P., Jentzen, A., & Salimova, D. (2021). Proof that artificial neural networks overcome the curse of dimensionality in the numerical approximation of solution sets of Kolmogorov partial differential equations. *Annals of Mathematics, 193*(3), 801-874.
[^210]: Barron, A. R., Birgé, L., & Massart, P. (1999). Risk bounds for model selection via penalization. *Probability Theory and Related Fields, 113*(3), 281-413.
[^212]: Yarotsky, D. (2017). Error bounds for approximations with deep ReLU networks. *Neural Networks, 94*, 103-114.
[^215]: Stein, E. M. (1970). Singular integrals and differentiability properties of functions (Vol. 30). *Princeton university press*.
[^235]: Rudelson, M., & Vershynin, R. (2007). Hanson-wright inequality for rectangular random matrices. *arXiv preprint math/0702293*.
[^238]: Klusowski, J. M. (2018). Approximation by compositions of ridge functions with applications to neural networks. *Advances in Neural Information Processing Systems, 31*.
[^239]: Srikumar, V., & Mhaskar, H. N. (2020). On the approximation properties of shallow neural networks with sigmoid-like and ReLU-like activation functions. *Neural Networks, 121*, 449-465.

<!-- END -->