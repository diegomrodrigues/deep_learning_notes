## Aproximação em Variedades de Dimensão Inferior

### Introdução
Em capítulos anteriores, foram estabelecidas taxas de convergência para a aproximação de funções $f: [0,1]^d \rightarrow \mathbb{R}$ por redes neurais, com um erro de aproximação que dependia exponencialmente da dimensão $d$ [^8]. Este fenômeno é conhecido como a *maldição da dimensionalidade* [^8]. No entanto, funções de interesse prático podem exibir propriedades adicionais que permitem taxas de convergência melhores [^8]. Este capítulo explora cenários onde a maldição da dimensionalidade pode ser mitigada [^8].

Uma dessas situações surge quando aproximamos funções definidas em um espaço de alta dimensão $\mathbb{R}^d$, mas restritas a uma variedade $M \subseteq \mathbb{R}^d$ de dimensão $m$, onde $m < d$ [^8, 96]. A ideia central é que o erro de aproximação seja determinado pela dimensão intrínseca $m$ da variedade, e não pela dimensão ambiente $d$ [^8]. Esta seção se concentra em como a aproximação em uma variedade de dimensão inferior pode contornar a maldição da dimensionalidade [^8].

### Conceitos Fundamentais

A Seção 8.3 [^96] aborda o cenário onde a entrada da rede neural pertence a $\mathbb{R}^d$, mas se origina de uma variedade $M \subseteq \mathbb{R}^d$ de dimensão $m$. Neste caso, o erro de aproximação pode ser determinado por $m$ em vez de $d$ [^96].

Assumimos que $M$ é uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$ [^97]. Supomos também a existência de $\delta > 0$ e um número finito de pontos $x_1, ..., x_M \in M$ tais que as $d$-bolas $B_{\delta/2}(x_i) := \{y \in \mathbb{R}^d : ||y - x||_2 < \delta/2\}$ para $i = 1, ..., M$ cobrem $M$ [^97]. Além disso, denotando por $T_xM \cong \mathbb{R}^m$ o espaço tangente de $M$ em $x$, assumimos que $\delta > 0$ é suficientemente pequeno para que a projeção ortogonal [^97]:
$$
\pi_j: B_{\delta}(x_j) \cap M \rightarrow T_{x_j}M
$$
seja injetiva, o conjunto $\pi_j(B_{\delta}(x_j) \cap M) \subseteq T_{x_j}M$ tenha fronteira $C^{\infty}$, e a projeção inversa [^97]:
$$
\pi_j^{-1}: \pi_j(B_{\delta}(x_j) \cap M) \rightarrow M
$$
seja $C^{\infty}$. Essa condição é satisfeita porque $M$ é uma variedade suave [^97].

Para uma função $f: M \rightarrow \mathbb{R}$ e $x \in B_{\delta}(x_j) \cap M$, podemos escrever [^97]:
$$
f(x) = f(\pi_j^{-1}(\pi_j(x))) = f_j(\pi_j(x))
$$
onde $f_j := f \circ \pi_j^{-1}: \pi_j(B_{\delta}(x_j) \cap M) \rightarrow \mathbb{R}$ [^97].

Para $f: M \rightarrow \mathbb{R}$, $k \in \mathbb{N}_0$, e $s \in [0, 1)$, definimos [^97]:
$$
||f||_{C^{k,s}(M)} := \sup_{j=1,...,M} ||f_j||_{C^{k,s}(\pi_j(B_{\delta}(x_j) \cap M))}
$$

**Proposição 8.7** [^98]: Sejam $d, k \in \mathbb{N}$, $s \geq 0$, e seja $M$ uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$. Então existe uma constante $C > 0$ tal que para todo $f \in C^{k,s}(M)$ e todo $N \in \mathbb{N}$, existe uma rede neural ReLU $\Phi_f$ tal que $size(\Phi_f) \leq CN \log(N)$, $depth(\Phi_f) \leq C \log(N)$ e
$$
\sup_{x \in M} |f(x) - \Phi_f(x)| \leq C ||f||_{C^{k,s}(M)} N^{-\frac{k+s}{m}}
$$

*Prova* [^98]: Como $M$ é compacta, existe $A > 0$ tal que $M \subseteq [-A, A]^d$. Semelhante à prova do Teorema 7.7, consideramos uma malha uniforme com nós $\{-A + 2A\frac{\nu}{n} : |\nu| \leq n\}$, e as funções de base lineares por partes correspondentes formando a partição da unidade $\sum_{\nu < n} \varphi_{\nu} = 1$ em $[-A, A]^d$ onde $supp \varphi_{\nu} \subseteq \{y \in \mathbb{R}^d : ||y - y_{\nu}||_{\infty} \leq \frac{A}{n}\}$. Seja $\delta > 0$ tal como no início desta seção. Como $M$ é coberta pelas bolas $(B_{\delta/2}(x_j))_{j=1}^M$, fixando $n \in \mathbb{N}$ suficientemente grande, para cada $\nu$ tal que $supp \varphi_{\nu} \cap M \neq \emptyset$ existe $j(\nu) \in \{1, ..., M\}$ tal que $supp \varphi_{\nu} \subseteq B_{\delta}(x_{j(\nu)})$ e definimos $I_j := \{\nu < M : j = j(\nu)\}$. Então temos para todo $x \in M$:

$$
f(x) = \sum_{\nu < n} \varphi_{\nu}(x) f_j(\pi_j(x)) = \sum_{j=1}^M \sum_{\nu \in I_j} \varphi_{\nu}(x) f_j(\pi_j(x)) \qquad (8.3.3)
$$

Em seguida, aproximamos as funções $f_j$. Seja $C_j$ o menor cubo (de dimensão $m$) em $T_{x_j}M \cong \mathbb{R}^m$ tal que $\pi_j(B_{\delta}(x_j) \cap M) \subseteq C_j$. A função $f_j$ pode ser estendida a uma função em $C_j$ (usaremos a mesma notação para esta extensão) tal que [^98]:
$$
||f||_{C^{k,s}(C_j)} \leq C ||f||_{C^{k,s}(\pi_j(B_{\delta}(x_j) \cap M))}
$$
para alguma constante dependendo de $\pi_j(B_{\delta}(x_j) \cap M)$, mas independente de $f$. Tal resultado de extensão pode, por exemplo, ser encontrado em [215, Capítulo VI]. Pelo Teorema 7.7 (também veja o Remark 7.9), existe uma rede neural $\tilde{f_j}: C_j \rightarrow \mathbb{R}$ tal que [^98]:
$$
\sup_{x \in C_j} |\tilde{f_j}(x) - f_j(x)| \leq CN^{-\frac{k+s}{m}} \qquad (8.3.4)
$$
e
$$
size(\tilde{f_j}) \leq CN \log(N), \quad depth(\tilde{f_j}) \leq C \log(N)
$$

Para aproximar $f$ em (8.3.3) agora definimos com $\epsilon := N^{-\frac{k+s}{m}}$ [^98]:
$$
\Phi_N := \sum_{j=1}^M \sum_{\nu \in I_j} \tilde{f_j} (\varphi_{\nu}, \tilde{f_j} \circ \pi_j)
$$
onde notamos que $\varphi_{\nu}$ é linear e, portanto, $\tilde{f_j} \circ \pi_j$ pode ser expressa por uma rede neural. Primeiro, vamos estimar o erro desta aproximação. Para $x \in M$ [^98]:
$$
|f(x) - \Phi(x)| = |\sum_{j=1}^M \sum_{\nu \in I_j} \varphi_{\nu}(x) f_j (\pi_j(x)) - \Phi_{\epsilon} (\varphi_{\nu}(x), \tilde{f_j} (\pi_j(x)))| \leq \sum_{j=1}^M \sum_{\nu \in I_j} |\varphi_{\nu}(x) f_j (\pi_j(x)) - \varphi_{\nu}(x) \tilde{f_j} (\pi_j(x))| + |\varphi_{\nu}(x) \tilde{f_j} (\pi_j(x)) - \Phi_{\epsilon} (\varphi_{\nu}(x), \tilde{f_j} (\pi_j(x)))| \leq sup_{i < M} ||\tilde{f_i} - f_i||_{L^{\infty}(C_i)} \sum_{j=1}^M \sum_{\nu \in I_j} |\varphi_{\nu}(x)| + \sum_{j=1}^M \sum_{\nu \in \{v \in I_j | x \in supp\epsilon\}} \epsilon \leq CN^{-\frac{k+s}{m}} + d \epsilon \leq CN^{-\frac{k+s}{m}}
$$
onde usamos que $x$ pode estar no suporte de no máximo $d$ dos $\varphi_{\nu}$, e onde $C$ é uma constante dependendo de $d$ e $M$.

Finalmente, vamos limitar o tamanho e a profundidade desta aproximação. Usando $size(\varphi_{\nu}) \leq C$, $depth(\varphi_{\nu}) \leq C$ (veja (5.3.9)) e $size(\Phi_{\epsilon}) < C \log(\epsilon) < C \log(N)$ e $depth(\Phi_{\epsilon}) < C depth(\epsilon) \leq C \log(N)$ (veja Lemma 7.3), encontramos [^98]:
$$
\sum_{j=1}^M \sum_{\nu \in I_j} (size(\Phi_{\epsilon^*}) + size(\varphi_{\nu}) + size(f_j \circ \pi_j)) \leq \sum_{j=1}^M \sum_{\nu \in I_j} C \log(N) + C + CN \log(N) = O(N \log(N))
$$
o que implica o limite no tamanho de $\Phi_N$. Além disso [^98]:
$$
depth(\Phi_N) \leq depth(\Phi_{\epsilon^*}) + max \{depth(\varphi_{\nu}, f_j)\} \leq C \log(N) + \log(N) = O(\log(N))
$$
Isto completa a prova. $\blacksquare$

### Conclusão

A aproximação de funções em variedades de dimensão inferior oferece uma abordagem promissora para mitigar a maldição da dimensionalidade [^96]. Ao focar na dimensão intrínseca da variedade, em vez da dimensão ambiente do espaço, as redes neurais podem alcançar taxas de convergência que dependem da dimensão menor [^96, 98]. A Proposição 8.7 [^98] formaliza este resultado, demonstrando que a taxa de aproximação é governada pela dimensão $m$ da variedade e pela suavidade da função [^98].

### Referências
[^8]: Capítulo 8, "High-dimensional approximation".
[^96]: Seção 8.3, "Functions on manifolds".
[^97]: Página 97, Capítulo 8.
[^98]: Proposição 8.7, Capítulo 8.

<!-- END -->