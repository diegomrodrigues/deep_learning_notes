## Aproximação em Variedades de Dimensão Inferior

### Introdução
Em capítulos anteriores, foram estabelecidas taxas de convergência para a aproximação de funções $f: [0,1]^d \rightarrow \mathbb{R}$ por redes neurais. Por exemplo, o Teorema 7.7 (não presente neste contexto) fornece um limite de erro de $O(N^{-(k+s)/d})$ em termos do tamanho da rede $N$, onde $k$ e $s$ descrevem a suavidade de $f$. Alcançar uma precisão de $\epsilon > 0$ necessita um tamanho de rede $N = O(\epsilon^{-d/(k+s)})$. Essa dependência exponencial na dimensão $d$ é conhecida como a *maldição da dimensionalidade* [^1]. Este capítulo explora cenários onde essa maldição pode ser mitigada. Uma dessas instâncias ocorre quando a entrada da rede neural pertence a $\mathbb{R}^d$, mas se origina de uma variedade $m$-dimensional $\mathcal{M} \subseteq \mathbb{R}^d$, onde $m < d$ [^96].

### Conceitos Fundamentais
Consideremos o caso em que a entrada para a rede neural pertence a $\mathbb{R}^d$, mas provém de uma variedade $m$-dimensional $\mathcal{M} \subseteq \mathbb{R}^d$, onde $m < d$ [^96]. Se medirmos apenas o erro de aproximação em $\mathcal{M}$, podemos mostrar que é $m$, e não $d$, que determina a taxa de convergência [^97].

Para formalizar essa ideia, assumimos que $\mathcal{M}$ é uma variedade $m$-dimensional suave e compacta em $\mathbb{R}^d$ [^97]. Supomos também que existem $\delta > 0$ e um número finito de pontos $x_1, ..., x_M \in \mathcal{M}$ tais que as *d-bolas* $B_{\delta/2}(x_i) := \{y \in \mathbb{R}^d : ||y - x_i||_2 < \delta/2\}$ para $i = 1, ..., M$, cobrem $\mathcal{M}$ [^97]. Além disso, denotando por $T_x\mathcal{M} \approx \mathbb{R}^m$ o espaço tangente de $\mathcal{M}$ em $x$, assumimos que $\delta > 0$ é suficientemente pequeno para que a projeção ortogonal

$$ \pi_j : B_{\delta}(x_j) \cap \mathcal{M} \rightarrow T_{x_j}\mathcal{M} \qquad (8.3.1) $$

seja injetiva, o conjunto $\pi_j(B_{\delta}(x_j) \cap \mathcal{M}) \subseteq T_{x_j}\mathcal{M}$ tenha fronteira $C^\infty$, e a projeção inversa

$$ \pi_j^{-1} : \pi_j(B_{\delta}(x_j) \cap \mathcal{M}) \rightarrow \mathcal{M} \qquad (8.3.2) $$

seja $C^\infty$ [^97]. Essa condição é possível devido à suavidade de $\mathcal{M}$ [^97]. A Figura 8.2 (não presente aqui) ilustra essa suposição [^97].

Para uma função $f : \mathcal{M} \rightarrow \mathbb{R}$ e $x \in B_{\delta}(x_j) \cap \mathcal{M}$, podemos escrever

$$ f(x) = f(\pi_j^{-1}(\pi_j(x))) = f_j(\pi_j(x)) $$

onde $f_j := f \circ \pi_j^{-1} : \pi_j(B_{\delta}(x_j) \cap \mathcal{M}) \rightarrow \mathbb{R}$ [^97]. Para $f : \mathcal{M} \rightarrow \mathbb{R}$, $k \in \mathbb{N}_0$ e $s \in [0, 1)$, definimos a norma

$$ ||f||_{C^{k,s}(\mathcal{M})} := \sup_{j=1,...,M} ||f_j||_{C^{k,s}(\pi_j(B_{\delta}(x_j) \cap \mathcal{M}))} $$

A Proposição 8.7 [^98] (apresentada a seguir) estabelece o resultado principal desta seção.

**Proposição 8.7.** Seja $d, k \in \mathbb{N}$, $s \geq 0$, e seja $\mathcal{M}$ uma variedade $m$-dimensional suave e compacta em $\mathbb{R}^d$ [^98]. Então, existe uma constante $C > 0$ tal que para todo $f \in C^{k,s}(\mathcal{M})$ e todo $N \in \mathbb{N}$, existe uma rede neural ReLU $\Phi_f$ tal que $size(\Phi_f) \leq CN \log(N)$, $depth(\Phi_f) \leq C \log(N)$ e

$$ \sup_{x \in \mathcal{M}} |f(x) - \Phi_f(x)| \leq C ||f||_{C^{k,s}(\mathcal{M})} N^{-\frac{k+s}{m}} $$

*Prova:* Como $\mathcal{M}$ é compacto, existe $A > 0$ tal que $\mathcal{M} \subseteq [-A, A]^d$ [^98]. Semelhante à prova do Teorema 7.7 (não presente neste contexto), consideramos uma malha uniforme com nós $\{-A + 2A\frac{\nu}{n} : |\nu| \leq n\}$, e as funções de base lineares por partes correspondentes formando a partição da unidade $\sum_{\nu < n} \phi_\nu = 1$ em $[-A, A]^d$, onde $supp \phi_\nu \subseteq \{y \in \mathbb{R}^d : ||y - \frac{\nu}{n}||_\infty \leq \frac{A}{n}\}$ [^98]. Seja $\delta > 0$ tal como no início desta seção [^98]. Como $\mathcal{M}$ é coberto pelas bolas $(B_{\delta/2}(x_j))_{j=1}^M$, fixando $n \in \mathbb{N}$ suficientemente grande, para cada $\nu$ tal que $supp \phi_\nu \cap \mathcal{M} \neq \emptyset$ existe $j(\nu) \in \{1,...,M\}$ tal que $supp \phi_\nu \subseteq B_{\delta}(x_{j(\nu)})$ e definimos $I_j := \{\nu < n : j = j(\nu)\}$ [^98]. Então, temos para todo $x \in \mathcal{M}$ [^98]:

$$ f(x) = \sum_{\nu < n} \phi_\nu(x) f_j(\pi_j(x)) = \sum_{j=1}^M \sum_{\nu \in I_j} \phi_\nu(x) f_j(\pi_j(x)) \qquad (8.3.3) $$

Em seguida, aproximamos as funções $f_j$ [^98]. Seja $C_j$ o menor cubo (m-dimensional) em $T_{x_j} \mathcal{M} \approx \mathbb{R}^m$ tal que $\pi_j(B_\delta(x_j) \cap \mathcal{M}) \subseteq C_j$ [^98]. A função $f_j$ pode ser estendida para uma função em $C_j$ (usaremos a mesma notação para esta extensão) tal que [^98]

$$ ||f_j||_{C^{k,s}(C_j)} \leq C ||f||_{C^{k,s}(\pi_j(B_\delta(x_j) \cap \mathcal{M}))} $$

para alguma constante dependendo de $\pi_j(B_\delta(x_j) \cap \mathcal{M})$ mas independente de $f$ [^98]. Tal resultado de extensão pode, por exemplo, ser encontrado em [215, Chapter VI] (não presente neste contexto) [^98]. Pelo Teorema 7.7 (não presente neste contexto) (e também ver Remark 7.9, não presente neste contexto), existe uma rede neural $\hat{f}_j : C_j \rightarrow \mathbb{R}$ tal que [^98]

$$ \sup_{x \in C_j} |\hat{f}_j(x) - f_j(x)| \leq C N^{-\frac{k+s}{m}} \qquad (8.3.4) $$

$$ size(\hat{f}_j) \leq CN \log(N), \qquad depth(\hat{f}_j) \leq C \log(N) $$

Para aproximar $f$ em (8.3.3) definimos agora com $\epsilon := N^{-\frac{k+s}{m}}$ [^98]

$$ \Phi_N := \sum_{j=1}^M \sum_{\nu \in I_j} \hat{f}_j(\phi_\nu, \hat{f}_j \circ \pi_j) $$

onde notamos que $\phi_\nu$ é linear e, portanto, $\hat{f}_j \circ \pi_j$ pode ser expresso por uma rede neural [^99]. Primeiro, vamos estimar o erro desta aproximação [^99]. Para $x \in \mathcal{M}$ [^99],

$$
\begin{aligned}
|f(x) - \Phi_N(x)| & = \left| \sum_{j=1}^M \sum_{\nu \in I_j} \phi_\nu(x) f_j(\pi_j(x)) - \Phi_N(\phi_\nu(x), \hat{f}_j(\pi_j(x))) \right| \\
& \leq \sum_{j=1}^M \sum_{\nu \in I_j} \left( |\phi_\nu(x) f_j(\pi_j(x)) - \phi_\nu(x) \hat{f}_j(\pi_j(x))| + |\phi_\nu(x) \hat{f}_j(\pi_j(x)) - \Phi_N(\phi_\nu(x), \hat{f}_j(\pi_j(x)))| \right) \\
& \leq \sup_{i < M} ||f_i - \hat{f}_i||_{L^\infty(C_i)} \sum_{j=1}^M \sum_{\nu \in I_j} |\phi_\nu(x)| + \sum_{j=1}^M \sum_{\{\nu \in I_j | x \in supp\}} \epsilon \\
& \leq C N^{-\frac{k+s}{m}} + d \epsilon \leq C N^{-\frac{k+s}{m}}
\end{aligned}
$$

onde usamos que $x$ pode estar no suporte de no máximo $d$ dos $\phi_\nu$, e onde $C$ é uma constante dependendo de $d$ e $M$ [^99].

Finalmente, vamos limitar o tamanho e a profundidade desta aproximação [^99]. Usando $size(\phi_\nu) \leq C$, $depth(\phi_\nu) \leq C$ (ver (5.3.9) - não presente neste contexto) e $size(\Phi_N) < C \log(\epsilon) < C \log(N)$ e $depth(\Phi_N) < C depth(\epsilon) \leq C \log(N)$ (ver Lemma 7.3 - não presente neste contexto), encontramos [^99]

$$
\begin{aligned}
\sum_{j=1}^M \sum_{\nu \in I_j} (size(\phi_\nu) + size(\phi_\nu) + size(\hat{f}_j \circ \pi_j)) & \leq \sum_{j=1}^M \sum_{\nu \in I_j} C \log(N) + C + CN \log(N) \\
& = O(N \log(N))
\end{aligned}
$$

o que implica o limite em $size(\Phi_N)$ [^99]. Além disso [^99],

$$ depth(\Phi_N) \leq depth(\phi_\nu) + \max \{ depth(\phi_\nu, \hat{f}_j) \} < C \log(N) + \log(N) = O(\log(N)) $$

Isso completa a prova [^99]. $\blacksquare$

### Conclusão

Este capítulo explorou cenários onde a maldição da dimensionalidade pode ser mitigada ao aproximar funções em variedades de dimensão inferior [^96]. Ao restringir a atenção à variedade $\mathcal{M}$, a taxa de convergência passa a depender da dimensão $m$ da variedade, em vez da dimensão $d$ do espaço ambiente $\mathbb{R}^d$ [^97]. Este resultado demonstra que o conhecimento da estrutura subjacente dos dados pode levar a aproximações mais eficientes com redes neurais [^97].

### Referências
[^1]: Página 88
[^96]: Página 96
[^97]: Página 97
[^98]: Página 98
[^99]: Página 99
<!-- END -->