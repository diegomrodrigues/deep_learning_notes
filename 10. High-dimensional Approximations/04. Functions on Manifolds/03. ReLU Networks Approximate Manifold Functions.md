## Aproximação em Variedades de Dimensão Inferior com Redes Neurais ReLU

### Introdução
Este capítulo explora a mitigação da **maldição da dimensionalidade** ao aproximar funções em variedades de dimensão inferior [^96]. Em capítulos anteriores, a taxa de convergência para a aproximação de funções $f:[0,1]^d \rightarrow \mathbb{R}$ por redes neurais dependia exponencialmente da dimensão $d$. No entanto, funções de interesse prático podem exibir propriedades adicionais que permitem taxas de convergência melhores. Aqui, investigamos o cenário onde aproximamos funções definidas em uma variedade de dimensão inferior, onde a taxa de aproximação é governada pela suavidade e pela dimensão da variedade [^1].

### Conceitos Fundamentais
**Variedades Suaves e Compactas:** Consideramos uma variedade suave e compacta $\mathcal{M}$ de dimensão $m$ em $\mathbb{R}^d$ [^96]. Uma variedade é *suave* se admite uma estrutura diferenciável, e *compacta* se é fechada e limitada. A compacticidade garante que a variedade pode ser coberta por um número finito de bolas [^97].

**Proposição 8.7:** A Proposição 8.7 [^98] afirma que, para $d, k \in \mathbb{N}$ e $s \geq 0$, existe uma constante $C > 0$ tal que, para toda função $f \in C^{k,s}(\mathcal{M})$ e todo $N \in \mathbb{N}$, existe uma rede neural ReLU $\Phi_f$ que satisfaz:

1.  **Tamanho da Rede:** `size`$(\Phi_f) \leq CN \log(N)$ [^98]
2.  **Profundidade da Rede:** `depth`$(\Phi_f) \leq C \log(N)$ [^98]
3.  **Erro de Aproximação:** $\sup_{x \in \mathcal{M}} |f(x) - \Phi_f(x)| \leq C ||f||_{C^{k,s}(\mathcal{M})} N^{-\frac{k+s}{m}}$ [^98]

Essa proposição demonstra que o erro de aproximação escala com $N^{-\frac{k+s}{m}}$, onde $m$ é a dimensão da variedade $\mathcal{M}$, em vez da dimensão $d$ do espaço ambiente $\mathbb{R}^d$. Isso representa uma mitigação da maldição da dimensionalidade, pois a complexidade da rede e a taxa de convergência dependem da dimensão intrínseca da variedade, e não da dimensão do espaço de imersão [^96].

**Demonstração da Proposição 8.7:** A demonstração da Proposição 8.7 envolve os seguintes passos [^97]:

1.  **Cobertura da Variedade:** Como $\mathcal{M}$ é compacta, existe $A > 0$ tal que $\mathcal{M} \subseteq [-A, A]^d$ [^97]. Similarmente à prova do Teorema 7.7, considera-se uma malha uniforme com nós $\\{-A + \frac{2A\nu}{n} : |\nu| \leq n\\}$, e as correspondentes funções de base lineares por partes formando a partição da unidade $\sum_{\nu < n} \phi_\nu = 1$ em $[-A, A]^d$, onde `supp`$\phi_\nu \subseteq \\{y \in \mathbb{R}^d : ||y - \frac{\nu}{n}||_\infty \leq \frac{1}{n}\\}$. Seja $\delta > 0$ como no início desta seção. Como $\mathcal{M}$ é coberta pelas bolas $(B_{\delta/2}(x_j))_{j=1}^M$, fixando $n \in \mathbb{N}$ grande o suficiente, para cada $\nu$ tal que `supp`$\phi_\nu \cap \mathcal{M} \neq 0$ existe $j(\nu) \in \\{1, \dots, M\\}$ tal que `supp`$\phi_\nu \subseteq B_\delta(x_{j(\nu)})$ e definimos $I_j := \\{\nu < M : j = j(\nu)\\}$. Então, para todo $x \in \mathcal{M}$,
    $$f(x) = \sum_{\nu < n} \phi_\nu(x) f_j(\pi_j(x)) = \sum_{j=1}^M \sum_{\nu \in I_j} \phi_\nu(x) f_j(\pi_j(x)).$$
2.  **Aproximação das Funções $f_j$:** Seja $C_j$ o menor cubo (m-dimensional) em $T_{x_j}\mathcal{M} \sim \mathbb{R}^m$ tal que $\pi_j(B_\delta(x_j) \cap \mathcal{M}) \subseteq C_j$. A função $f_j$ pode ser estendida para uma função em $C_j$ tal que $||f_j||_{C^{k,s}(C_j)} \leq C ||f||_{C^{k,s}(\pi_j(B_\delta(x_j) \cap \mathcal{M}))}$, para alguma constante dependendo de $\pi_j(B_\delta(x_j) \cap \mathcal{M})$ mas independente de $f$. Tal resultado de extensão pode ser encontrado em [215, Chapter VI]. Pelo Teorema 7.7 (e também ver Remark 7.9), existe uma rede neural $\hat{f_j} : C_j \rightarrow \mathbb{R}$ tal que
    $$\sup_{x \in C_j} |f_j(x) - \hat{f_j}(x)| \leq CN^{-\frac{k+s}{m}},$$
    e
    `size`$(\hat{f_j}) \leq CN \log(N)$, `depth`$(\hat{f_j}) \leq C \log(N)$.
3.  **Construção da Rede Neural $\Phi_N$:** Para aproximar $f$ em (8.3.3), definimos com $\epsilon := N^{-\frac{k+s}{m}}$
    $$\Phi_N := \sum_{j=1}^M \sum_{\nu \in I_j} \hat{\Phi}_\epsilon(\phi_\nu, \hat{f_j} \circ \pi_j),$$
    onde notamos que $\pi_j$ é linear e, portanto, $\hat{f_j} \circ \pi_j$ pode ser expresso por uma rede neural.
4. **Estimativa do Erro de Aproximação:** Para $x \in \mathcal{M}$,
    $$|f(x) - \Phi(x)| \leq \sum_{j=1}^M \sum_{\nu \in I_j} |\phi_\nu(x) f_j(\pi_j(x)) - \hat{\Phi}_\epsilon(\phi_\nu(x), \hat{f_j}(\pi_j(x)))|$$
    $$ \leq \sum_{j=1}^M \sum_{\nu \in I_j} (|\phi_\nu(x)f_j(\pi_j(x)) - \phi_\nu(x) \hat{f_j}(\pi_j(x))| + |\phi_\nu(x)\hat{f_j}(\pi_j(x)) - \hat{\Phi}_\epsilon(\phi_\nu(x), \hat{f_j}(\pi_j(x)))|)$$\
    $$ \leq \sup_{i < M} ||\hat{f_i} - f_i||_{L^\infty(C_i)} \sum_{j=1}^M \sum_{\nu \in I_j} |\phi_\nu(x)| + \sum_{j=1}^M \sum_{\\{\nu \in I_j | x \in supp \\}} \epsilon$$\
    $$ \leq CN^{-\frac{k+s}{m}} + d \epsilon \leq CN^{-\frac{k+s}{m}},$$
    onde usamos que $x$ pode estar no suporte de no máximo $d$ dos $\phi_\nu$, e onde $C$ é uma constante dependendo de $d$ e $M$.

5. **Estimativa do Tamanho e Profundidade:** Usando `size`$(\phi_\nu) \leq C$, `depth`$(\phi_\nu) \leq C$ (ver (5.3.9)) e `size`$(\hat{\Phi}_\epsilon) \leq C\'\log(\epsilon) \leq C\'\log(N)$ e `depth`$(\hat{\Phi}_\epsilon) \leq C$`depth`$(\hat{\phi}_\epsilon) \leq C\'\log(N)$ (ver Lemma 7.3) encontramos
    $$\sum_{j=1}^M \sum_{\nu \in I_j} (\text{size}(\phi_\nu) + \text{size}(\hat{f_j} \circ \pi_j)) \leq \sum_{j=1}^M \sum_{\nu \in I_j} C\log(N) + C + CN \log(N) = O(N \log(N)),$$
    o que implica o limite em `size`$(\Phi_N)$. Além disso,
    `depth`$(\Phi_N) \leq$`depth`$(\hat{\Phi}_\epsilon) + \max\\{\text{depth}(\phi_\nu, \hat{f_j})\\} \leq C\'\log(N) + \log(N) = O(\log(N))$.

### Conclusão
A Proposição 8.7 demonstra que, ao aproximar funções em variedades de dimensão inferior, é possível mitigar a maldição da dimensionalidade. A taxa de aproximação e a complexidade da rede neural dependem da dimensão da variedade, e não da dimensão do espaço ambiente. Este resultado tem implicações significativas para problemas de aprendizado de máquina e aproximação de funções em domínios complexos e de alta dimensão [^96].

### Referências
[^1]: Página 88, "High-dimensional approximation"
[^96]: Página 96, "Functions on manifolds"
[^97]: Página 97, "Proof. Since M is compact there exists..."
[^98]: Página 98, "Proposition 8.7. Let d,k ∈ N..."

<!-- END -->