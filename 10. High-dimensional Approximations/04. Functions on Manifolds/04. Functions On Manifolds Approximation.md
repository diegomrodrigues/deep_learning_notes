## Aproximação de Funções em Variedades Utilizando Redes Neurais
### Introdução
Em capítulos anteriores, foram estabelecidas taxas de convergência para a aproximação de funções $f: [0, 1]^d \rightarrow \mathbb{R}$ por redes neurais. No entanto, a dimensão $d$ pode influenciar exponencialmente o tamanho da rede necessária para atingir uma dada precisão, fenômeno conhecido como a *maldição da dimensionalidade* [^1]. Este capítulo explora cenários onde essa maldição pode ser mitigada. Uma dessas situações ocorre quando se aproxima funções em uma subvariedade de dimensão inferior [^1]. A ideia central é aproximar as funções $f_j$ definidas nos espaços tangentes $T_{x_j}\mathcal{M}$ da variedade $\mathcal{M}$ usando redes neurais e, em seguida, combinar essas aproximações para aproximar a função original $f$ na variedade [^9].

### Conceitos Fundamentais
Considere $\mathcal{M}$ como uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$ [^9]. Assume-se a existência de $\delta > 0$ e um número finito de pontos $x_1, ..., x_M \in \mathcal{M}$ de tal forma que as $d$-bolas $B_{\delta/2}(x_j) := \{y \in \mathbb{R}^d : ||y - x||_2 < \delta/2\}$ para $j = 1, ..., M$ cobrem $\mathcal{M}$ [^9]. Além disso, denotando por $T_x\mathcal{M} \simeq \mathbb{R}^m$ o espaço tangente de $\mathcal{M}$ em $x$, assume-se que $\delta > 0$ é suficientemente pequeno para que a projeção ortogonal:
$$
\pi_j : B_{\delta}(x_j) \cap \mathcal{M} \rightarrow T_{x_j}\mathcal{M}
$$
seja injetiva, o conjunto $\pi_j(B_{\delta}(x_j) \cap \mathcal{M}) \subseteq T_{x_j}\mathcal{M}$ tenha fronteira $C^\infty$ e a projeção inversa:
$$
\pi_j^{-1} : \pi_j(B_{\delta}(x_j) \cap \mathcal{M}) \rightarrow \mathcal{M}
$$
seja $C^\infty$ [^9]. Para uma função $f: \mathcal{M} \rightarrow \mathbb{R}$ e $x \in B_{\delta}(x_j) \cap \mathcal{M}$, pode-se escrever:
$$
f(x) = f(\pi_j^{-1}(\pi_j(x))) = f_j(\pi_j(x))
$$
onde $f_j := f \circ \pi_j^{-1} : \pi_j(B_{\delta}(x_j) \cap \mathcal{M}) \rightarrow \mathbb{R}$ [^9]. Para $f: \mathcal{M} \rightarrow \mathbb{R}$, $k \in \mathbb{N}_0$ e $s \in [0, 1)$, define-se a norma:
$$
||f||_{C^{k,s}(\mathcal{M})} := \sup_{j=1,...,M} ||f_j||_{C^{k,s}(\pi_j(B_{\delta}(x_j) \cap \mathcal{M}))}
$$
[^9].

A Proposition 8.7 [^10] apresenta o resultado principal desta seção: dados $d, k \in \mathbb{N}$, $s \geq 0$ e $\mathcal{M}$ uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$, existe uma constante $C > 0$ tal que para toda $f \in C^{k,s}(\mathcal{M})$ e todo $N \in \mathbb{N}$, existe uma rede neural ReLU $\Phi_f$ tal que:
- $size(\Phi_f) \leq CN \log(N)$
- $depth(\Phi_f) \leq C \log(N)$
- $\sup_{x \in \mathcal{M}} |f(x) - \Phi_f(x)| \leq C ||f||_{C^{k,s}(\mathcal{M})} N^{-\frac{k+s}{m}}$

A prova desta proposição envolve cobrir $\mathcal{M}$ com bolas $B_{\delta/2}(x_j)$ [^10] e considerar uma malha uniforme com nós $\{-A + \frac{2A}{n} \nu \leq n\}$, onde $M \subseteq [-A, A]^d$. As funções de base lineares por partes correspondentes formam a partição da unidade $\sum_{\nu < n} \phi_\nu = 1$ em $[-A, A]^d$, onde $supp \phi_\nu \subseteq \{y \in \mathbb{R}^d : ||y - y_\nu||_\infty \leq \frac{A}{n}\}$. Define-se $I_j := \{\nu < M : j = j(\nu)\}$, onde para cada $\nu$ tal que $supp \phi_\nu \cap M \neq 0$ existe $j(\nu) \in \{1, ..., M\}$ tal que $supp \phi_\nu \subseteq B_{\delta}(x_{j(\nu)})$. Tem-se então que para todo $x \in M$:
$$
f(x) = \sum_{\nu < n} \phi_\nu(x) f_{j(\nu)}(\pi_j(x)) = \sum_{j=1}^M \sum_{\nu \in I_j} \phi_\nu(x) f_j(\pi_j(x))
$$
Em seguida, as funções $f_j$ são aproximadas. Seja $C_j$ o menor cubo (m-dimensional) em $T_{x_j}M \simeq \mathbb{R}^m$ tal que $\pi_j(B_\delta(x_j) \cap M) \subseteq C_j$. A função $f_j$ pode ser estendida para uma função em $C_j$ tal que:
$$
||f||_{C^{k,s}(C_j)} \leq C ||f||_{C^{k,s}(\pi_j(B_\delta(x_j) \cap M))}
$$
Para essa extensão, pode-se encontrar em [215, Chapter VI]. Pelo Theorem 7.7 [^10] (e Remark 7.9 [^10]), existe uma rede neural $\hat{f_j} : C_j \rightarrow \mathbb{R}$ tal que:
$$
\sup_{x \in C_j} |f_j(x) - \hat{f_j}(x)| \leq CN^{-\frac{k+s}{m}}
$$
e
$$
size(\hat{f_j}) \leq CN \log(N), \quad depth(\hat{f_j}) \leq C \log(N)
$$
Para aproximar $f$ em (8.3.3), define-se com $\varepsilon := N^{-\frac{k+s}{m}}$:
$$
\Phi_N := \sum_{j=1}^M \sum_{\nu \in I_j} \hat{\phi_\nu}(\phi_\nu, \hat{f_j} \circ \pi_j),
$$
onde notamos que $\pi_j$ é linear e, portanto, $\hat{f_j} \circ \pi_j$ pode ser expressa por uma rede neural [^10].

### Conclusão
A aproximação de funções em variedades $\mathcal{M}$ utilizando redes neurais mitiga a maldição da dimensionalidade, uma vez que a taxa de convergência depende da dimensão $m$ da variedade e não da dimensão $d$ do espaço ambiente $\mathbb{R}^d$ [^9]. A estratégia consiste em aproximar localmente a função em espaços tangentes à variedade, utilizando redes neurais, e combinar essas aproximações locais para obter uma aproximação global na variedade [^9].

### Referências
[^1]: Capítulo 8, High-dimensional approximation.
[^9]: Section 8.3, Functions on manifolds.
[^10]: Proposition 8.7.
<!-- END -->