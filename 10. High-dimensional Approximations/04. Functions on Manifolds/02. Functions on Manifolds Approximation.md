## Aproximação em Subvariedades de Dimensão Inferior

### Introdução
Em capítulos anteriores, analisamos a aproximação de funções $f: [0,1]^d \rightarrow \mathbb{R}$ usando redes neurais, onde a complexidade da rede cresce exponencialmente com a dimensão $d$, fenômeno conhecido como a *maldição da dimensionalidade* [^1]. Este capítulo explora cenários onde essa maldição pode ser mitigada. Uma dessas situações surge quando restringimos a análise da aproximação a uma subvariedade de dimensão inferior do espaço de entrada [^1]. Especificamente, este capítulo aborda o caso em que a função a ser aproximada está definida em uma variedade suave e compacta de dimensão $m$ imersa em $\mathbb{R}^d$ [^9].

### Conceitos Fundamentais
Suponha que $\mathcal{M}$ seja uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$ [^9]. A ideia central é que, ao restringir a atenção à aproximação em $\mathcal{M}$, a taxa de convergência da aproximação depende de $m$ em vez de $d$, reduzindo efetivamente a dimensionalidade do problema [^1, 9].

Para formalizar essa ideia, cobrimos $\mathcal{M}$ com um número finito de $d$-bolas $B_{\delta/2}(x_i)$, onde $x_i \in \mathcal{M}$ [^9]. Assumimos que existe $\delta > 0$ tal que $B_{\delta/2}(x_i)$ cobrem $\mathcal{M}$. Além disso, denotando por $T_x \mathcal{M} \approx \mathbb{R}^m$ o espaço tangente de $\mathcal{M}$ em $x$, assumimos que $\delta > 0$ é suficientemente pequeno para que a projeção ortogonal $\pi_j: B_\delta(x_j) \cap \mathcal{M} \rightarrow T_{x_j} \mathcal{M}$ seja injetiva, o conjunto $\pi_j(B_\delta(x_j) \cap \mathcal{M}) \subseteq T_{x_j} \mathcal{M}$ tem fronteira $C^\infty$, e a projeção inversa $\pi_j^{-1}: \pi_j(B_\delta(x_j) \cap \mathcal{M}) \rightarrow \mathcal{M}$ é $C^\infty$ [^9, 10]. Essa condição é ilustrada na Figura 8.2 [^10].

Para uma função $f: \mathcal{M} \rightarrow \mathbb{R}$ e $x \in B_\delta(x_j) \cap \mathcal{M}$, podemos escrever:
$$f(x) = f(\pi_j^{-1}(\pi_j(x))) = f_j(\pi_j(x))$$
onde $f_j = f \circ \pi_j^{-1}: \pi_j(B_\delta(x_j) \cap \mathcal{M}) \rightarrow \mathbb{R}$ [^10].

Definimos a norma $C^{k,s}$ de $f$ em $\mathcal{M}$ como:
$$||f||_{C^{k,s}(\mathcal{M})} := \sup_{j=1,...,M} ||f_j||_{C^{k,s}(\pi_j(B_\delta(x_j) \cap \mathcal{M}))}$$
onde $k \in \mathbb{N}_0$ e $s \in [0,1)$ [^10].

**Proposição 8.7** [^11]: Sejam $d, k \in \mathbb{N}$, $s \geq 0$, e $\mathcal{M}$ uma variedade suave e compacta de dimensão $m$ em $\mathbb{R}^d$. Então, existe uma constante $C > 0$ tal que para toda $f \in C^{k,s}(\mathcal{M})$ e todo $N \in \mathbb{N}$, existe uma rede neural ReLU $\Phi_f$ tal que:
- $size(\Phi_f) \leq CN \log(N)$
- $depth(\Phi_f) \leq C \log(N)$
- $\sup_{x \in \mathcal{M}} |f(x) - \Phi_f(x)| \leq C ||f||_{C^{k,s}(\mathcal{M})} N^{-\frac{k+s}{m}}$

**Prova:**
A prova envolve a construção de uma aproximação piecewise linear de $f$ usando uma partição da unidade e aproximando localmente as funções $f_j$ em cada $T_{x_j} \mathcal{M} \approx \mathbb{R}^m$ [^11]. A taxa de convergência $N^{-\frac{k+s}{m}}$ demonstra que a complexidade da aproximação depende da dimensão $m$ da variedade, e não da dimensão $d$ do espaço ambiente [^11]. A prova detalhada é encontrada na referência [^11]. $\blacksquare$

### Conclusão
Este resultado demonstra que, ao restringir a aproximação a uma variedade de dimensão inferior, a maldição da dimensionalidade pode ser significativamente mitigada [^1, 9]. A taxa de convergência da aproximação depende da dimensão intrínseca da variedade, e não da dimensão do espaço ambiente [^9, 11]. Este cenário tem implicações importantes em diversas aplicações, como o aprendizado de máquina em dados que residem em subespaços de baixa dimensão [^1].

### Referências
[^1]: Capítulo 8, "High-dimensional approximation"
[^9]: Seção 8.3, "Functions on manifolds"
[^10]: Página 97
[^11]: Página 98
<!-- END -->