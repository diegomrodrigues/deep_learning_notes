## Splines e Teoria da Aproximação: Fundamentos e Aplicações

### Introdução
Este capítulo explora o conceito de **splines** como um fundamento na teoria da aproximação, oferecendo um método para aproximar funções contínuas com precisão arbitrária usando uma superposição de funções de base polinomiais *piecewise* [^1]. A teoria da aproximação estabelece *trade-offs* entre as propriedades da função (por exemplo, suavidade), a precisão da aproximação e o número de parâmetros necessários para alcançar essa precisão, oferecendo *insights* sobre as capacidades de aproximação das redes neurais [^1]. Teoremas de aproximação universal afirmam que redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária, levantando questões sobre o tamanho e a arquitetura ideais para uma determinada função e precisão desejada [^1].

Expandindo sobre o Capítulo 3, onde vimos que redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária [^1], este capítulo mergulha na teoria de aproximação utilizando splines. Os resultados do capítulo anterior forneceram pouco *insight* sobre o significado de "suficientemente grande" e a escolha de uma arquitetura adequada [^1]. Idealmente, dado uma função $f$ e uma precisão desejada $\epsilon > 0$, gostaríamos de ter um limite (possivelmente nítido) sobre o tamanho, profundidade e largura necessários, garantindo a existência de uma rede neural aproximando $f$ até o erro $\epsilon$ [^1].

### Conceitos Fundamentais
Um **spline** serve como um conceito fundamental na teoria da aproximação, oferecendo um método para aproximar funções contínuas com precisão arbitrária usando uma superposição de funções de base polinomial *piecewise* [^1]. A teoria da aproximação estabelece *trade-offs* entre as propriedades da função (e.g., suavidade), a precisão da aproximação e o número de parâmetros necessários para alcançar esta precisão, oferecendo *insights* sobre as capacidades de aproximação de redes neurais [^1].

**Definição 4.1.** Para $n \in \mathbb{N}$, o **B-spline cardinal univariado** de ordem $n \in \mathbb{N}$ é dado por [^1]:
$$\
S_n(x) := \frac{1}{(n-1)!} \sum_{l=0}^{n} (-1)^l \binom{n}{l} \text{ReLU}(x - l)^{n-1} \quad \text{para } x \in \mathbb{R}, \tag{4.1.1}\
$$
onde $0^0 = 0$ e $\text{ReLU}$ denota a função ReLU [^1].

Ao deslocar e dilatar o B-spline cardinal, obtemos um sistema de splines univariados [^1]. Tomando produtos tensoriais desses splines univariados, obtemos um conjunto de funções de dimensão superior conhecidas como os B-splines multivariados [^1].

**Definição 4.2.** Para $t \in \mathbb{R}$ e $n, l \in \mathbb{N}$, definimos $S_{l,t,n} := S_n(2^l(\cdot - t))$ [^2]. Adicionalmente, para $d \in \mathbb{N}$, $t \in \mathbb{R}^d$ e $n, l \in \mathbb{N}$, definimos o **B-spline multivariado** $S_{l,t,n}$ como [^2]:
$$\
S_{l,t,n}(x) := \prod_{i=1}^{d} S_{l,t_i,n}(x_i) \quad \text{para } x = (x_1, \dots, x_d) \in \mathbb{R}^d,\
$$
e
$$\
\mathcal{B}_n := \\{S_{l,t,n} \mid l \in \mathbb{N}, t \in \mathbb{R}^d\\}\
$$
é o **dicionário de B-splines** de ordem $n$ [^2].

Tendo introduzido o sistema $\mathcal{B}_n$, gostaríamos de entender quão bem podemos representar cada função suave por superposições de elementos de $\mathcal{B}_n$ [^2]. O seguinte teorema é adaptado do resultado mais geral [166, Theorem 7]; veja também [139, Theorem D.3] para uma apresentação mais próxima da presente formulação [^2].

**Teorema 4.3.** Sejam $d, n, k \in \mathbb{N}$ tais que $0 < k \leq n$ [^2]. Então existe $C$ tal que para toda $f \in C^k([0,1]^d)$ e toda $N \in \mathbb{N}$, existem $c_i \in \mathbb{R}$ com $|c_i| \leq C ||f||_{L^\infty([0,1]^d)}$ e $B_i \in \mathcal{B}_n$ para $i = 1, \dots, N$, tal que [^2]:
$$\
\left\\| f - \sum_{i=1}^{N} c_i B_i \right\\|_{L^\infty([0,1]^d)} \leq C N^{-k/d} ||f||_{C^k([0,1]^d)}.\
$$

**Observação 4.4.** Existem alguns conceitos críticos no Teorema 4.3 que reaparecerão ao longo deste livro [^2]. O número de parâmetros $N$ determina a precisão da aproximação $N^{-k/d}$ [^2]. Isto implica que alcançar a precisão $\epsilon > 0$ requer $O(\epsilon^{-d/k})$ parâmetros (de acordo com este limite superior), que cresce exponencialmente em $d$ [^2]. Essa dependência exponencial de $d$ é referida como a "maldição da dimensionalidade" e será discutida novamente nos capítulos subsequentes [^2]. O parâmetro de suavidade $k$ tem o efeito oposto de $d$ e melhora a taxa de convergência [^2]. Assim, funções mais suaves podem ser aproximadas com menos B-splines do que funções mais grosseiras [^2]. Essa aproximação mais eficiente requer o uso de B-splines de ordem $n$ com $n \geq k$ [^2]. Veremos no que segue que a ordem do B-spline está intimamente ligada ao conceito de profundidade em redes neurais [^2].

**Definição 4.5.** Uma função $\sigma : \mathbb{R} \to \mathbb{R}$ é chamada **sigmoidal de ordem** $q \in \mathbb{N}$, se $\sigma \in C^{q-1}(\mathbb{R})$ e existe $C > 0$ tal que [^3]:
$$\
\begin{aligned}\
\frac{\sigma(x)}{x^q} &\to 0 \quad \text{as } x \to -\infty, \\\\\
\frac{\sigma(x)}{x^q} &\to 1 \quad \text{as } x \to \infty, \\\\\
|\sigma(x)| &\leq C \cdot (1 + |x|)^q \quad \text{para todo } x \in \mathbb{R}.\
\end{aligned}\
$$

**Exemplo 4.6.** A unidade de potência retificada $x \mapsto \text{ReLU}(x)^q$ é sigmoidal de ordem $q$ [^3].

**Proposição 4.7.** Sejam $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e seja $\sigma : \mathbb{R} \to \mathbb{R}$ sigmoidal de ordem $q \geq 2$ [^3]. Existe uma constante $C > 0$ tal que para todo $\epsilon > 0$ existe uma rede neural $S_n^\epsilon$ com função de ativação $\sigma$, $[\log_q(n-1)]$ camadas e tamanho $C$, tal que [^3]:
$$\
||S_n - S_n^\epsilon||_{L^\infty([-K,K]^d)} \leq \epsilon.\
$$

**Teorema 4.9.** Sejam $d, n, k \in \mathbb{N}$ tais que $0 < k \leq n$ e $n \geq 2$ [^6]. Seja $q \geq 2$, e seja $\sigma$ sigmoidal de ordem $q$ [^6]. Então existe $C$ tal que para toda $f \in C^k([0,1]^d)$ e toda $N \in \mathbb{N}$ existe uma rede neural $\Phi_N$ com função de ativação $\sigma$, $[\log_2(d)] + [\log_q(k-1)]$ camadas, e tamanho limitado por $CN$, tal que [^6]:
$$\
||f - \Phi_N||_{L^\infty([0,1]^d)} \leq CN^{-k/d} ||f||_{C^k([0,1]^d)}.\
$$

### Conclusão

Este capítulo explorou o uso de splines na teoria da aproximação e sua relação com a capacidade de aproximação de redes neurais [^1]. Vimos como os B-splines podem ser usados para aproximar funções suaves e como a ordem do spline está relacionada à profundidade da rede neural [^2]. O Teorema 4.9 demonstra que redes neurais com funções sigmoidais de ordem superior podem aproximar funções suaves com a mesma precisão que aproximações de splines, enquanto possuem um número comparável de parâmetros [^6]. Além disso, a profundidade da rede deve se comportar como $O(\log(k))$ em termos do parâmetro de suavidade $k$ [^6].

### Referências
[^1]: Capítulo 4, Splines, p. 35.
[^2]: Capítulo 4, Splines, p. 36.
[^3]: Capítulo 4, Splines, p. 37.
[^6]: Capítulo 4, Splines, p. 40.
<!-- END -->