## B-Splines e Funções Suaves

### Introdução
Em continuidade ao Capítulo 3, onde foi demonstrado que redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária, este capítulo se aprofunda na compreensão de como essa aproximação é alcançada, explorando o papel fundamental das *splines* [^1]. O foco recai sobre as *B-splines*, um tipo específico de spline com propriedades notáveis para a representação de funções suaves [^1]. Este capítulo visa apresentar uma análise detalhada das B-splines, suas propriedades de aproximação e sua relação com a arquitetura de redes neurais [^1].

### Conceitos Fundamentais

#### Definição de B-Splines Univariadas
Uma *B-spline cardinal univariada* de ordem *n*, denotada por $S_n(x)$, é definida como uma função polinomial por partes, construída utilizando a função de ativação ReLU e coeficientes binomiais [^1]. Formalmente, para $n \in \mathbb{N}$, temos:

$$
S_n(x) := \frac{1}{(n-1)!} \sum_{l=0}^{n} (-1)^l \binom{n}{l} \text{ReLU}(x - l)^{n-1} \quad \text{para } x \in \mathbb{R},
$$

onde $\text{ReLU}(x) = \max\{0, x\}$ representa a função de ativação ReLU [^1]. É importante notar que $0^0 = 0$ [^1].

#### Construção de B-Splines Multivariadas
A partir da B-spline cardinal, um sistema de splines univariadas é obtido através de deslocamentos e dilatações [^1]. Tomando produtos tensoriais dessas splines univariadas, obtém-se um conjunto de funções de dimensões superiores conhecidas como *B-splines multivariadas* [^1]. Para $t \in \mathbb{R}$ e $n, l \in \mathbb{N}$, define-se $S_{l,t,n} := S_n(2^l(x - t))$ [^2].  Para $d \in \mathbb{N}$, $t \in \mathbb{R}^d$ e $n, l \in \mathbb{N}$, a B-spline multivariada $S_{l,t,n}$ é definida como:

$$
S_{l,t,n}(x) := \prod_{i=1}^{d} S_{l,t_i,n}(x_i) \quad \text{para } x = (x_1, \dots, x_d) \in \mathbb{R}^d,
$$

onde $x_i$ e $t_i$ representam a *i*-ésima coordenada de $x$ e $t$, respectivamente [^2].

#### Dicionário de B-Splines
O *dicionário de B-splines* de ordem *n*, denotado por $B_n$, é o conjunto de todas as B-splines deslocadas e dilatadas [^1]:

$$
B_n := \{S_{l,t,n} \mid l \in \mathbb{N}, t \in \mathbb{R}^d\}.
$$

Este dicionário permite representar funções suaves através de superposições desses elementos de base [^1].

#### Propriedades de Aproximação
As B-splines são conhecidas por sua capacidade de aproximar funções suaves com alta precisão [^1]. O Teorema 4.3 [^2] formaliza essa propriedade:

**Teorema 4.3.** Seja $d, n, k \in \mathbb{N}$ tal que $0 < k \leq n$. Então, existe uma constante $C$ tal que, para toda função $f \in C^k([0, 1]^d)$ e para todo $N \in \mathbb{N}$, existem coeficientes $c_i \in \mathbb{R}$ com $|c_i| \leq C ||f||_{L^\infty([0, 1]^d)}$ e $B_i \in B_n$ para $i = 1, \dots, N$, de modo que

$$
\left\| f - \sum_{i=1}^{N} c_i B_i \right\|_{L^\infty([0, 1]^d)} \leq C N^{-\frac{k}{d}} \|f\|_{C^k([0, 1]^d)}.
$$

Este teorema estabelece que a precisão da aproximação é determinada pelo número de parâmetros *N*, a suavidade da função *k* e a dimensão do espaço *d* [^2]. A taxa de convergência é dada por $N^{-k/d}$, indicando que funções mais suaves (maior *k*) podem ser aproximadas com maior precisão usando o mesmo número de parâmetros [^2]. A dependência exponencial em *d* é conhecida como a "maldição da dimensionalidade" [^2].

**Observação 4.4.** O número de parâmetros $N$ determina a precisão da aproximação $N^{-k/d}$ [^2]. Alcançar uma precisão $\epsilon > 0$ requer $O(\epsilon^{-d/k})$ parâmetros, que cresce exponencialmente em $d$ [^2]. O parâmetro de suavidade $k$ tem o efeito oposto de $d$ e melhora a taxa de convergência [^2]. Funções mais suaves podem ser aproximadas com menos B-splines do que funções mais irregulares [^2]. Essa aproximação mais eficiente requer o uso de B-splines de ordem $n$ com $n \geq k$ [^2]. A ordem da B-spline está intimamente ligada ao conceito de profundidade em redes neurais [^2].

#### B-Splines e Redes Neurais
A ordem da B-spline está intimamente ligada ao conceito de profundidade em redes neurais [^2]. Redes neurais podem aproximar combinações lineares de N B-splines com um número de parâmetros proporcional a N [^3].

**Teorema 4.9.** Sejam $d, n, k \in \mathbb{N}$ tais que $0 < k \leq n$ e $n \geq 2$. Seja $q \geq 2$, e seja $\sigma$ sigmoidal de ordem $q$ [^6].
Então existe $C$ tal que para toda $f \in C^k([0,1]^d)$ e todo $N \in \mathbb{N}$ existe uma rede neural $\Phi_N$ com função de ativação $\sigma$, $[\log_2(d)] + [\log_q(k-1)]$ layers, e size bounded por $CN$, tal que
$$
||f - \Phi_N||_{L^\infty([0,1]^d)} \leq CN^{-\frac{k}{d}}||f||_{C^k([0,1]^d)}.
$$
Este teorema mostra que redes neurais com funções sigmoidal de ordem superior podem aproximar funções suaves com a mesma precisão que aproximações spline, enquanto têm um número comparável de parâmetros [^6]. A profundidade da rede é necessária para se comportar como $O(\log(k))$ em termos do parâmetro de suavidade $k$, cp. Remark 4.4 [^6].

### Conclusão
As B-splines representam uma ferramenta poderosa para a aproximação de funções suaves, oferecendo um equilíbrio entre precisão e complexidade [^2]. Sua construção modular, baseada em deslocamentos, dilatações e produtos tensoriais, permite estender sua aplicação a espaços de dimensões superiores [^1]. A relação intrínseca entre a ordem da B-spline e a profundidade de redes neurais abre caminho para a construção de arquiteturas eficientes para a aproximação de funções complexas [^2, 6]. O estudo das B-splines, portanto, não apenas enriquece a teoria da aproximação, mas também oferece insights valiosos para o desenvolvimento de modelos de aprendizado de máquina mais eficazes [^1, 6].

### Referências
[^1]: Seção 4.1 do Capítulo 4
[^2]: Seção 4.1 e Definição 4.2 do Capítulo 4
[^3]: Exemplo 4.6 e Seção 4.2 do Capítulo 4
[^6]: Teorema 4.9 do Capítulo 4
<!-- END -->