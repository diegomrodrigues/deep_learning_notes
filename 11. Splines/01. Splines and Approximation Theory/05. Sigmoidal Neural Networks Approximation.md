## Capítulo 4.2: Reaproximação de B-Splines com Ativações Sigmoidais

### Introdução
No Capítulo 3, foi demonstrado que redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária. No entanto, esses resultados não fornecem *insights* sobre o significado de "suficientemente grande" e a escolha de uma arquitetura adequada. O objetivo deste capítulo é apresentar uma análise teórica da aproximação de funções suaves utilizando redes neurais sigmoidais, comparando seu desempenho com o de *splines*. Em particular, exploraremos como certas redes neurais sigmoidais podem igualar o desempenho de *splines* em termos de tamanho da rede, do ponto de vista da teoria da aproximação [^1]. Este resultado demonstra que as redes neurais consideradas são pelo menos tão expressivas quanto superposições de *splines* [^1].

### Conceitos Fundamentais

#### B-Splines Univariadas e Multivariadas
Inicialmente, introduzimos o conceito de **B-spline cardinal univariada** de ordem *n* [^1], definida como:

$$
S_n(x) := \frac{1}{(n-1)!} \sum_{l=0}^{n} (-1)^l \binom{n}{l} ReLU(x - l)^{n-1} \quad \text{para } x \in \mathbb{R},
$$

onde $ReLU(x) = \max\{0, x\}$ é a função de ativação ReLU e $0^0 = 0$ [^1].  Deslocando e dilatando a B-*spline* cardinal, obtemos um sistema de *splines* univariadas. O produto tensorial dessas *splines* univariadas gera um conjunto de funções de dimensão superior conhecidas como **B-splines multivariadas** [^1]. Para $t \in \mathbb{R}$ e $n, l \in \mathbb{N}$, definimos $S_{l,t,n} := S_n(2^l( \cdot - t))$ [^2]. Adicionalmente, para $d \in \mathbb{N}$, $t \in \mathbb{R}^d$ e $n, l \in \mathbb{N}$, definimos a B-*spline* multivariada $S_{l,t,n}$ como:

$$
S_{l,t,n}(x) := \prod_{i=1}^{d} S_{l,t_i,n}(x_i) \quad \text{para } x = (x_1, \dots, x_d) \in \mathbb{R}^d,
$$

e o dicionário de B-*splines* de ordem *n* é dado por $B_n := \{S_{l,t,n} \mid l \in \mathbb{N}, t \in \mathbb{R}^d\}$ [^2].

#### Reaproximação com Ativações Sigmoidais
O objetivo é demonstrar que as taxas de aproximação de B-*splines* podem ser transferidas para certas redes neurais [^2]. Este argumento é baseado em [142]. Uma função $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ é chamada **sigmoidal de ordem** $q \in \mathbb{N}$ se $\sigma \in C^{q-1}(\mathbb{R})$ e existe $C > 0$ tal que [^3]:

*   $\lim_{x \to -\infty} \frac{\sigma(x)}{x^q} = 0$
*   $\lim_{x \to \infty} \frac{\sigma(x)}{x^q} = 1$
*   $|\sigma(x)| \leq C \cdot (1 + |x|)^q$ para todo $x \in \mathbb{R}$.

Um exemplo de função sigmoidal de ordem *q* é a unidade de potência retificada $x \mapsto ReLU(x)^q$ [^3].

**Proposição 4.7** [^3]: Sejam $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ sigmoidal de ordem $q \geq 2$. Existe uma constante $C > 0$ tal que para todo $\epsilon > 0$ existe uma rede neural $S_n^\epsilon$ com função de ativação $\sigma$, $[\log_q(n-1)]$ camadas e tamanho $C$, tal que:

$$
||S_n - S_n^\epsilon||_{L^\infty([-K, K]^d)} \leq \epsilon.
$$

A prova desta proposição começa observando que $S_n$ é uma combinação linear de $n+1$ deslocamentos de $ReLU^{n-1}$ [^3]. O primeiro passo é aproximar $\sigma \circ \dots \circ \sigma (ax)$, $t$ vezes, por $ReLU(x)^{q^t}$ quando $a \rightarrow \infty$ uniformemente para todo $x \in [-K', K']$ [^3].  Definindo $t := [\log_q(n-1)]$, então $t \geq 1$ uma vez que $n \geq 2$, e $q^t > n-1$ [^3].  Portanto, para todo $K' > 0$ e $\epsilon > 0$ existe uma rede neural $\Phi_t^{\epsilon}$ com $[\log_q(n-1)]$ camadas satisfazendo [^3]:

$$
| \Phi_t^{\epsilon}(x) - ReLU(x)^{q^t} | \leq \epsilon \quad \text{para todo } x \in [-K', K'].
$$

Para reduzir a ordem, emulamos derivadas aproximadas de $\Phi_t^{\epsilon}$ [^3]. Concretamente, para todo $1 \leq p \leq q^t$, para todo $K' > 0$ e $\epsilon > 0$ existe uma rede neural $\Phi_p^{\epsilon}$ com $[\log_q(n-1)]$ camadas satisfazendo [^3]:

$$
| \Phi_p^{\epsilon}(x) - ReLU(x)^{p} | \leq \epsilon \quad \text{para todo } x \in [-K', K'].
$$

**Proposição 4.8** [^4]: Sejam $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ sigmoidal de ordem $q \geq 2$. Além disso, sejam $l \in \mathbb{N}$ e $t \in \mathbb{R}^d$. Então, existe uma constante $C > 0$ tal que para todo $\epsilon > 0$ existe uma rede neural $S_{l,t,n}^\epsilon$ com função de ativação $\sigma$, $[\log_2(d)] + [\log_q(n-1)]$ camadas, e tamanho $C$, tal que:

$$
|| S_{l,t,n} - S_{l,t,n}^\epsilon ||_{L^\infty([-K, K]^d)} \leq \epsilon.
$$

A prova se baseia na definição de $S_{l,t,n}(x) = \prod_{i=1}^d S_{l,t_i,n}(x_i)$, onde $S_{l,t_i,n}(x_i) = S_n(2^l(x_i - t_i))$ [^4]. Pela Proposição 4.7, existe uma constante $C' > 0$ tal que para cada $i = 1, \dots, d$ e todo $\epsilon > 0$, existe uma rede neural $S_{l,t_i,n}^\epsilon$ com tamanho $C'$ e $[\log_q(n-1)]$ camadas tal que [^4]:

$$
|| S_{l,t_i,n} - S_{l,t_i,n}^\epsilon ||_{L^\infty([-K, K]^d)} \leq \epsilon.
$$

**Teorema 4.9** [^6]: Sejam $d, n, k \in \mathbb{N}$ tais que $0 < k \leq n$ e $n \geq 2$. Seja $q \geq 2$, e seja $\sigma$ sigmoidal de ordem $q$. Então, existe $C$ tal que para toda $f \in C^k([0, 1]^d)$ e toda $N \in \mathbb{N}$ existe uma rede neural $\Phi_N$ com função de ativação $\sigma$, $[\log_2(d)] + [\log_q(k-1)]$ camadas, e tamanho limitado por $CN$, tal que:

$$
|| f - \Phi_N ||_{L^\infty([0, 1]^d)} \leq CN^{-k/d} ||f||_{C^k([0, 1]^d)}.
$$

### Conclusão

Este capítulo demonstrou que redes neurais com funções de ativação sigmoidais de ordem superior podem aproximar funções suaves com a mesma precisão que as aproximações por *splines*, mantendo um número comparável de parâmetros [^6]. A profundidade da rede necessária se comporta como $O(\log(k))$ em termos do parâmetro de suavidade $k$ [^6]. Este resultado sublinha a expressividade das redes neurais sigmoidais na teoria da aproximação, evidenciando sua capacidade de emular e, em certos aspectos, superar as *splines* na representação de funções [^1].

### Referências
[^1]: Capítulo 4, Splines, p. 35
[^2]: Capítulo 4, Splines, p. 36
[^3]: Capítulo 4, Splines, p. 37
[^4]: Capítulo 4, Splines, p. 38
[^5]: Capítulo 4, Splines, p. 39
[^6]: Capítulo 4, Splines, p. 40
<!-- END -->