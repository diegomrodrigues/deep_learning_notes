## Funções Sigmoidais de Ordem *q* e Reaproximação de *B-Splines*

### Introdução
Este capítulo explora a conexão entre as funções sigmoidais de ordem *q* e a aproximação de *B-splines* usando redes neurais. Como vimos anteriormente [^1], redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária. No entanto, determinar o tamanho, profundidade e largura necessários para uma dada função e precisão é um problema central na teoria da aproximação. Este capítulo, baseado em [144] [^1], demonstra que certas redes neurais sigmoidais podem atingir o mesmo desempenho que as *splines* na aproximação de funções, e que as redes neurais consideradas são pelo menos tão expressivas quanto superposições de *splines*.

### Conceitos Fundamentais
**Funções Sigmoidais de Ordem *q***
Uma função $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ é dita *sigmoidal de ordem q*, onde $q \in \mathbb{N}$, se $\sigma \in C^{q-1}(\mathbb{R})$ e existe uma constante $C > 0$ tal que [^3]:
*   $\frac{\sigma(x)}{x^q} \rightarrow 0$ quando $x \rightarrow -\infty$
*   $\frac{\sigma(x)}{x^q} \rightarrow 1$ quando $x \rightarrow \infty$
*   $|\sigma(x)| \leq C \cdot (1 + |x|)^q$ para todo $x \in \mathbb{R}$

Em outras palavras, uma função sigmoidal de ordem *q* possui derivadas contínuas até a ordem *q-1* e um comportamento assintótico específico relacionado à potência *q*.

**Exemplo: Rectified Power Unit (ReLU)**
A função *rectified power unit* (ReLU) definida como $x \rightarrow \text{ReLU}(x) = \max(0, x)$ é um exemplo de função sigmoidal de ordem 1. Mais geralmente, $x \rightarrow \text{ReLU}(x)^q$ é sigmoidal de ordem *q* [^3, 4]. A função ReLU é amplamente utilizada como função de ativação em redes neurais devido à sua simplicidade e capacidade de facilitar a aproximação de *B-splines* e outras funções.

**B-Splines Univariadas Cardinais**
Para $n \in \mathbb{N}$, a *B-spline* cardinal univariada de ordem *n* é dada por [^1]:
$$\
S_n(x) := \frac{1}{(n-1)!} \sum_{l=0}^{n} (-1)^l \binom{n}{l} \text{ReLU}(x - l)^{n-1} \quad \text{para } x \in \mathbb{R},\
$$
onde $0^0 = 0$ e ReLU denota a função de ativação ReLU. Deslocando e dilatando a *B-spline* cardinal, obtém-se um sistema de *splines* univariadas [^1].

**Reaproximação de B-Splines com Ativações Sigmoidais**
O objetivo principal é demonstrar que redes neurais podem aproximar uma combinação linear de *N* *B-splines* com um número de parâmetros proporcional a *N* [^3]. Isso leva a uma taxa de convergência para redes neurais. Inicialmente, aproximamos uma única *B-spline* univariada com uma rede neural de tamanho fixo.

**Proposição 4.7**
Seja $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ uma função sigmoidal de ordem $q \geq 2$. Então, existe uma constante $C > 0$ tal que para todo $\varepsilon > 0$, existe uma rede neural $\mathcal{S}_n$ com função de ativação $\sigma$, $[\log_2(n - 1)]$ camadas e tamanho $C$, tal que [^3]:
$$\
||\mathcal{S}_n - S_n||_{L^\infty([-K, K]^d)} \leq \varepsilon.\
$$
*Prova*: A *B-spline* $S_n$ é uma combinação linear de $n + 1$ deslocamentos de $\text{ReLU}^{n-1}$ [^3]. Podemos aproximar $\sigma \circ \sigma \circ \dots \circ \sigma(ax)$ (*t* vezes) por $\text{ReLU}(x)^{n-1}$ uniformemente para todo $x \in [-K', K']$ quando $a \rightarrow \infty$ [^3]. Definindo $t := [\log_q(n - 1)]$, temos $t > 1$ desde que $n \geq 2$, e $q^t > n - 1$. Portanto, para todo $K' > 0$ e $\varepsilon > 0$, existe uma rede neural $\Phi_t$ com $[\log_q(n - 1)]$ camadas satisfazendo [^3]:
$$\
|\Phi_t(x) - \text{ReLU}(x)^t| \leq \varepsilon \quad \text{para todo } x \in [-K', K'].\
$$
Isso mostra que podemos aproximar a ReLU elevada à potência $q^t > n - 1$. Para reduzir a ordem, emulamos derivadas aproximadas de $\Phi_t$ [^3].

**Proposição 4.8**
Seja $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ uma função sigmoidal de ordem $q \geq 2$. Além disso, seja $l \in \mathbb{N}$ e $t \in \mathbb{R}^d$. Então, existe uma constante $C > 0$ tal que para todo $\varepsilon > 0$, existe uma rede neural $S_{l,t,n}$ com função de ativação $\sigma$, $[\log_2(d)] + [\log_q(n - 1)]$ camadas e tamanho $C$, tal que [^4]:
$$\
||S_{l,t,n} - \widehat{S_{l,t,n}}||_{L^\infty([-K, K]^d)} \leq \varepsilon.\
$$
*Prova*: Por definição, $S_{l,t,n}(x) = \prod_{i=1}^d S_{l,t_i,n}(x_i)$, onde $S_{l,t_i,n}(x_i) = S_n(2^l(x_i - t_i))$ [^4]. Pela Proposição 4.7, existe uma constante $C' > 0$ tal que para cada $i = 1, \dots, d$ e todo $\varepsilon > 0$, existe uma rede neural $\widehat{S_{l,t_i,n}}$ com tamanho $C'$ e $[\log_q(n - 1)]$ camadas tal que [^4]:
$$\
||S_{l,t_i,n} - \widehat{S_{l,t_i,n}}||_{L^\infty([-K, K]^d)} \leq \varepsilon.\
$$
Se $d = 1$, a afirmação é verdadeira. Para $d$ geral, resta mostrar que o produto dos $\widehat{S_{l,t_i,n}}$ para $i = 1, \dots, d$ pode ser aproximado [^4].

**Teorema 4.9**
Seja $d, n, k \in \mathbb{N}$ tal que $0 < k \leq n$ e $n \geq 2$. Seja $q \geq 2$, e seja $\sigma$ sigmoidal de ordem $q$. Então, existe $C$ tal que para toda $f \in C^k([0, 1]^d)$ e toda $N \in \mathbb{N}$, existe uma rede neural $\Phi_N$ com função de ativação $\sigma$, $[\log_2(d)] + [\log_q(k - 1)]$ camadas e tamanho limitado por $CN$, tal que [^5]:
$$\
||f - \Phi_N||_{L^\infty([0, 1]^d)} \leq CN^{-k/d} ||f||_{C^k([0, 1]^d)}.\
$$
*Prova*: Fixe $N \in \mathbb{N}$. Pelo Teorema 4.3, existem coeficientes $|c_i| < C ||f||_{L^\infty([0, 1]^d)}$ e $B_i \in \mathcal{B}_n$ para $i = 1, \dots, N$, tal que [^5]:
$$\
||f - \sum_{i=1}^N c_i B_i||_{L^\infty([0, 1]^d)} \leq CN^{-k/d} ||f||_{C^k([0, 1]^d)}.\
$$
Além disso, pela Proposição 4.8, para cada $i = 1, \dots, N$, existe uma rede neural $\widehat{B_i}$ com $[\log_2(d)] + [\log_q(k - 1)]$ camadas e um tamanho fixo, que aproxima $B_i$ em $[-1, 1]^d \supset [0, 1]^d$ até um erro de $\varepsilon := N^{-k/d}/N$ [^5]. O tamanho de $\widehat{B_i}$ é independente de $i$ e $N$ [^5].

### Conclusão
Este capítulo demonstrou que redes neurais com funções sigmoidais de ordem superior podem aproximar funções suaves com a mesma precisão que aproximações por *splines*, mantendo um número comparável de parâmetros [^6]. A profundidade da rede é proporcional a $O(\log(k))$ em relação ao parâmetro de suavidade *k*. Este resultado reforça a capacidade das redes neurais como aproximadores universais e destaca a importância da escolha da função de ativação para obter taxas de aproximação ótimas.

### Referências
[^1]: Capítulo 4, Splines.
[^2]: Definição 4.1, B-spline cardinal univariada de ordem n.
[^3]: Definição 4.5 e Exemplo 4.6, Funções Sigmoidais de Ordem *q* e ReLU. Proposição 4.7, Aproximação de B-Splines com Ativações Sigmoidais.
[^4]: Proposição 4.8, Aproximação de B-Splines Multivariadas.
[^5]: Teorema 4.9, Taxas de Aproximação para Redes Neurais com Funções Sigmoidais.
[^6]: Conclusão do Teorema 4.9, Comparação com Aproximações por Splines.
<!-- END -->