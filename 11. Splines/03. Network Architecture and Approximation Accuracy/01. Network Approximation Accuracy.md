## Aproximação de Funções Suaves com Redes Neurais Sigmoidais

### Introdução
No Capítulo 3, foi demonstrado que redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária [^1]. No entanto, esses resultados forneceram pouca informação sobre o significado de "suficientemente grande" e a escolha de uma arquitetura adequada [^1]. O presente capítulo explora a capacidade de redes neurais com funções de ativação sigmoidal de aproximar funções suaves, especificamente funções em $C^k([0,1]^d)$, e relaciona a profundidade da rede com o parâmetro de suavidade *k* [^1]. Este capítulo se baseia nos conceitos de aproximação de funções e arquiteturas de redes neurais, expandindo o conhecimento sobre a relação entre a complexidade da rede e a precisão da aproximação [^1].

### Conceitos Fundamentais
O objetivo é demonstrar que redes neurais podem aproximar combinações lineares de *N* B-splines com um número de parâmetros proporcional a *N* [^3]. Como consequência imediata do Teorema 4.9, obtém-se uma taxa de convergência para redes neurais [^3]. Para isso, inicia-se aproximando um único B-spline univariado com uma rede neural de tamanho fixo [^3].

**Definição de Função Sigmoidal:** Uma função $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ é chamada *sigmoidal de ordem q ∈ N* se $\sigma \in C^{q-1}(\mathbb{R})$ e existe $C > 0$ tal que [^3]:
*   $\lim_{x \to -\infty} \frac{\sigma(x)}{x^q} = 0$
*   $\lim_{x \to \infty} \frac{\sigma(x)}{x^q} = 1$
*   $|\sigma(x)| \leq C \cdot (1 + |x|)^q$ para todo $x \in \mathbb{R}$

Um exemplo de função sigmoidal é a rectified power unit $x \rightarrow ReLU(x)^q$ [^3].

**Proposição 4.7:** Sejam $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ sigmoidal de ordem $q \geq 2$ [^3]. Existe uma constante $C > 0$ tal que, para todo $\varepsilon > 0$, existe uma rede neural $S_n^{\epsilon}$ com função de ativação $\sigma$, $\lceil \log_q(n-1) \rceil$ layers, e tamanho $C$, tal que [^3]:

$$||S_n - S_n^{\epsilon}||_{L^{\infty}([-K,K]^d)} \leq \varepsilon$$

*Prova:* Pela definição (4.1.1), $S_n$ é uma combinação linear de $n+1$ shifts de $ReLU^{n-1}$ [^3]. Inicia-se aproximando $\sigma^t$, onde [^3]:

$$\alpha^{-t} \sigma \circ \sigma \circ ... \circ \sigma(\alpha x) \xrightarrow{\alpha \to \infty} ReLU(x)^{q^t}$$

uniformemente para todo $x \in [-K', K']$ [^3].

Define-se $t := \lceil \log_q(n-1) \rceil$ [^3]. Então $t > 1$ uma vez que $n \geq 2$, e $q^t > n - 1$ [^3]. Assim, para todo $K' > 0$ e $\varepsilon > 0$ existe uma rede neural $\Phi_t^{\epsilon}$ com $\lceil \log_q(n-1) \rceil$ layers satisfazendo [^3]:

$$|\Phi_t^{\epsilon}(x) - ReLU(x)^{q^t}| \leq \varepsilon \text{ para todo } x \in [-K', K']$$

Isto demonstra que podemos aproximar $ReLU$ à potência de $q^t > n-1$ [^3]. O objetivo é obter uma aproximação de $ReLU$ elevado à potência $n-1$, que pode ser menor que $q^t$ [^3]. Para reduzir a ordem, emulamos derivadas aproximadas de $\Phi_t^{\epsilon}$ [^3]. Concretamente, demonstramos a seguinte afirmação: Para todo $1 \leq p < q^t$, para todo $K' > 0$ e $\varepsilon > 0$ existe uma rede neural $\Phi_p^{\epsilon}$ tendo $\lceil \log_q(n-1) \rceil$ layers e satisfazendo [^3]:

$$|\Phi_p^{\epsilon}(x) - ReLU(x)^p| \leq \varepsilon \text{ para todo } x \in [-K', K']$$ $\blacksquare$

**Proposição 4.8:** Sejam $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ sigmoidal de ordem $q \geq 2$ [^4]. Além disso, seja $l \in \mathbb{N}$ e $t \in \mathbb{R}^d$ [^4].

Então, existe uma constante $C > 0$ tal que, para todo $\varepsilon > 0$, existe uma rede neural $S_{l,t,n}^{\epsilon}$ com função de ativação $\sigma$, $\lceil \log_2(d) \rceil + \lceil \log_q(n-1) \rceil$ layers e tamanho $C$, tal que [^4]:

$$||S_{l,t,n} - S_{l,t,n}^{\epsilon}||_{L^{\infty}([-K,K]^d)} \leq \varepsilon$$

**Teorema 4.9:** Sejam $d, n, k \in \mathbb{N}$ tais que $0 < k \leq n$ e $n \geq 2$ [^6]. Seja $q \geq 2$, e seja $\sigma$ sigmoidal de ordem $q$ [^6].

Então, existe $C$ tal que para toda $f \in C^k([0,1]^d)$ e toda $N \in \mathbb{N}$ existe uma rede neural $\Phi_N$ com função de ativação $\sigma$, $\lceil \log_2(d) \rceil + \lceil \log_q(k-1) \rceil$ layers, e tamanho limitado por $CN$, tal que [^6]:

$$||f - \Phi_N||_{L^{\infty}([0,1]^d)} \leq CN^{-k/d}||f||_{C^k([0,1]^d)}$$

*Prova:* Fixe $N \in \mathbb{N}$ [^6]. Pelo Teorema 4.9, existem coeficientes $|c_i| < C||f||_{L^{\infty}([0,1]^d)}$ e $B_i \in B^n$ para $i = 1, ..., N$, tal que [^6]:

$$||f - \sum_{i=1}^{N} c_i B_i||_{L^{\infty}([0,1]^d)} \leq CN^{-k/d}||f||_{C^k([0,1]^d)}$$

Além disso, pela Proposição 4.8, para cada $i = 1, ..., N$ existe uma rede neural $B_i^{\epsilon}$ com $\lceil \log_2(d) \rceil + \lceil \log_q(k-1) \rceil$ layers, e um tamanho fixo, que aproxima $B_i$ em $[-1,1]^d \supset [0,1]^d$ até um erro de $\epsilon := N^{-k/d}/N$ [^6]. O tamanho de $B_i^{\epsilon}$ é independente de $i$ e $N$ [^6].

Pela Proposição 2.3, existe uma rede neural $\Phi_N$ que aproxima uniformemente $\sum_{i=1}^{N} c_i B_i^{\epsilon}$ até um erro $\epsilon$ em $[0,1]^d$, e tem $\lceil \log_2(d) \rceil + \lceil \log_q(k-1) \rceil$ layers [^6]. O tamanho desta rede é linear em $N$ (ver Exercício 4.11) [^6]. Isto conclui a prova [^6]. $\blacksquare$

### Conclusão
O Teorema 4.9 demonstra que redes neurais com funções sigmoidais de ordem superior podem aproximar funções suaves com a mesma precisão que as aproximações spline, enquanto mantêm um número comparável de parâmetros [^6]. A profundidade da rede deve se comportar como $O(\log(k))$ em termos do parâmetro de suavidade *k* [^6]. Este resultado estabelece uma conexão importante entre a arquitetura da rede neural, a suavidade da função a ser aproximada e a precisão da aproximação [^6]. Além disso, destaca a relevância da profundidade da rede para capturar a estrutura de funções suaves [^6].

### Referências
[^1]: Capítulo 4, Splines, p. 35
[^2]: Capítulo 4, Splines, p. 36
[^3]: Capítulo 4, Splines, p. 37
[^4]: Capítulo 4, Splines, p. 38
[^5]: Capítulo 4, Splines, p. 39
[^6]: Capítulo 4, Splines, p. 40
<!-- END -->