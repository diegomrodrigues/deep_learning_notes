## Reaproximação de B-Splines com Redes Neurais Sigmoidais

### Introdução
Em continuidade ao Capítulo 3, onde vimos que redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária, este capítulo se aprofunda na relação entre redes neurais e *splines*, especificamente B-splines. O objetivo é demonstrar que redes neurais sigmoidais podem alcançar taxas de convergência comparáveis às aproximações por splines, oferecendo um *trade-off* entre a suavidade da função, a precisão da aproximação e o número de parâmetros necessários [^1]. Este capítulo segue a abordagem de [144] e [142].

### Conceitos Fundamentais
**B-Splines:**
Definimos a *univariate cardinal B-spline* de ordem $n \in \mathbb{N}$ como [^2]:
$$ S_n(x) := \frac{1}{(n-1)!} \sum_{l=0}^{n} (-1)^l \binom{n}{l} ReLU(x - l)^{n-1} \quad \text{para } x \in \mathbb{R}, $$
onde $ReLU(x) = \max\{0, x\}$ é a função ReLU. Ao deslocar e dilatar a cardinal B-spline, obtemos um sistema de splines univariadas. Os produtos tensoriais dessas splines univariadas resultam em um conjunto de funções de dimensão superior conhecidas como B-splines multivariadas [^2].

**Aproximação de B-Splines com Redes Neurais Sigmoidais:**
O objetivo central é demonstrar que uma combinação linear de $N$ B-splines pode ser aproximada por uma rede neural com um número de parâmetros proporcional a $N$ [^3]. Isso leva a taxas de convergência comparáveis às obtidas por aproximações de spline.

**Aproximando a Função ReLU:**
Um passo crucial é aproximar a função ReLU usando redes neurais. A Proposição 4.7 [^3] estabelece que, para qualquer $K' > 0$ e $\varepsilon > 0$, existe uma rede neural $\Phi_\varepsilon$ com $[\log_q(n-1)]$ camadas (onde $q$ é a ordem da função sigmoidal) tal que $||\Phi_\varepsilon(x) - ReLU(x)|| \leq \varepsilon$ para todo $x \in [-K', K']$. Isso demonstra que a função ReLU pode ser aproximada por redes neurais com um número limitado de camadas.

**Prova da Proposição 4.7:**
A prova da Proposição 4.7 [^3] começa observando que $S_n$ é uma combinação linear de $n+1$ deslocamentos de $ReLU^{n-1}$.  Portanto, o objetivo é aproximar $\sigma^t$, onde $\sigma$ é uma função sigmoidal de ordem $q \geq 2$.  Para todo $K' > 0$ e $t \in \mathbb{N}$,
$$ \alpha^{-t} \underbrace{\sigma \circ \sigma \circ \dots \circ \sigma}_{t \text{ vezes}} (\alpha x) \xrightarrow{\alpha \to \infty} ReLU(x) \quad \text{uniformemente para } x \in [-K', K']. $$
Definindo $t := [\log_q(n-1)]$, temos $t \geq 1$ (pois $n \geq 2$) e $q^t \geq n-1$. Assim, para todo $K' > 0$ e $\varepsilon > 0$, existe uma rede neural $\Phi_t$ com $[\log_q(n-1)]$ camadas satisfazendo
$$ ||\Phi_t(x) - ReLU(x)^t|| \leq \varepsilon \quad \text{para todo } x \in [-K', K']. $$
Isso mostra que podemos aproximar a ReLU à potência de $q^t > n-1$. No entanto, nosso objetivo é obter uma aproximação da ReLU elevada à potência $n-1$, que pode ser menor que $q^t$. Para reduzir a ordem, emulamos derivadas aproximadas de $\sigma^t$.

**Lema 4.7.1 (Emulação de Derivadas):** Para todo $1 \leq p < q^t$, para todo $K' > 0$ e $\varepsilon > 0$, existe uma rede neural $\Phi_p$ tendo $[\log_q(n-1)]$ camadas e satisfazendo
$$ ||\Phi_p(x) - ReLU(x)^p|| \leq \varepsilon \quad \text{para todo } x \in [-K', K']. $$

**Prova do Lema 4.7.1:**
A prova procede por indução sobre $p = q^t, q^t - 1, \dots$. Assumimos que a afirmação vale para algum $p \in \{2, \dots, q^t\}$.  Fixamos $\delta > 0$. Então,
$$ \left| \frac{\Phi_p(x + \delta) - \Phi_p(x)}{\rho \delta} - ReLU(x)^{p-1} \right| \leq 2 \frac{\varepsilon}{\rho \delta} + \frac{|ReLU(x + \delta)^p - ReLU(x)^p|}{\rho \delta}. $$
Pelo teorema binomial, existe $\delta^* > 0$ tal que
$$ \left| \frac{\Phi_p(x + \delta^*) - \Phi_p(x)}{\rho \delta^*} - ReLU(x)^{p-1} \right| < \varepsilon, $$
para todo $x \in [-K', K']$. Pela Proposição 2.3, $(\Phi_p(x + \delta^*) - \Phi_p(x))/(\rho \delta^*)$ é uma rede neural com $[\log_q(n-1)]$ camadas e tamanho independente de $\varepsilon$. Chamando esta rede neural de $\Phi_{p-1}$, mostramos que a afirmação vale para $p-1$, o que conclui o argumento de indução e prova o lema. $\blacksquare$

**Extensão para B-Splines Multivariadas:**
A Proposição 4.8 [^4] estende a Proposição 4.7 para B-splines multivariadas $S_{l,t,n}$ para $l, d \in \mathbb{N}$ e $t \in \mathbb{R}^d$. Ela estabelece que existe uma rede neural $S_{l,t,n}^{\sigma}$ com função de ativação $\sigma$, $[\log_2(d)] + [\log_q(n-1)]$ camadas e tamanho $C$ tal que
$$ ||S_{l,t,n} - S_{l,t,n}^{\sigma}||_{L^\infty([-K, K]^d)} \leq \varepsilon. $$

**Teorema 4.9 (Aproximação de Funções Suaves):**
O Teorema 4.9 [^6] formaliza a capacidade de redes neurais com funções sigmoidais de ordem superior de aproximar funções suaves com a mesma precisão que as aproximações por splines, mantendo um número comparável de parâmetros.

### Conclusão
Este capítulo demonstrou que redes neurais sigmoidais podem aproximar B-splines e combinações lineares destas com um número de parâmetros proporcional ao número de B-splines. Isso implica que redes neurais podem atingir taxas de convergência comparáveis às obtidas por aproximações de spline, estabelecendo uma conexão importante entre esses dois campos e abrindo caminho para o uso de redes neurais na aproximação de funções suaves [^6].

### Referências
[^1]: Capítulo 3 do contexto fornecido.
[^2]: Definition 4.1 e texto subsequente.
[^3]: Example 4.6 e Proposition 4.7.
[^4]: Proposition 4.8.
[^5]: Proposition 2.3.
[^6]: Theorem 4.9.
<!-- END -->