## Reaproximação de B-Splines com Ativações Sigmoidais

### Introdução
Em capítulos anteriores, foi demonstrado que redes neurais suficientemente grandes podem aproximar qualquer função contínua com precisão arbitrária [^1]. O presente capítulo visa aprofundar essa análise, especificamente no contexto da aproximação de *B-splines* utilizando redes neurais com ativações sigmoidais. Expandindo o conceito apresentado no Capítulo 3, exploraremos como as taxas de aproximação de *B-splines* podem ser transferidas para certas arquiteturas de redes neurais [^1]. A discussão a seguir baseia-se no trabalho de [142] [^2].

### Conceitos Fundamentais

**Definição de B-Spline Univariada Cardinal:** Para $n \in \mathbb{N}$, a *B-spline* univariada cardinal de ordem $n$ é definida como [^1]:
$$
S_n(x) := \frac{1}{(n-1)!} \sum_{l=0}^{n} (-1)^l \binom{n}{l} ReLU(x - l)^{n-1} \quad \text{para } x \in \mathbb{R},
$$
onde $0^0 = 0$ e $ReLU$ denota a função de ativação ReLU [^1].

**Definição de B-Spline Multivariada:** Para $t \in \mathbb{R}$ e $n, l \in \mathbb{N}$, definimos $S_{l,t,n} := S_n(2^l( \cdot - t))$. Adicionalmente, para $d \in \mathbb{N}$, $t \in \mathbb{R}^d$ e $n, l \in \mathbb{N}$, definimos a *B-spline* multivariada $S_{l,t,n}$ como [^2]:

$$
S_{l,t,n}(x) := \prod_{i=1}^{d} S_{l,t_i,n}(x_i) \quad \text{para } x = (x_1, \dots, x_d) \in \mathbb{R}^d,
$$

e o dicionário de *B-splines* de ordem $n$ é dado por [^2]:

$$
\mathcal{B}_n := \{ S_{l,t,n} \mid l \in \mathbb{N}, t \in \mathbb{R}^d \}.
$$

**Definição de Função Sigmoidal:** Uma função $\sigma: \mathbb{R} \to \mathbb{R}$ é dita *sigmoidal* de ordem $q \in \mathbb{N}$, se $\sigma \in C^{q-1}(\mathbb{R})$ e existe $C > 0$ tal que [^3]:
$$
\frac{\sigma(x)}{x^q} \to 0 \text{ as } x \to -\infty,
$$
$$
\frac{\sigma(x)}{x^q} \to 1 \text{ as } x \to \infty,
$$
$$
|\sigma(x)| \leq C \cdot (1 + |x|)^q \text{ for all } x \in \mathbb{R}.
$$
Um exemplo de função sigmoidal é a *rectified power unit* $x \to ReLU(x)^q$ [^3].

**Proposição 4.7:** Sejam $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e seja $\sigma: \mathbb{R} \to \mathbb{R}$ sigmoidal de ordem $q \geq 2$. Existe uma constante $C > 0$ tal que para todo $\varepsilon > 0$, existe uma rede neural $S_n$ com função de ativação $\sigma$, $\lceil \log_q(n-1) \rceil$ camadas e tamanho $C$, tal que [^3]:

$$
||S_n - S_n||_{L^\infty([-K, K]^d)} \leq \varepsilon.
$$

*Prova:* Por definição, $S_n$ é uma combinação linear de $n+1$ deslocamentos de $ReLU^{n-1}$ [^3]. Aproximaremos $\sigma^t$, a composição $t$ vezes de $\sigma$, de $ReLU^{n-1}$. Para cada $K' > 0$ e $t \in \mathbb{N}$ [^3]:

$$
\alpha^{-1} \underbrace{\sigma \circ \sigma \circ \dots \circ \sigma}_{t \text{ times}}(a x) \xrightarrow{a \to \infty} ReLU(x)^q
$$

uniformemente para todo $x \in [-K', K']$ [^3].

Seja $t := \lceil \log_q(n-1) \rceil$. Então $t \geq 1$ uma vez que $n \geq 2$, e $q^t > n-1$. Assim, para todo $K' > 0$ e $\varepsilon > 0$ existe uma rede neural $\Phi_t$ com $\lceil \log_q(n-1) \rceil$ camadas satisfazendo [^3]:

$$
|\Phi_t(x) - ReLU(x)^{q^t}| \leq \varepsilon \quad \text{para todo } x \in [-K', K'].
$$

Isso mostra que podemos aproximar $ReLU$ elevado à potência $q^t > n-1$. No entanto, nosso objetivo é obter uma aproximação de $ReLU$ elevado à potência $n-1$, que pode ser menor que $q^t$. Para reduzir a ordem, emulamos derivadas aproximadas de $\Phi_t$. Concretamente, mostramos a seguinte afirmação: Para todo $1 < p < q^t$, para todo $K' > 0$ e $\varepsilon > 0$, existe uma rede neural $\Phi_p$ tendo $\lceil \log_q(n-1) \rceil$ camadas e satisfazendo [^3]:

$$
|\Phi_p(x) - ReLU(x)^p| \leq \varepsilon \quad \text{para todo } x \in [-K', K'].
$$

A afirmação é válida para $p = q^t$. Agora procedemos por indução sobre $p = q^t, q^t-1, \dots$. Assumimos que a equação (4.2.3) [^3] vale para algum $p \in \{2, \dots, q^t\}$. Fixe $\delta \geq 0$. Então [^3]:

$$
\left| \frac{\Phi_p(x + \delta) - \Phi_p(x)}{\rho_\delta} - ReLU(x)^{p-1} \right| \leq \frac{ \left| \Phi_p(x + \delta) - ReLU(x + \delta)^p \right| + \left| ReLU(x + \delta)^p - ReLU(x)^p \right| }{\rho_\delta} < 2 \frac{\varepsilon}{\rho_\delta} + \frac{|ReLU(x + \delta)^p - ReLU(x)^p|}{\rho_\delta}.
$$

Portanto, pelo teorema binomial, segue que existe $\delta^* > 0$ tal que [^3]:

$$
\left| \frac{\Phi_p(x + \delta^*) - \Phi_p(x)}{\rho_{\delta^*}} - ReLU(x)^{p-1} \right| < \varepsilon,
$$

para todo $x \in [-K', K']$ [^3]. Pela Proposição 2.3 [^3], $(\Phi_p(x + \delta^*) - \Phi_p(x)) / (\rho_{\delta^*})$ é uma rede neural com $\lceil \log_q(n-1) \rceil$ camadas e tamanho independente de $\varepsilon$. Chamando esta rede neural de $\Phi_{p-1}$, mostra que (4.2.3) [^3] vale para $p-1$, o que conclui o argumento de indução e prova a afirmação.

Para cada rede neural $\Phi$, cada translação espacial $\Phi(\cdot - t)$ é uma rede neural da mesma arquitetura. Portanto, cada termo na soma (4.1.1) [^1] pode ser aproximado com precisão arbitrária por uma rede neural de tamanho fixo. Como pela Proposição 2.3 [^3], somas de redes neurais da mesma profundidade são novamente redes neurais da mesma profundidade, o resultado segue. $\blacksquare$

**Proposição 4.8:** Sejam $n, d \in \mathbb{N}$, $n \geq 2$, $K > 0$, e seja $\sigma: \mathbb{R} \to \mathbb{R}$ sigmoidal de ordem $q \geq 2$. Além disso, seja $l \in \mathbb{N}$ e $t \in \mathbb{R}^d$. Então, existe uma constante $C > 0$ tal que para todo $\varepsilon > 0$, existe uma rede neural $S_{l,t,n}$ com função de ativação $\sigma$, $\lceil \log_2(d) \rceil + \lceil \log_q(n-1) \rceil$ camadas e tamanho $C$, tal que [^4]:

$$
||S_{l,t,n} - \Phi_{S_{l,t,n}}||_{L^\infty([-K, K]^d)} \leq \varepsilon.
$$

*Prova:* Por definição, $S_{l,t,n}(x) = \prod_{i=1}^{d} S_{l,t_i,n}(x_i)$, onde $S_{l,t_i,n}(x_i) = S_n(2^l(x_i - t_i))$ [^4].

Pela Proposição 4.7 [^3], existe uma constante $C' > 0$ tal que para cada $i = 1, \dots, d$ e todo $\varepsilon > 0$, existe uma rede neural $\Phi_{S_{l,t_i,n}}$ com tamanho $C'$ e $\lceil \log_q(n-1) \rceil$ camadas tal que [^4]:

$$
||S_{l,t_i,n} - \Phi_{S_{l,t_i,n}}||_{L^\infty([-K, K]^d)} \leq \varepsilon.
$$

Se $d = 1$, isso mostra a afirmação. Para $d$ geral, resta mostrar que o produto dos $\Phi_{S_{l,t_i,n}}$ para $i = 1, \dots, d$ pode ser aproximado [^4].

Primeiro provamos a seguinte afirmação por indução: Para todo $d \in \mathbb{N}$, $d > 2$, existe uma constante $C'' > 0$, tal que para todo $K' > 1$ e todo $\varepsilon > 0$, existe uma rede neural $\Phi_{mult, \varepsilon, d}$ com tamanho $C''$, $\lceil \log_2(d) \rceil$ camadas e função de ativação $\sigma$ tal que para todo $x_1, \dots, x_d$ com $|x_i| \leq K'$ para todo $i = 1, \dots, d$ [^4]:

$$
\left| \Phi_{mult, \varepsilon, d}(x_1, \dots, x_d) - \prod_{i=1}^{d} x_i \right| < \varepsilon.
$$

Para o caso base, seja $d = 2$. Semelhante à prova da Proposição 4.7 [^3], pode-se mostrar que existe $C'''' > 0$ tal que para todo $\varepsilon > 0$ e $K' > 0$, existe uma rede neural $\Phi_{square, \varepsilon}$ com uma camada oculta e tamanho $C''''$ tal que [^5]:

$$
\left| \Phi_{square, \varepsilon} - ReLU(x)^2 \right| \leq \varepsilon \quad \text{para todo } |x| < K'.
$$

Para todo $x = (x_1, x_2) \in \mathbb{R}^2$ [^5]:

$$
x_1 x_2 = \frac{1}{2} \left( (x_1 + x_2)^2 - x_1^2 - x_2^2 \right) = \frac{1}{2} \left( ReLU(x_1 + x_2)^2 + ReLU(-x_1 - x_2)^2 - ReLU(x_1)^2 - ReLU(-x_1)^2 - ReLU(x_2)^2 - ReLU(-x_2)^2 \right).
$$

Cada termo no lado direito pode ser aproximado até um erro uniforme $\varepsilon/6$ com uma rede de tamanho $C''''$ e uma camada oculta [^5]. Pela Proposição 2.3 [^3], concluímos que existe uma rede neural $\Phi_{mult, \varepsilon, 2}$ satisfazendo (4.2.4) [^5] para $d = 2$.

Assumimos que a hipótese de indução (4.2.4) [^5] vale para $d-1 > 1$, e seja $\varepsilon > 0$ e $K' > 1$. Temos [^5]:

$$
\prod_{i=1}^{d} x_i = \prod_{i=1}^{\lfloor d/2 \rfloor} x_i \cdot \prod_{i=\lfloor d/2 \rfloor + 1}^{d} x_i.
$$

Aproximaremos agora cada um dos termos no produto no lado direito de (4.2.6) [^5] por uma rede neural usando a hipótese de indução [^5].

Para simplificar, assumimos no seguinte que $\lceil \log_2(\lfloor d/2 \rfloor) \rceil = \lceil \log_2(d - \lfloor d/2 \rfloor) \rceil$ [^5]. O caso geral pode ser abordado via Proposição 3.16 [^5]. Pela hipótese de indução, existem então redes neurais $\Phi_{mult, 1}$ e $\Phi_{mult, 2}$, ambas com $\lceil \log_2(\lfloor d/2 \rfloor) \rceil$ camadas, tais que para todo $x_i$ com $|x_i| \leq K'$ para $i = 1, \dots, d$ [^5]:

$$
\left| \Phi_{mult, 1}(x_1, \dots, x_{\lfloor d/2 \rfloor}) - \prod_{i=1}^{\lfloor d/2 \rfloor} x_i \right| < \frac{\varepsilon}{4((K')^{\lfloor d/2 \rfloor} + \varepsilon)},
$$

$$
\left| \Phi_{mult, 2}(x_{\lfloor d/2 \rfloor + 1}, \dots, x_d) - \prod_{i=\lfloor d/2 \rfloor + 1}^{d} x_i \right| < \frac{\varepsilon}{4((K')^{\lfloor d/2 \rfloor} + \varepsilon)}.
$$

Pela Proposição 2.3 [^3], $\Phi_{mult, \varepsilon, d} := \Phi_{mult, \varepsilon/2, 2}(\Phi_{mult, 1}, \Phi_{mult, 2})$ é uma rede neural com $1 + \lceil \log_2(\lfloor d/2 \rfloor) \rceil = \lceil \log_2(d) \rceil$ camadas [^5]. Por construção, o tamanho de $\Phi_{mult, \varepsilon, d}$ não depende de $K'$ ou $\varepsilon$ [^5]. Assim, para completar a indução, resta apenas mostrar (4.2.4) [^5].

Para todo $a, b, c, d \in \mathbb{R}$, vale [^5]:

$$
|ab - cd| \leq |a| |b - d| + |d| |a - c|.
$$

Portanto, para $x_1, \dots, x_d$ com $|x_i| \leq K'$ para todo $i = 1, \dots, d$, temos que [^6]:

$$
\left| \prod_{i=1}^{d} x_i - \Phi_{mult, \varepsilon, d}(x_1, \dots, x_d) \right| = \left| \prod_{i=1}^{\lfloor d/2 \rfloor} x_i \cdot \prod_{i=\lfloor d/2 \rfloor + 1}^{d} x_i - \Phi_{mult, 1}(x_1, \dots, x_{\lfloor d/2 \rfloor}) \Phi_{mult, 2}(x_{\lfloor d/2 \rfloor + 1}, \dots, x_d) \right| \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} < \varepsilon.
$$

Isso completa a prova de (4.2.4) [^6].

O resultado geral segue usando a Proposição 2.3 [^3] para mostrar que a rede de multiplicação pode ser composta com uma rede neural composta pelos $\Phi_{S_{l,t_i,n}}$ para $i = 1, \dots, d$ [^6]. Como em nenhum passo acima o tamanho das redes individuais dependeu da precisão da aproximação, isso também é verdade para a rede final. $\blacksquare$

**Teorema 4.9:** Sejam $d, n, k \in \mathbb{N}$ tais que $0 < k \leq n$ e $n \geq 2$. Seja $q \geq 2$, e seja $\sigma$ sigmoidal de ordem $q$. Então existe $C$ tal que para toda $f \in C^k([0, 1]^d)$ e todo $N \in \mathbb{N}$, existe uma rede neural $\Phi_N$ com função de ativação $\sigma$, $\lceil \log_2(d) \rceil + \lceil \log_q(k-1) \rceil$ camadas, e tamanho limitado por $CN$, tal que [^6]:

$$
||f - \Phi_N||_{L^\infty([0, 1]^d)} \leq CN^{-k/d} ||f||_{C^k([0, 1]^d)}.
$$

*Prova:* Fixe $N \in \mathbb{N}$. Pelo Teorema 4.9 [^6], existem coeficientes $|c_i| < C ||f||_{L^\infty([0, 1]^d)}$ e $B_i \in \mathcal{B}_n$ para $i = 1, \dots, N$, tal que [^6]:

$$
\left| \left| f - \sum_{i=1}^{N} c_i B_i \right| \right|_{L^\infty([0, 1]^d)} \leq CN^{-k/d} ||f||_{C^k([0, 1]^d)}.
$$

Além disso, pela Proposição 4.8 [^4], para cada $i = 1, \dots, N$ existe uma rede neural $\Phi_{B_i}$ com $\lceil \log_2(d) \rceil + \lceil \log_q(k-1) \rceil$ camadas, e um tamanho fixo, que aproxima $B_i$ em $[-1, 1]^d \supset [0, 1]^d$ até um erro de $\varepsilon := N^{-k/d}/N$ [^6]. O tamanho de $\Phi_{B_i}$ é independente de $i$ e $N$ [^6].

Pela Proposição 2.3 [^3], existe uma rede neural $\Phi_N$ que aproxima uniformemente $\sum_{i=1}^{N} c_i B_i$ até um erro $\varepsilon$ em $[0, 1]^d$, e tem $\lceil \log_2(d) \rceil + \lceil \log_q(k-1) \rceil$ camadas [^6]. O tamanho desta rede é linear em $N$ (ver Exercício 4.11) [^6]. Isso conclui a prova. $\blacksquare$

### Conclusão
Em resumo, demonstramos que redes neurais com funções sigmoidais podem aproximar *B-splines* univariadas com redes de tamanho fixo. Além disso, redes neurais com funções sigmoidais de ordem superior conseguem aproximar funções suaves com a mesma precisão que as aproximações de *splines*, mantendo um número comparável de parâmetros. A profundidade da rede é influenciada pelo parâmetro de suavidade $k$, comportando-se como $O(\log(k))$, conforme a observação do Remark 4.4 [^6].

### Referências
[^1]: Definição de B-spline univariada cardinal e introdução ao capítulo.
[^2]: Definição de B-spline multivariada e o dicionário de B-splines.
[^3]: Definição de função sigmoidal e a Proposição 4.7.
[^4]: Proposição 4.8 e detalhes sobre a aproximação de B-splines multivariadas.
[^5]: Detalhes da prova da Proposição 4.8, incluindo a aproximação do produto de variáveis.
[^6]: Teorema 4.9 e a conclusão sobre a aproximação de funções suaves com redes neurais.
<!-- END -->