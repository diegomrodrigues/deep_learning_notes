## Establishing Bounds for Width, Depth, and Size of Linear Combination Networks

### Introdução
Este capítulo aprofunda a análise de **redes neurais ReLU** explorando os limites de sua complexidade em termos de largura, profundidade e tamanho, especialmente quando combinadas linearmente. Baseando-nos no framework introduzido no capítulo anterior [^1], focaremos em como essas propriedades se comportam ao somar ou combinar redes neurais. Este conhecimento é fundamental para entender a capacidade de aproximação e a eficiência das redes ReLU.

### Conceitos Fundamentais

Anteriormente, introduzimos as operações básicas de **cálculo ReLU**, incluindo a reprodução da identidade, composição e paralelização [^1]. Agora, nos concentraremos nas **combinações lineares** de redes neurais ReLU e estabeleceremos limites precisos para o tamanho da rede resultante [^1].

Considere $m \in \mathbb{N}$ e $(\Phi_j)_{j=1}^m$ como redes neurais ReLU com arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_j+1})$ [^1]. Assumimos que $d_{L_1+1} = ... = d_{L_m+1}$, ou seja, todas as redes $\Phi_1, ..., \Phi_m$ têm a mesma dimensão de saída [^1]. Para escalares $a_j \in \mathbb{R}$, desejamos construir uma rede neural ReLU $\sum_{j=1}^m a_j \Phi_j$ que realize a função [^1]:
$$(\mathbf{x}_1, ..., \mathbf{x}_m) \mapsto \sum_{j=1}^m a_j \Phi_j(\mathbf{x}_j)$$\nde $\mathbb{R}^{\sum_{j=1}^m d_0} \to \mathbb{R}^{d_{L_1+1}}$ [^1].

Essa construção corresponde à **paralelização** das redes $(\Phi_1, ..., \Phi_m)$ combinada com uma **transformação linear** que soma as saídas ponderadas [^1]. O seguinte lema formaliza essa ideia:

**Lemma 5.4 (Linear combinations)** [^1]. Seja $m \in \mathbb{N}$ e $(\Phi_j)_{j=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_j+1})$. Assuma que $d_{L_1+1} = ... = d_{L_m+1}$, seja $\mathbf{a} \in \mathbb{R}^m$ e defina $L_{max} := \max_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m a_j \Phi_j$ tal que $(\sum_{j=1}^m a_j \Phi_j)(\mathbf{x}) = \sum_{j=1}^m a_j \Phi_j(\mathbf{x})$ para todo $\mathbf{x} = (\mathbf{x}_j)_{j=1}^m \in \mathbb{R}^{\sum_{j=1}^m d_0}$. Além disso,

*   **Largura:**
    $$width\left(\sum_{j=1}^m a_j \Phi_j\right) \leq 2 \sum_{j=1}^m width(\Phi_j) \quad [^1](5.1.6a)$$
*   **Profundidade:**
    $$depth\left(\sum_{j=1}^m a_j \Phi_j\right) = \max_{j \leq m} depth(\Phi_j) \quad [^1](5.1.6b)$$
*   **Tamanho:**
    $$size\left(\sum_{j=1}^m a_j \Phi_j\right) \leq 2 \sum_{j=1}^m size(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j)d_0 + 1 \quad [^1](5.1.6c)$$

*Proof.* A construção de $\sum_{j=1}^m a_j \Phi_j$ é análoga à de $(\Phi_1, ..., \Phi_m)$ [^1]. Primeiro, definimos a combinação linear das redes neurais com a mesma profundidade [^1]. Então, os pesos são escolhidos como em (5.1.2) [^1], mas com a última transformação linear substituída por
$$(\alpha_1 W_1^{(L)} \quad ... \quad \alpha_m W_m^{(L)} \quad \sum_{j=1}^m \alpha_j b_j^{(L)}).$$ [^1]

Para profundidades gerais, definimos a soma das redes neurais como a soma das redes neurais estendidas $\Phi_j$ como em (5.1.3) [^1]. Todas as afirmações do lema seguem imediatamente desta construção [^1]. $\blacksquare$

Em casos onde $d_1 = ... = d_m =: d_0$ (todas as redes neurais têm a mesma dimensão de entrada), também consideramos **combinações lineares com entradas compartilhadas**, ou seja, uma rede neural realizando
$$\mathbf{x} \mapsto \sum_{j=1}^m a_j \Phi_j(\mathbf{x})$$
para $\mathbf{x} \in \mathbb{R}^{d_0}$ [^1].

### Conclusão

Este capítulo detalhou como estabelecer limites para a largura, profundidade e tamanho de redes neurais ReLU combinadas linearmente. O Lemma 5.4 fornece uma ferramenta crucial para analisar a complexidade de modelos mais elaborados construídos a partir de redes ReLU básicas [^1]. A capacidade de combinar redes com entradas compartilhadas também abre caminho para arquiteturas mais eficientes em termos de parâmetros [^1]. Estes resultados são essenciais para entender o trade-off entre complexidade e capacidade de aproximação ao trabalhar com redes neurais ReLU [^1].

### Referências
[^1]: Capítulo anterior do livro sobre Linear Combinations of Neural Networks.
<!-- END -->