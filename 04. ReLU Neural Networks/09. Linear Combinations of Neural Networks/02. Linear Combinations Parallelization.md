## Capítulo 7: Combinações Lineares em Redes Neurais ReLU

### Introdução
Este capítulo explora as combinações lineares de redes neurais ReLU, um conceito fundamental para manipular e construir redes mais complexas [^43]. Expandindo sobre as operações básicas de ReLU calculus discutidas na Seção 5.1, focaremos em como combinar linearmente redes neurais ReLU para realizar funções específicas [^43]. Como visto anteriormente, as operações de reprodução de identidade, composição e paralelização (Seções 5.1.1, 5.1.2 e 5.1.3) são essenciais, e agora adicionamos a combinação linear como mais uma ferramenta poderosa [^44, ^45].

### Conceitos Fundamentais
A combinação linear de redes neurais ReLU envolve a criação de uma nova rede neural que é uma soma ponderada de outras redes neurais ReLU [^45, ^47]. Formalmente, sejam $\Phi_1, ..., \Phi_m$ redes neurais ReLU com arquiteturas (ReLU; $d_0, ..., d_{L_i+1}$), respectivamente. Assumimos que $d_{L_1+1} = ... = d_{L_m+1}$, ou seja, todas as redes $\Phi_i$ têm a mesma dimensão de saída [^47]. Dados escalares $\alpha_j \in \mathbb{R}$, desejamos construir uma rede neural ReLU $\sum_{j=1}^m \alpha_j \Phi_j$ que realize a função:

$$\
(x_1, ..., x_m) \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x_j)\
$$

Essa transformação corresponde à **paralelização** das redes $(\Phi_1, ..., \Phi_m)$ composta com a transformação linear $(z_1, ..., z_m) \mapsto \sum_{j=1}^m \alpha_j z_j$ [^47].

**Lemma 5.4 (Linear combinations)** [^47]. Sejam $m \in \mathbb{N}$ e $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas (ReLU; $d_0, ..., d_{L_i+1}$), respectivamente. Assumimos que $d_{L_1+1} = ... = d_{L_m+1}$. Seja $\alpha \in \mathbb{R}^m$ e defina $L_{max} := \max_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m \alpha_j \Phi_j$ tal que $(\sum_{j=1}^m \alpha_j \Phi_j)(x) = \sum_{j=1}^m \alpha_j \Phi_j(x)$ para todo $x = (x_j)_{j=1}^m \in \mathbb{R}^{\sum_{j=1}^m d_0}$. Além disso:

*   **Largura (width):**
    $$\
    \text{width}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) \leq 2 \sum_{j=1}^m \text{width}(\Phi_j) \qquad (5.1.6a)\
    $$
*   **Profundidade (depth):**
    $$\
    \text{depth}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) = \max_{j \leq m} \text{depth}(\Phi_j) \qquad (5.1.6b)\
    $$
*   **Tamanho (size):**
    $$\
    \text{size}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) \leq 2 \sum_{j=1}^m \text{size}(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j) d_0 + 1 \qquad (5.1.6c)\
    $$

*Proof:* A construção de $\sum_{j=1}^m \alpha_j \Phi_j$ é análoga à da paralelização $(\Phi_1, ..., \Phi_m)$ (Seção 5.1.3), ou seja, definimos primeiro a combinação linear de redes neurais com a mesma profundidade. Os pesos são escolhidos como em (5.1.2), mas com a última transformação linear substituída por:

$$\
(\alpha_1 W_1^{(L)}, ..., \alpha_m W_m^{(L)}, \sum_{j=1}^m \alpha_j b_j^{(L)})\
$$

Para profundidades diferentes, definimos a soma das redes neurais como a soma das redes neurais estendidas $\Phi_i$ como em (5.1.3). Todas as afirmações do lemma seguem imediatamente dessa construção. $\blacksquare$

Em casos onde $d_1 = ... = d_m =: d_0$ (todas as redes neurais têm a mesma dimensão de entrada), também podemos considerar **combinações lineares com entradas compartilhadas**, ou seja, uma rede neural que realiza:

$$\
x \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x) \quad \text{para} \quad x \in \mathbb{R}^{d_0}\
$$

Essa construção requer um ajuste menor na construção da paralelização, conforme discutido no final da Seção 5.1.3 [^47]. O Lemma 5.4 permanece válido nesse caso, e não distinguimos na notação para combinações lineares com ou sem entradas compartilhadas [^47].

### Conclusão
A capacidade de formar combinações lineares de redes neurais ReLU é crucial para construir aproximações complexas e para analisar as propriedades teóricas dessas redes [^43]. Juntamente com a composição, paralelização e reprodução de identidade, a combinação linear fornece um conjunto completo de ferramentas para manipular redes neurais ReLU e explorar suas capacidades de aproximação [^44, ^45]. Os limites precisos no tamanho, largura e profundidade das redes combinadas linearmente são essenciais para entender a complexidade e a eficiência dessas construções [^47]. O tópico da combinação linear será retomado no próximo capítulo, quando explorarmos as aplicações dessa técnica em cenários mais avançados.

### Referências
[^43]: Capítulo 5, ReLU neural networks.
[^44]: Seção 5.1, Basic ReLU calculus.
[^45]: Seção 5.1.3, Parallelization.
[^47]: Seção 5.1.4, Linear combinations.
<!-- END -->