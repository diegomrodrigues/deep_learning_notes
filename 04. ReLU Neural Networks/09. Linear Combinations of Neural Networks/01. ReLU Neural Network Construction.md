## Construindo Redes Neurais ReLU para Realizar Combinações Lineares

### Introdução
Este capítulo explora a construção de redes neurais ReLU que realizam combinações lineares de outras redes neurais ReLU, expandindo os conceitos apresentados anteriormente sobre manipulação e combinação de redes neurais [^43, ^44]. O objetivo é formalizar como somar redes neurais mantendo o controle sobre a complexidade da rede resultante, um aspecto crucial para o desenvolvimento de modelos eficientes [^43]. Este capítulo se baseia nos resultados anteriores sobre a aproximação de funções por redes neurais, mas com foco nas propriedades específicas das funções ReLU, que são *piecewise linear* e exigem técnicas de análise diferentes das funções de ativação mais suaves [^43].

### Conceitos Fundamentais
A formalização da combinação linear de redes neurais ReLU começa com a definição precisa das operações básicas e suas propriedades. O contexto detalha as seguintes operações:
1.  **Reprodução da identidade:** Uma rede neural ReLU que reproduz a identidade exatamente, crucial para estender outras redes neurais e facilitar a composição [^44]. O Lemma 5.1 [^44] afirma que, para qualquer $L \in \mathbb{N}$, existe uma rede neural ReLU $\Phi_{id}$ tal que $\Phi_{id}(x) = x$ para todo $x \in \mathbb{R}^d$. Além disso, depth($\Phi_{id}$) = L, width($\Phi_{id}$) = 2d e size($\Phi_{id}$) = 2d(L+1).

    **Demonstração:**

    Escrevendo $I_d \in \mathbb{R}^{d \times d}$ para a matriz identidade, escolhemos os pesos:
    $$\
    (W^{(0)}, b^{(0)}), \dots, (W^{(L)}, b^{(L)}) := \left( \begin{pmatrix} I_d \\\\ -I_d \end{pmatrix}, 0 \right), \underbrace{(I_{2d}, 0), \dots, (I_{2d}, 0)}_{L-1 \text{ vezes}}, ((I_d, -I_d), 0)\
    $$
    Usando que $x = \sigma_{\text{ReLU}}(x) - \sigma_{\text{ReLU}}(-x)$ para todo $x \in \mathbb{R}$ e $\sigma_{\text{ReLU}}(x) = x$ para todo $x \geq 0$, é óbvio que a rede neural $\Phi_{id}$ associada aos pesos acima satisfaz a asserção do Lemma. $\blacksquare$

2.  **Composição:** A concatenação de duas redes neurais ReLU, onde a saída de uma rede serve como entrada para a outra [^44]. O Lemma 5.2 [^45] formaliza as propriedades da composição, estabelecendo limites para a largura, profundidade e tamanho da rede resultante em termos das redes originais.

    **Lemma 5.2 (Composição):** Sejam $\Phi_1, \Phi_2$ redes neurais com arquiteturas $(\sigma_{\text{ReLU}}; d_0, \dots, d_{L_1+1})$ e $(\sigma_{\text{ReLU}}; d_0, \dots, d_{L_2+1})$. Assuma que $d_{L_1+1} = d_0$. Então $\Phi_2 \circ \Phi_1(x) = \Phi_2 \cdot \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^d$. Além disso:

    *   width($\Phi_2 \circ \Phi_1$) $\leq$ max\\{width($\Phi_1$), width($\Phi_2$)\\}
    *   depth($\Phi_2 \circ \Phi_1$) = depth($\Phi_1$) + depth($\Phi_2$)
    *   size($\Phi_2 \circ \Phi_1$) $\leq$ size($\Phi_1$) + size($\Phi_2$) + $(d_{L_1} + 1)d$
    *   width($\Phi_2 \bullet \Phi_1$) $\leq$ 2max\\{width($\Phi_1$), width($\Phi_2$)\\}
    *   depth($\Phi_2 \bullet \Phi_1$) = depth($\Phi_1$) + depth($\Phi_2$) + 1
    *   size($\Phi_2 \bullet \Phi_1$) $\leq$ 2(size($\Phi_1$) + size($\Phi_2$))

3.  **Paralelização:** A execução de várias redes neurais independentemente em diferentes partes da entrada, com o objetivo de construir uma rede maior que realiza várias transformações simultaneamente [^45].

4.  **Combinações lineares:** A soma ponderada de duas ou mais redes neurais, onde o objetivo é criar uma nova rede que realize a combinação linear das funções representadas pelas redes originais [^47]. O Lemma 5.4 [^47] formaliza essa operação, fornecendo limites precisos para o tamanho da rede resultante.

A operação central para o tópico deste capítulo é a **combinação linear**. O Lemma 5.4 [^47] aborda especificamente a construção de redes neurais ReLU que realizam combinações lineares de outras redes.

**Lemma 5.4 (Combinações Lineares):** Sejam $m \in \mathbb{N}$ e $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{\text{ReLU}}; d_0, \dots, d_{L_i+1})$. Assuma que $d_{L_1+1} = \dots = d_{L_m+1}$, seja $\alpha \in \mathbb{R}^m$ e defina $L_{\text{max}} := \text{max}_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m \alpha_j \Phi_j$ tal que $(\sum_{j=1}^m \alpha_j \Phi_j)(x) = \sum_{j=1}^m \alpha_j \Phi_j(x)$ para todo $x = (x_i)_{i=1}^m \in \mathbb{R}^{\sum_{i=1}^m d_i}$. Além disso:

*   width($\sum_{j=1}^m \alpha_j \Phi_j$) $\leq 2\sum_{j=1}^m \text{width}(\Phi_j)$
*   depth($\sum_{j=1}^m \alpha_j \Phi_j$) = $\text{max}_{j \leq m} \text{depth}(\Phi_j)$
*   size($\sum_{j=1}^m \alpha_j \Phi_j$) $\leq 2\sum_{j=1}^m \text{size}(\Phi_j) + 2\sum_{j=1}^m (L_{\text{max}} - L_j) d_j + 1$

**Demonstração:**

A construção de $\sum_{j=1}^m \alpha_i \Phi_j$ é análoga à de $(\Phi_1, \dots, \Phi_m)$, isto é, primeiro definimos a combinação linear de redes neurais com a mesma profundidade. Então, os pesos são escolhidos como em (5.1.2) [^45], mas com a última transformação linear substituída por:

$$\
(\alpha_1 W_1^{(L)} \dots \alpha_m W_m^{(L)}), \sum_{j=1}^m \alpha_j b_j^{(L)}\
$$

Para profundidades gerais, definimos a soma das redes neurais como a soma das redes neurais estendidas $\Phi_i$ como em (5.1.3) [^45]. Todas as afirmações do Lemma seguem imediatamente desta construção. $\blacksquare$

Caso todas as redes neurais tenham a mesma dimensão de entrada ($d_1 = \dots = d_m =: d_0$), também consideramos combinações lineares com entradas compartilhadas, isto é, uma rede neural realizando

$$\
x \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x) \quad \text{para} \quad x \in \mathbb{R}^{d_0}\
$$

### Conclusão
Este capítulo estabeleceu um framework formal para a construção de redes neurais ReLU que realizam combinações lineares de outras redes neurais. Os limites precisos derivados para a complexidade da rede resultante são essenciais para entender as capacidades e limitações dessas construções [^43]. A capacidade de construir combinações lineares eficientes é fundamental para o desenvolvimento de arquiteturas de redes neurais mais complexas e para a aplicação de técnicas de *transfer learning*, onde o conhecimento aprendido em uma rede é transferido para outra [^43, ^44].

### Referências
[^43]: Capítulo 5: ReLU neural networks.
[^44]: Seção 5.1: Basic ReLU calculus.
[^45]: Seção 5.1.2: Composition.
[^47]: Seção 5.1.4: Linear combinations.

<!-- END -->