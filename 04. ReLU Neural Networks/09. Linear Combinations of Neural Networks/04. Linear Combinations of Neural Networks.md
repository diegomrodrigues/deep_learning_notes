## Capítulo 5.1.4: Combinações Lineares de Redes Neurais ReLU

### Introdução
Este capítulo expande o conceito de combinar redes neurais, focando especificamente em **combinações lineares de redes neurais ReLU**. Em continuidade ao estudo da **identidade** [^43, ^44], **composição** [^44, ^45] e **paralelização** [^45, ^46], este tópico explora como combinar redes neurais de forma linear, estabelecendo limites precisos no tamanho da rede resultante. Este estudo é crucial para manipular e construir redes neurais complexas a partir de componentes mais simples [^43]. O tópico atual se baseia nos conceitos de paralelização e transformações lineares para criar redes neurais que realizam somas ponderadas de funções implementadas por outras redes neurais.

### Conceitos Fundamentais

Considere $m \in \mathbb{N}$ e $(\Phi_j)_{j=1}^m$ como redes neurais ReLU, cada uma com arquiteturas $(\text{ReLU}; d_0, ..., d_{L_j+1})$ [^47]. Assume-se que $d_{L_1+1} = ... = d_{L_m+1}$, ou seja, todas as redes $\Phi_1, ..., \Phi_m$ possuem a mesma dimensão de saída. Para escalares $\alpha_j \in \mathbb{R}$, o objetivo é construir uma rede neural ReLU $\sum_{j=1}^m \alpha_j \Phi_j$ que realize a função:

$$\
\left(x_1, ..., x_m\right) \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x_j)
$$

Essa construção corresponde à **paralelização** $(\Phi_1, ..., \Phi_m)$ composta com a **transformação linear** $(z_1, ..., z_m) \mapsto \sum_{j=1}^m \alpha_j z_j$ [^47].

O seguinte resultado estabelece limites para a arquitetura da rede resultante:

**Lemma 5.4 (Combinações Lineares)**: Seja $m \in \mathbb{N}$ e $(\Phi_j)_{j=1}^m$ redes neurais com arquiteturas $(\text{ReLU}; d_0, ..., d_{L_j+1})$. Assuma que $d_{L_1+1} = ... = d_{L_m+1}$, seja $\alpha \in \mathbb{R}^m$ e defina $L_{max} := \max_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m \alpha_j \Phi_j$ tal que $\left(\sum_{j=1}^m \alpha_j \Phi_j\right)(x) = \sum_{j=1}^m \alpha_j \Phi_j(x)$ para todo $x = (x_j)_{j=1}^m \in \mathbb{R}^{\sum_{j=1}^m d_0}$. Além disso:

*   **Largura (Width)**:
    $$\
    \text{width}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) \leq 2 \sum_{j=1}^m \text{width}(\Phi_j)
    $$
    [^47]
*   **Profundidade (Depth)**:
    $$\
    \text{depth}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) = \max_{j \leq m} \text{depth}(\Phi_j)
    $$
    [^47]
*   **Tamanho (Size)**:
    $$\
    \text{size}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) \leq 2 \sum_{j=1}^m \text{size}(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j) d_0 + 1
    $$
    [^47]

*Proof:* A construção de $\sum_{j=1}^m \alpha_j \Phi_j$ é análoga à de $(\Phi_1, ..., \Phi_m)$ [^47]. Primeiro, define-se a combinação linear das redes neurais com a mesma profundidade. Os pesos são escolhidos como em (5.1.2) [^46], mas com a última transformação linear substituída por $(\alpha_1 W_1^{(L)} \dots \alpha_m W_m^{(L)})$. Para profundidades gerais, define-se a soma das redes neurais como a soma das redes estendidas $\Phi_j$ como em (5.1.3) [^46]. Todas as afirmações do lema seguem imediatamente desta construção. $\blacksquare$

Em casos onde $d_0^1 = \dots = d_0^m =: d_0$ (todas as redes neurais têm a mesma dimensão de entrada), também podemos considerar **combinações lineares com entradas compartilhadas**, ou seja, uma rede neural que realiza:

$$\
x \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x) \quad \text{for } x \in \mathbb{R}^{d_0}
$$
[^47]

Este cenário requer os mesmos ajustes menores discutidos no final da Seção 5.1.3 [^46]. O Lemma 5.4 permanece válido neste caso, e a notação não distingue combinações lineares com ou sem entradas compartilhadas [^47].

### Conclusão
Este capítulo formaliza a construção e análise de combinações lineares de redes neurais ReLU, fornecendo limites precisos na largura, profundidade e tamanho da rede resultante. Os resultados apresentados são fundamentais para entender como manipular e combinar redes neurais ReLU de forma eficiente, permitindo a construção de arquiteturas complexas com controle sobre sua complexidade. O conceito de combinações lineares com entradas compartilhadas demonstra uma forma eficiente de reutilizar representações aprendidas para realizar tarefas complexas.

### Referências
[^43]: Capítulo 5, p.43: "To formalize these results, we begin this chapter by adopting a framework from [172]. This framework enables the tracking of the number of network parameters for basic manipulations such as adding up or composing two neural networks. This will allow to bound the network complexity, when constructing more elaborate networks from simpler ones."
[^44]: Capítulo 5, p.44: "Linear combinations: Similarly, for the sum of two neural networks, we will give precise bounds on the size of the resulting neural network."
[^45]: Capítulo 5, p.45: "Lemma 5.2 (Composition). Let Ф1, Ф2 be neural networks with architectures (ReLU; do, ..., d11+1) and (ReLU; do, ..., d12+1). Assume d11+1 = do. Then Φ20 Φ1(x) = Φ2 • Ф1(х) = Φ2(Ф1(x)) for all x ∈ Rd."
[^46]: Capítulo 5, p.46: "If all input dimensions d] =···= dm =: do are the same, we will also use parallelization with shared inputs to realize the function x → (Ф1(x),..., Фm(x)) from Rdo → RdL1+1+...+dZm+1."
[^47]: Capítulo 5, p.47: "Let m ∈ N and let (i)₁ be ReLU neural networks that have architectures (ReLU; do, ..., d₁₁+1), respectively. Assume that d₁₁+1 = L1+1 = dmm+1, i.e., all Ф1,..., Фm have the same output dimension. For scalars aj ∈ R, we wish to construct a ReLU neural network Σj=1 α; Φ; realizing the function."
<!-- END -->