## Establishing Bounds for Width, Depth, and Size in Parallelized Networks

### Introdução
Este capítulo explora as propriedades das redes neurais paralelizadas, com foco na determinação de limites para a largura, profundidade e tamanho da rede resultante. O conceito de paralelização de redes neurais foi introduzido na Proposição 2.3 [^1] e refinado posteriormente. O Lemma 5.3 [^1] fornece um ponto de partida para analisar a complexidade dessas redes. Definimos $L_{max} := \max_{j \leq m} L_j$ [^1], onde $L_j$ representa a profundidade da $j$-ésima rede neural na paralelização. Este capítulo se baseia nos conceitos de ReLU calculus, introduzido na Seção 5.1 [^1], e nas operações básicas de manipulação de redes neurais.

### Conceitos Fundamentais
A **paralelização** de redes neurais envolve a construção de uma nova rede que executa várias redes menores em paralelo [^1]. Dada uma coleção de $m$ redes neurais $(\Phi_j)_{j=1}^{m}$ com arquiteturas (ReLU; $d_0, \dots, d_{L_j+1}$), o objetivo é construir uma rede $(\Phi_1, \dots, \Phi_m)$ que mapeie a concatenação dos inputs para a concatenação dos outputs [^1]:
$$(\Phi_1, \dots, \Phi_m): \mathbb{R}^{\sum_{j=1}^{m} d_0} \to \mathbb{R}^{\sum_{j=1}^{m} d_{L_j+1}}$$
$$(x_1, \dots, x_m) \mapsto (\Phi_1(x_1), \dots, \Phi_m(x_m)).$$
Para realizar isso, primeiro, assumimos que todas as redes têm a mesma profundidade, $L_1 = \dots = L_m = L$ [^1]. No caso geral, definimos $L_{max} := \max_{1 \leq i \leq m} L_i$ e $I := \{1 \leq i \leq m | L_i < L_{max}\}$ [^1]. Para $j \in I^c$, definimos $\tilde{\Phi}_j := \Phi_j$, e para $j \in I$:
$$\tilde{\Phi}_j := \Phi_{L_{max} - L_j}^{id} \circ \Phi_j,$$
onde $\Phi^{id}$ representa a rede identidade (Lemma 5.1 [^1]). Finalmente, construímos a rede paralelizada como [^1]:
$$(\Phi_1, \dots, \Phi_m) := (\tilde{\Phi}_1, \dots, \tilde{\Phi}_m).$$
O Lemma 5.3 [^1] resume as propriedades desta construção, estabelecendo limites para a largura, profundidade e tamanho da rede paralelizada:
*   **Largura:** A largura da rede paralelizada é limitada pela soma das larguras das redes individuais:
    $$width((\Phi_1, \dots, \Phi_m)) \leq 2 \sum_{j=1}^{m} width(\Phi_j).$$
*   **Profundidade:** A profundidade da rede paralelizada é determinada pela profundidade máxima das redes individuais:
    $$depth((\Phi_1, \dots, \Phi_m)) = \max_{j \leq m} depth(\Phi_j).$$
*   **Tamanho:** O tamanho da rede paralelizada é limitado pela soma dos tamanhos das redes individuais, juntamente com um termo de correção para redes de diferentes profundidades:
    $$size((\Phi_1, \dots, \Phi_m)) \leq 2 \sum_{j=1}^{m} size(\Phi_j) + 2 \sum_{j=1}^{m} (L_{max} - L_j) d_j + 1.$$
Se todas as dimensões de entrada $d_j$ forem iguais, $d_1 = \dots = d_m =: d_0$, podemos usar paralelização com entradas compartilhadas para realizar a função $x \mapsto (\Phi_1(x), \dots, \Phi_m(x))$ de $\mathbb{R}^{d_0}$ para $\mathbb{R}^{d_{L_1+1} + \dots + d_{L_m+1}}$ [^1]. Nesse caso, a matriz de blocos diagonais diag($W_1^{(0)}, \dots, W_m^{(0)}$) torna-se a matriz em $\mathbb{R}^{\sum_{j=1}^{m} d_j \times d_0}$ que empilha $W_1^{(0)}, \dots, W_m^{(0)}$ um em cima do outro [^1]. Podemos permitir que $\Phi_j$ use apenas algumas das entradas de $x$ como entrada [^1].

### Conclusão
Este capítulo forneceu uma análise detalhada da paralelização de redes neurais, estabelecendo limites importantes para a largura, profundidade e tamanho da rede resultante. O Lemma 5.3 [^1] consolida essas descobertas, fornecendo uma base teórica para entender a complexidade das redes neurais paralelizadas. A compreensão desses limites é crucial para projetar e otimizar arquiteturas de redes neurais paralelas, permitindo a criação de modelos mais eficientes e escaláveis. Além disso, a discussão sobre paralelização com entradas compartilhadas oferece insights valiosos sobre como otimizar ainda mais o uso de recursos computacionais.

### Referências
[^1]: Capítulo 5, ReLU neural networks.
<!-- END -->