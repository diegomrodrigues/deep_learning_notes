## Manipulação de Redes Neurais ReLU: Operações Fundamentais e Complexidade

### Introdução
Este capítulo formaliza a combinação e manipulação de redes neurais ReLU, refinando resultados anteriores ao adicionar limites no número de pesos que as redes neurais resultantes possuem [^43]. Quatro operações formam a base de todas as construções: reprodução de uma identidade, composição, paralelização e combinações lineares [^43]. Estas operações são cruciais para construir redes mais complexas a partir de blocos mais simples e para analisar a complexidade das redes resultantes. Em continuidade ao resultado já visto na Proposição 2.3 [^43], este capítulo aprofunda a análise sob a condição de que a função de ativação é a ReLU, refinando a Proposição 2.3 ao adicionar limites no número de pesos nas redes neurais resultantes [^43].

### Conceitos Fundamentais

#### 5.1.1 Identidade
A capacidade de reproduzir a **identidade** é fundamental para estender redes neurais e facilitar operações de composição eficientes [^44]. O Lemma 5.1 estabelece que, para qualquer profundidade $L \in \mathbb{N}$, existe uma rede neural ReLU $\Phi_{id}$ que computa a função identidade em $\mathbb{R}^d$, ou seja, $\Phi_{id}(x) = x$ para todo $x \in \mathbb{R}^d$ [^44].

**Lemma 5.1 (Identidade):** Seja $L \in \mathbb{N}$. Então, existe uma rede neural ReLU $\Phi_{id}$ tal que $\Phi_{id}(x) = x$ para todo $x \in \mathbb{R}^d$. Além disso, $depth(\Phi_{id}) = L$, $width(\Phi_{id}) = 2d$ e $size(\Phi_{id}) = 2d \cdot (L+1)$ [^44].

*Prova:* Para a matriz identidade $I_d \in \mathbb{R}^{d \times d}$, escolhemos os pesos
$$\n(W^{(0)}, b^{(0)}), \dots, (W^{(L)}, b^{(L)}) := \left( \begin{pmatrix} I_d \\\\ -I_d \end{pmatrix}, 0 \right), (I_{2d}, 0), \dots, (I_{2d}, 0), ((I_d, -I_d), 0)\n$$
com $L-1$ repetições de $(I_{2d}, 0)$ [^44]. Usando que $x = \sigma_{ReLU}(x) - \sigma_{ReLU}(-x)$ para todo $x \in \mathbb{R}$ e $\sigma_{ReLU}(x) = x$ para todo $x \geq 0$, é óbvio que a rede neural $\Phi_{id}$ associada aos pesos acima satisfaz a afirmação do lema [^44]. $\blacksquare$

É importante notar que a capacidade de representar exatamente a identidade não é compartilhada por funções de ativação sigmoidais, mas se mantém para funções de ativação polinomiais [^44].

#### 5.1.2 Composição
A **composição** de redes neurais permite construir funções mais complexas combinando redes mais simples [^44]. O Lemma 5.2 formaliza este conceito, fornecendo limites para a largura, profundidade e tamanho da rede resultante da composição [^45].

**Lemma 5.2 (Composição):** Sejam $\Phi_1$ e $\Phi_2$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_1+1})$ e $(\sigma_{ReLU}; d_0, \dots, d_{L_2+1})$, respectivamente. Assuma que $d_{L_1+1} = d_0$. Então, $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^{d_0}$. Além disso,
*   $width(\Phi_2 \circ \Phi_1) \leq max\\{width(\Phi_1), width(\Phi_2)\\}$
*   $depth(\Phi_2 \circ \Phi_1) = depth(\Phi_1) + depth(\Phi_2)$
*   $size(\Phi_2 \circ \Phi_1) \leq size(\Phi_1) + size(\Phi_2) + (d_0 + 1)d_0$

e
*   $width(\Phi_2 \bullet \Phi_1) \leq 2 max\\{width(\Phi_1), width(\Phi_2)\\}$
*   $depth(\Phi_2 \bullet \Phi_1) = depth(\Phi_1) + depth(\Phi_2) + 1$
*   $size(\Phi_2 \bullet \Phi_1) \leq 2(size(\Phi_1) + size(\Phi_2))$ [^45].

*Prova:* A igualdade $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ decorre imediatamente da construção [^45]. O mesmo vale para os limites de largura e profundidade [^45]. Para confirmar o limite de tamanho, notamos que $W_2^{(0)}W_1^{(L_1)} \in \mathbb{R}^{d_0 \times d_{L_1}}$ e, portanto, $W_2^{(0)}W_1^{(L_1)}$ tem no máximo $d_0^2$ entradas não nulas [^45]. Além disso, $W_2^{(0)}b_1^{(L_1)} + b_2^{(0)} \in \mathbb{R}^{d_0}$. Assim, a $L_1$-ésima camada de $\Phi_2 \circ \Phi_1(x)$ tem no máximo $d_0^2 \times (1 + d_0)$ entradas [^45]. O restante é óbvio a partir da construção [^45]. $\blacksquare$

#### 5.1.3 Paralelização
A **paralelização** permite combinar redes neurais para processar entradas independentes simultaneamente [^45]. O Lemma 5.3 formaliza essa operação, fornecendo limites para a largura, profundidade e tamanho da rede resultante [^46].

**Lemma 5.3 (Paralelização):** Seja $m \in \mathbb{N}$ e sejam $(\Phi_j)_{j=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_j+1})$, respectivamente. Então, a rede neural $(\Phi_1, \dots, \Phi_m)$ satisfaz
$$\n(\Phi_1, \dots, \Phi_m)(x) = (\Phi_1(x_1), \dots, \Phi_m(x_m))\n$$
para todo $x = (x_j)_{j=1}^m \in \mathbb{R}^{\sum_{j=1}^m d_0}$. Além disso, com $L_{max} := max_{j \leq m} L_j$, temos que
*   $width((\Phi_1, \dots, \Phi_m)) \leq 2 \sum_{j=1}^m width(\Phi_j)$
*   $depth((\Phi_1, \dots, \Phi_m)) = max_{j \leq m} depth(\Phi_j)$
*   $size((\Phi_1, \dots, \Phi_m)) \leq 2\sum_{j=1}^m size(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j)d_0 + 1$ [^46].

*Prova:* Todas as afirmações, exceto o limite no tamanho, seguem imediatamente da construção [^46]. Para obter o limite no tamanho, notamos que, pela construção, os tamanhos de $(\tilde{\Phi}_j)$, em (5.1.3), simplesmente serão somados [^46]. O tamanho de cada $\tilde{\Phi}_j$ pode ser limitado com o Lemma 5.2 [^46]. $\blacksquare$

#### 5.1.4 Combinações Lineares
As **combinações lineares** permitem criar novas redes neurais combinando linearmente as saídas de redes existentes [^47]. O Lemma 5.4 formaliza essa operação, fornecendo limites para a largura, profundidade e tamanho da rede resultante [^47].

**Lemma 5.4 (Combinações Lineares):** Seja $m \in \mathbb{N}$ e sejam $(\Phi_j)_{j=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_j+1})$, respectivamente. Assuma que $d_{L_1+1} = \dots = d_{L_m+1}$, seja $a \in \mathbb{R}^m$ e defina $L_{max} := max_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m a_j \Phi_j$ tal que $(\sum_{j=1}^m a_j \Phi_j)(x) = \sum_{j=1}^m a_j \Phi_j(x)$ para todo $x = (x_j)_{j=1}^m \in \mathbb{R}^{\sum_{j=1}^m d_0}$. Além disso,
*   $width(\sum_{j=1}^m a_j \Phi_j) \leq 2 \sum_{j=1}^m width(\Phi_j)$
*   $depth(\sum_{j=1}^m a_j \Phi_j) = max_{j \leq m} depth(\Phi_j)$
*   $size(\sum_{j=1}^m a_j \Phi_j) \leq 2 \sum_{j=1}^m size(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j)d_0 + 1$ [^47].

*Prova:* A construção de $\sum_{j=1}^m a_j \Phi_j$ é análoga à de $(\Phi_1, \dots, \Phi_m)$, ou seja, primeiro definimos a combinação linear de redes neurais com a mesma profundidade [^47]. Então, os pesos são escolhidos como em (5.1.2), mas com a última transformação linear substituída por $(a_1 W_1^{(L)}, \dots, a_m W_m^{(L)}), \sum_{j=1}^m a_j b_j^{(L)}$ [^47]. Para profundidades gerais, definimos a soma das redes neurais como a soma das redes neurais estendidas $\tilde{\Phi}_j$ como em (5.1.3) [^47]. Todas as afirmações do lema seguem imediatamente desta construção [^47]. $\blacksquare$

### Conclusão
Este capítulo formalizou as operações básicas para manipular redes neurais ReLU e forneceu limites para a complexidade das redes resultantes. Estes resultados são cruciais para entender a capacidade de aproximação e a complexidade de redes neurais mais avançadas construídas a partir destas operações fundamentais. As operações de identidade, composição, paralelização e combinações lineares fornecem as ferramentas necessárias para analisar e construir arquiteturas de redes neurais complexas com garantias teóricas sobre seu tamanho e profundidade. Os lemmas apresentados estabelecem limites importantes que serão utilizados em capítulos subsequentes para derivar resultados de aproximação para funções suaves e para analisar a eficiência de diferentes arquiteturas de redes neurais.

### Referências
[^43]: Seção 5.1, página 43
[^44]: Seção 5.1.1, página 44
[^45]: Seção 5.1.2, página 45
[^46]: Seção 5.1.3, página 46
[^47]: Seção 5.1.4, página 47
<!-- END -->