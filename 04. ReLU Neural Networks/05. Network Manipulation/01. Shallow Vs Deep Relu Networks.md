## Limitações de Redes Neurais ReLU Rasas

### Introdução
Este capítulo explora as limitações das redes neurais ReLU rasas em comparação com as redes neurais ReLU profundas. Como mencionado anteriormente, a chave para o sucesso em tarefas complexas reside na habilidade de modelar funções complexas [^1]. Um método tradicional para analisar as limitações de redes neurais ReLU é examinar o número de regiões lineares que essas funções podem gerar [^2].

### Conceitos Fundamentais
**Deficiência de regiões lineares:** Redes neurais ReLU rasas podem ser limitadas em sua capacidade de gerar um grande número de regiões lineares, o que pode ser um gargalo ao aproximar funções complexas.
*Definição de regiões lineares:* Seja $d \in \mathbb{N}$, $\Omega \subseteq \mathbb{R}^d$, e $f: \Omega \rightarrow \mathbb{R}$ uma função cpwl (contínua, linear por partes) [^2, 5]. Dizemos que $f$ tem $p \in \mathbb{N}$ peças (ou regiões lineares) se $p$ é o menor número de conjuntos abertos conectados $(\Omega_i)_{i=1}^p$ tal que $\bigcup_{i=1}^p \Omega_i = \Omega$, e $f|_{\Omega_i}$ é uma função afim para todo $i = 1, \dots, p$. Denotamos por $Pieces(f, \Omega) := p$ [^2]. Em uma dimensão, cada ponto onde $f$ não é diferenciável é chamado de ponto de quebra de $f$ [^2].

**Teorema 6.3** Seja $L \in \mathbb{N}$. Seja $\sigma$ cpwl com $p$ peças. Então, cada rede neural com arquitetura $(\sigma; 1, d_1, \dots, d_L, 1)$ tem no máximo $(p \cdot \text{width}(\Phi))^L$ peças [^2].

Este teorema mostra que existem limites para quantas peças podem ser criadas com uma determinada arquitetura. É notável que os efeitos da profundidade e da largura de uma rede neural são muito diferentes. Enquanto aumentar a largura pode aumentar polinomialmente o número de peças, aumentar a profundidade pode resultar em um aumento exponencial [^2]. Esta é uma primeira indicação da capacidade das redes neurais profundas [^2].

**Teorema 6.4.** Seja $d_0 \in \mathbb{N}$ e $f \in C^3([0, 1]^{d_0})$. Assuma que existe um segmento de reta $s \subseteq [0, 1]^{d_0}$ de comprimento positivo tal que $0 < c := \int_s \sqrt{|f\'\'(x)|} dx$. Então existe $C > 0$ dependendo unicamente de $c$, tal que para todas as redes neurais ReLU $\Phi: \mathbb{R}^{d_0} \rightarrow \mathbb{R}$ com $L$ camadas [^2]:
$$\
||f - \Phi||_{L^\infty([0, 1]^{d_0})} \geq c \cdot (2\text{width}(\Phi))^{-2L}.\
$$

Este teorema fornece um limite inferior para as taxas de aproximação alcançáveis em dependência da profundidade $L$. À medida que as funções alvo se tornam mais suaves, esperamos que possamos alcançar taxas de convergência mais rápidas [^2].

**Funções "Sawtooth":** Para demonstrar a capacidade de redes profundas, é útil considerar funções "sawtooth" [^2]. Estas são funções que oscilam rapidamente e podem ser criadas de forma eficiente com redes profundas.

**Teorema 6.6** Para cada $n \in \mathbb{N}$, existe uma rede neural $f \in N_1(\sigma_{ReLU}; n^2 + 3, 2)$ tal que para qualquer $g \in N_1(\sigma_{ReLU}; n, 2^{n-1})$ valha [^2]:
$$\
\int_0^1 |f(x) - g(x)| dx \geq \frac{1}{32}\
$$
A rede neural $f$ pode ter quadraticamente mais camadas que $g$, mas width($g$) = $2^{n-1}$ e width($f$) = 2. Portanto, o tamanho de $g$ pode ser exponencialmente maior que o tamanho de $f$, mas, no entanto, nenhum $g$ pode aproximar $f$. Assim, mesmo o aumento exponencial na largura não pode necessariamente compensar o aumento na profundidade [^2].

### Conclusão
Em resumo, redes neurais ReLU rasas podem ter limitações inerentes devido ao número limitado de regiões lineares que podem gerar. Redes neurais profundas, por outro lado, podem criar funções mais complexas com um número maior de regiões lineares, permitindo-lhes aproximar funções mais difíceis com maior precisão [^2]. A capacidade de criar funções "sawtooth" e outras funções complexas de forma eficiente é uma das razões pelas quais redes neurais profundas frequentemente superam as rasas em uma variedade de tarefas [^2].
<!-- END -->