## Simplicial Pieces in ReLU Networks

### Introdução
Este capítulo explora uma classe específica de funções *continuous piecewise linear (cpwl)* no contexto de **redes neurais ReLU**: aquelas cujas regiões nas quais a função é afim são *simplices*. Em contraste com o caso geral de funções cpwl, onde a complexidade da rede neural escala exponencialmente com o número de regiões [^53], mostraremos que, sob essa condição, é possível construir redes neurais com uma escala *linear* em relação ao número dessas regiões. Este resultado representa uma melhoria significativa em relação à dependência exponencial observada no Teorema 5.7 [^48].

### Conceitos Fundamentais

**Definição de n-simplex e Regular Triangulation**
Para analisar redes neurais ReLU com regiões afins simplificiais, é crucial definir formalmente o conceito de n-simplex e triangulação regular [^54].

**Definição 5.12** [^54]: *Sejam n ∈ No, d ∈ N e n ≤ d. Chamamos x0,...,xn ∈ Rd de **afim independentes** se e somente se n = 0 ou n ≥ 1 e os vetores x1 − x0,...,xn − x0 são linearmente independentes. Neste caso, chamamos co(x0,...,xn) := co({x0,...,xn}) de um **n-simplex**.*

Em outras palavras, um **n-simplex** é o *convex hull* de n pontos afim independentes no espaço Rd. Por exemplo, um 0-simplex é um ponto, um 1-simplex é um segmento de reta, um 2-simplex é um triângulo e um 3-simplex é um tetraedro.

**Definição 5.13** [^54]: *Sejam d ∈ N e Ω ⊆ Rd compacto. Seja T um conjunto finito de d-simplices, e para cada τ ∈ T, seja V(τ) ⊆ Ω com cardinalidade d + 1 tal que τ = co(V(τ)). Chamamos T de uma **regular triangulation** de Ω, se e somente se:*
*(i) ⋃τ∈Tτ = Ω,*
*(ii) para todo τ, τ' ∈ T, vale que τ ∩ τ' = co(V(τ) ∩ V(τ')).*

Em termos mais simples, uma *regular triangulation* de um conjunto compacto Ω é uma partição de Ω em um número finito de d-simplices, de modo que a interseção de quaisquer dois simplices seja também um *convex hull* dos vértices em comum.

**Node e Elemento da Triangulação**

*η ∈ V := ⋃τ∈TV(τ) é chamado um **node** (ou vértice)* [^54].
*τ ∈ T é chamado um **elemento da triangulação*** [^54].

**Teorema 5.14 e Limitações**

O Teorema 5.14 [^55] estabelece limites para o tamanho, largura e profundidade de uma rede neural ReLU que representa uma função cpwl com respeito a uma *regular triangulation* T, onde a função se anula na fronteira de Ω (f|∂Ω = 0). No entanto, esse teorema não explora explicitamente a dependência dos limites na dimensão d e no número máximo de elementos compartilhados por um único nó (kr), conforme definido na equação (5.3.1) [^55].

**Definição 5.18:** Uma regular triangulation T é chamada **localmente convexa** se e somente se ω(η) é convexo para todos os nós interiores η ∈ V ∩ Ω, onde ω(η) é o "patch" do nó η [^58].

**Teorema 5.19:** Seja d ∈ N, e seja Ω ⊆ Rd um domínio limitado. Seja T uma regular triangulation localmente convexa de Ω. Seja f : Ω → R cpwl com respeito a T e f|∂Ω = 0. Então, existe uma constante C > 0 (independente de d, f e T) e existe uma rede neural Φf : Ω → R tal que Φf = f,
size(Φf) ≤ C ⋅ (1 + d²kT)|T|,
width(Φf) ≤ C ⋅ (1 + d log(kT))|T|,
depth(Φf) ≤ C ⋅ (1 + log2(kT)).

### Conclusão

Este capítulo estabelece uma ponte entre a geometria simplicial e a arquitetura de redes neurais ReLU, demonstrando que funções cpwl definidas em *regular triangulations* podem ser representadas eficientemente por redes neurais, particularmente quando as triangulações são localmente convexas. O Teorema 5.19 [^58] oferece uma perspectiva valiosa sobre como a complexidade da triangulação (número de simplices e conectividade local) influencia o tamanho, a largura e a profundidade da rede neural necessária para representar a função cpwl. Essa análise fornece insights cruciais para o design de arquiteturas de redes neurais eficientes para aplicações onde a função a ser aproximada possui uma estrutura *piecewise linear* bem definida e relacionada a *simplices*.

### Referências
[^53]: Página 12, Capítulo 5
[^54]: Página 12, Capítulo 5
[^55]: Página 13, Capítulo 5
[^58]: Página 16, Capítulo 5
<!-- END -->