## Capítulo 5.1: Composição e Adição de Neurônios e o Número de Peças em Redes Neurais ReLU

### Introdução

Este capítulo explora as capacidades de aproximação de redes neurais ReLU, um tema fundamentalmente diferente das funções de ativação mais suaves [^1]. Ao contrário dos capítulos anteriores, que se baseavam na aproximação de derivadas da função de ativação para emular polinômios, a natureza *linear por partes* da ReLU exige uma abordagem distinta [^1]. Aqui, investigamos como a composição e a adição de neurônios, as operações básicas na construção de redes neurais, afetam o número de *regiões lineares* ou *peças* que essas redes podem representar [^1]. O objetivo é formalizar como combinar e manipular redes neurais ReLU, estendendo resultados anteriores e adicionando limites precisos ao número de parâmetros [^1].

### Conceitos Fundamentais

As redes neurais são construídas através da composição e adição de neurônios, operações que influenciam o número de *peças* de forma específica [^23].

**Summation:** Dada uma rede neural ReLU, a soma de duas funções cpwl (continuous piecewise linear) $f_1, f_2 : \Omega \rightarrow R$, onde $\Omega \subseteq R$, satisfaz [^23]:

$$\text{Pieces}(f_1 + f_2, \Omega) \leq \text{Pieces}(f_1, \Omega) + \text{Pieces}(f_2, \Omega) - 1 \qquad (6.1.1)$$

Essa relação se mantém porque a soma é afim em cada ponto onde $f_1$ e $f_2$ são afins. Portanto, a soma tem no máximo tantos pontos de quebra quanto $f_1$ e $f_2$ combinados [^23]. Além disso, o número de peças de uma função univariada é igual ao número de seus pontos de quebra mais um [^23].

**Composition:** Dada uma rede neural ReLU, a composição de duas funções $f_1 : R^d \rightarrow R$ e $f_2 : \Omega \rightarrow R^d$, onde $\Omega \subseteq R$, satisfaz [^23]:

$$\text{Pieces}(f_1 \circ f_2, \Omega) \leq \text{Pieces}(f_1, R^d) \cdot \text{Pieces}(f_2, \Omega) \qquad (6.1.2)$$

Isso ocorre porque, para cada uma das peças afins de $f_2$ (digamos, $A \subseteq R$), $f_2$ é constante ou injetora em $A$ [^23]. Se for constante, então $f_1 \circ f_2$ é constante [^23]. Se for injetora, então $\text{Pieces}(f_1 \circ f_2, A) = \text{Pieces}(f_1, f_2(A)) \leq \text{Pieces}(f_1, R^d)$ [^23]. Como isso vale para todas as peças de $f_2$, obtemos (6.1.2) [^23].

Essas considerações levam ao seguinte resultado, que segue o argumento de [226, Lemma 2.1] [^23]. É declarado para funções de ativação cpwl gerais, com a função de ativação ReLU correspondendo a $p = 2$ [^23].

**Teorema 6.3.** Seja $L \in N$. Seja $\sigma$ cpwl com $p$ peças. Então, cada rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ tem no máximo $(p \cdot \text{width}(\Phi))^L$ peças [^23].

*Prova:* A prova é feita por indução sobre a profundidade $L$ [^23]. Seja $L = 1$ e seja $\Phi : R \rightarrow R$ uma rede neural da arquitetura $(\sigma; 1, d_1, 1)$ [^23]. Então

$$\Phi(x) = \sum_{k=1}^{d_1} w_k^{(1)} \sigma(w_k^{(0)} x + b_k^{(0)}) + b^{(1)}$$

para $x \in R$ [^23]. Por (6.1.1), $\text{Pieces}(\Phi) \leq p \cdot \text{width}(\Phi)$ [^23].

Para o passo de indução, assuma que a afirmação valha para $L \in N$ e seja $\Phi : R \rightarrow R$ uma rede neural da arquitetura $(\sigma; 1, d_1, ..., d_{L+1}, 1)$ [^23]. Então, podemos escrever

$$\Phi(x) = \sum_{j=1}^{d_{L+1}} w_j \sigma(h_j(x)) + b$$

para $x \in R$, para alguns $w \in R^{d_{L+1}}$, $b \in R$, e onde cada $h_j$ é uma rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ [^23]. Usando a hipótese de indução, cada $\sigma \circ h_j$ tem no máximo $p \cdot (\text{width}(\Phi))^L$ peças afins [^23]. Portanto, $\Phi$ tem no máximo $\text{width}(\Phi) \cdot p \cdot (p \cdot \text{width}(\Phi))^L = (p \cdot \text{width}(\Phi))^{L+1}$ peças afins [^23]. Isso completa a prova [^23]. $\blacksquare$

### Conclusão

Este capítulo estabelece uma base para entender as capacidades de aproximação das redes neurais ReLU, focando em como as operações de composição e adição afetam o número de *peças* ou *regiões lineares* que essas redes podem representar [^1, 23]. O Teorema 6.3 fornece um limite superior para o número de peças que uma rede neural ReLU pode gerar, destacando a influência da profundidade e largura da rede [^23]. A análise subsequente investiga sob quais condições esses limites superiores são alcançáveis e como isso afeta a capacidade da rede de aproximar funções não lineares [^23].

### Referências
[^1]: Capítulo 5, ReLU neural networks
[^2]: Capítulo 5, ReLU neural networks
[^23]: Section 6.1 Upper bounds
<!-- END -->