## Capítulo 6: Limitações das Redes ReLU Superficiais

### Introdução
Nos capítulos anteriores, observamos resultados notáveis na aproximação de funções utilizando redes ReLU superficiais [^64]. No entanto, na prática, arquiteturas mais profundas são mais comuns. Para entender o porquê, este capítulo discute algumas potenciais limitações de redes ReLU superficiais em comparação com redes ReLU profundas [^64]. Tradicionalmente, uma abordagem para estudar as limitações de redes ReLU tem sido analisar o número de regiões lineares que essas funções podem gerar [^64].

### Conceitos Fundamentais
Definição 6.1 define o número de **peças (ou regiões lineares)** de uma função *cpwl* (contínua, linear por partes) [^64, ^64]. Uma função $f$ tem $p$ peças se $p$ é o menor número de conjuntos abertos conectados $(\Omega_i)_{i=1}^p$ tal que $\bigcup_{i=1}^p \Omega_i = \Omega$, e $f|_{\Omega_i}$ é uma função afim para todo $i = 1, ..., p$. Denotamos Pieces$(f, \Omega) := p$ [^64].  Para $d = 1$, chamamos cada ponto onde $f$ não é diferenciável de um **ponto de quebra** de $f$ [^64].

Para obter uma aproximação *cpwl* precisa de uma função, a função de aproximação precisa ter muitas peças. O Teorema 6.2 quantifica essa afirmação [^64]:

**Teorema 6.2.** Seja $-\infty < a < b < \infty$ e $f \in C^3([a, b])$ tal que $f$ não é afim. Então existe uma constante $c > 0$ dependendo apenas de $\int_a^b \sqrt{|f''(x)|} dx$ tal que
$$||g - f||_{L^\infty([a, b])} > c p^{-2}$$
para todo *cpwl* $g$ com no máximo $p \in \mathbb{N}$ peças [^64].

O Teorema 6.2 implica que para redes neurais ReLU, precisamos de arquiteturas que permitam muitas peças se quisermos aproximar funções não lineares com alta precisão [^64]. Mas quantas peças podemos criar para uma profundidade e largura fixas? O Teorema 6.3 estabelece um limite superior teórico simples [^65]:

**Teorema 6.3.** Seja $L \in \mathbb{N}$. Seja $\sigma$ *cpwl* com $p$ peças. Então, cada rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ tem no máximo $(p \cdot \text{width}(\Phi))^L$ peças [^65].

*Prova.* A prova é feita por indução sobre a profundidade $L$. Para $L = 1$, seja $\Phi: \mathbb{R} \to \mathbb{R}$ uma rede neural de arquitetura $(\sigma; 1, d_1, 1)$. Então
$$\Phi(x) = \sum_{k=1}^{d_1} w_k^{(1)} \sigma(w_k^{(0)} x + b_k^{(0)}) + b^{(1)} \quad \text{para } x \in \mathbb{R}$$
para certos $w^{(0)}, w^{(1)}, b^{(0)} \in \mathbb{R}^{d_1}$ e $b^{(1)} \in \mathbb{R}$. Pelo Teorema 6.1, Pieces$(\Phi) \le p \cdot \text{width}(\Phi)$ [^65].

Para o passo de indução, assuma que a afirmação vale para $L \in \mathbb{N}$, e seja $\Phi: \mathbb{R} \to \mathbb{R}$ uma rede neural de arquitetura $(\sigma; 1, d_1, ..., d_{L+1}, 1)$. Então podemos escrever
$$\Phi(x) = \sum_{j=1}^{d_{L+1}} w_j \sigma(h_j(x)) + b \quad \text{para } x \in \mathbb{R}$$
para alguns $w \in \mathbb{R}^{d_{L+1}}$, $b \in \mathbb{R}$, e onde cada $h_j$ é uma rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$. Usando a hipótese de indução, cada $\sigma \circ h_j$ tem no máximo $p \cdot (p \cdot \text{width}(\Phi))^L$ peças afins. Portanto, $\Phi$ tem no máximo width$(\Phi) \cdot p \cdot (p \cdot \text{width}(\Phi))^L = (p \cdot \text{width}(\Phi))^{L+1}$ peças afins. Isso completa a prova. $\blacksquare$

O Teorema 6.3 mostra que existem limites para quantas peças podem ser criadas com uma certa arquitetura [^65]. É notável que os efeitos da profundidade e da largura de uma rede neural são vastamente diferentes [^65]. Enquanto aumentar a largura pode aumentar polinomialmente o número de peças, aumentar a profundidade pode resultar em um aumento exponencial [^65]. Esta é uma primeira indicação da proeza da profundidade de redes neurais [^65]. Para entender o efeito disso no problema de aproximação, aplicamos o limite do Teorema 6.3 ao Teorema 6.2 [^66]:

**Teorema 6.4.** Seja $d_0 \in \mathbb{N}$ e $f \in C^3([0,1]^{d_0})$. Assuma que existe um segmento de linha $s \subseteq [0,1]^{d_0}$ de comprimento positivo tal que $0 < c := \int_s \sqrt{|f''(x)|} dx$. Então existe $C > 0$ dependendo apenas de $c$, tal que para todas as redes neurais ReLU $\Phi: \mathbb{R}^{d_0} \to \mathbb{R}$ com $L$ camadas,
$$||f - \Phi||_{L^\infty([0,1]^{d_0})} \ge c \cdot (2 \text{width}(\Phi))^{-2L}$$

O Teorema 6.4 fornece um limite inferior nas taxas de aproximação alcançáveis em dependência da profundidade $L$ [^66]. À medida que as funções alvo se tornam mais suaves, esperamos alcançar taxas de convergência mais rápidas [^66]. No entanto, sem aumentar a profundidade, parece ser impossível aproveitar tal suavidade adicional [^67]. Esta observação indica fortemente que arquiteturas mais profundas podem ser superiores [^67].

### Conclusão
Este capítulo demonstrou que, embora as redes ReLU superficiais tenham mostrado resultados notáveis na aproximação de funções, elas enfrentam limitações em comparação com as redes profundas. A análise do número de regiões lineares geradas revela que as redes superficiais podem ter dificuldade em aproximar funções não lineares com alta precisão, especialmente quando a profundidade é limitada.

A análise do número de regiões lineares geradas revela que a complexidade do modelo cresce exponencialmente com a profundidade [^65].

<!-- END -->