## A Theorem on the Number of Pieces in ReLU Neural Networks

### Introdução
Este capítulo explora um teorema fundamental que limita o número de regiões afins em redes neurais ReLU. Como mencionado no início do Capítulo 5 [^1], a função de ativação ReLU, devido à sua simplicidade e à sua capacidade de mitigar o problema do desaparecimento e explosão de gradientes, é amplamente utilizada na prática. No entanto, a natureza *piecewise linear* da ReLU exige uma análise diferente daquela utilizada para funções de ativação mais suaves [^1]. O Teorema 6.3 [^65] fornece um limite superior para o número de regiões afins que uma rede neural ReLU pode gerar, estabelecendo um limite no poder expressivo destas redes.

### Conceitos Fundamentais
O Teorema 6.3 [^65] é central para entender as limitações de redes neurais ReLU com uma dada arquitetura. Ele afirma:

**Teorema 6.3.** Seja $L \in \mathbb{N}$. Seja $\sigma$ uma função cpwl (*continuous piecewise linear*) com $p$ peças. Então, toda rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ tem no máximo $(p \cdot width(\Phi))^L$ peças [^65].

Aqui, $\Phi$ representa a rede neural, $L$ é o número de camadas, e $width(\Phi)$ denota a largura máxima da rede (i.e., o número máximo de neurônios em qualquer camada) [^65]. O número $p$ corresponde ao número de regiões lineares da função de ativação $\sigma$ [^65]. Para a função ReLU, $p = 2$, pois a ReLU tem duas regiões lineares: uma onde a saída é zero e outra onde a saída é igual à entrada [^65].

*Prova do Teorema 6.3* [^65]: A prova é feita por indução sobre a profundidade $L$. Para $L = 1$, considere uma rede neural $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ com arquitetura $(\sigma; 1, d_1, 1)$. Então,

$$\Phi(x) = \sum_{k=1}^{d_1} w_k \sigma(w_k^{(0)} x + b_k^{(0)}) + b^{(1)}$$

para $x \in \mathbb{R}$ [^65]. Pela equação (6.1.1) [^65], o número de peças de $\Phi$ é no máximo $p \cdot width(\Phi)$ [^65].

Para o passo indutivo, assuma que a afirmação é válida para $L \in \mathbb{N}$, e considere $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ uma rede neural com arquitetura $(\sigma; 1, d_1, ..., d_{L+1}, 1)$ [^65]. Então, podemos escrever

$$\Phi(x) = \sum_{j=1}^{d_{L+1}} w_j \sigma(h_j(x)) + b$$

para $x \in \mathbb{R}$ [^65], para algum $w \in \mathbb{R}^{d_{L+1}}$, $b \in \mathbb{R}$, e onde cada $h_j$ é uma rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ [^65]. Usando a hipótese indutiva, cada $\sigma \circ h_j$ tem no máximo $p \cdot (p \cdot width(\Phi))^L$ peças afins [^65]. Portanto, $\Phi$ tem no máximo $(p \cdot width(\Phi))^{L+1}$ peças afins [^65]. Isto completa a prova. $\blacksquare$

O teorema implica que o número de regiões lineares cresce exponencialmente com a profundidade $L$, mas apenas polinomialmente com a largura da rede [^65].

### Conclusão
O Teorema 6.3 [^65] fornece um limite superior importante para o número de regiões afins que uma rede neural ReLU pode gerar. Este limite é crucial para entender o poder expressivo e as limitações de tais redes. A prova do teorema, baseada na composição e adição de neurônios, destaca como a profundidade e a largura da rede influenciam a complexidade da função que a rede pode representar. Este resultado teórico é essencial para projetar arquiteturas de redes neurais ReLU adequadas para tarefas específicas, equilibrando a capacidade de expressar funções complexas com a necessidade de evitar o overfitting.

### Referências
[^1]: Página 43 do documento original.
[^65]: Página 65 do documento original.
<!-- END -->