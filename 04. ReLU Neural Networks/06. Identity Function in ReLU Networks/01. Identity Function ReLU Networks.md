## Expressing Identity with ReLU Networks

### Introdução
Este capítulo explora a representação da **função identidade** em redes neurais ReLU de profundidade *L* [^1]. O objetivo é demonstrar que é possível construir uma rede neural ReLU que reproduza exatamente a entrada para todos os *x* ∈ R^d. A rede neural ReLU, denotada como ΦidL,d, deve satisfazer ΦidL,d(x) = x para todo *x* ∈ R^d, com *depth*(ΦidL,d) = *L*, *width*(ΦidL,d) = 2d e *size*(ΦidL,d) = 2d ⋅ (*L* + 1). A capacidade de representar a função identidade com precisão é crucial para estender redes neurais e facilitar operações de composição eficientes [^2].

### Conceitos Fundamentais
O principal resultado deste capítulo é formalizado pelo seguinte lema:

**Lemma 5.1 (Identity)** [^1]. *Seja L ∈ N. Então, existe uma rede neural ReLU ΦidL,d tal que ΦidL,d(x) = x para todo x ∈ Rd. Além disso, depth(ΦidL,d) = L, width(ΦidL,d) = 2d e size(ΦidL,d) = 2d ⋅ (L + 1).*

**Prova:** Para construir essa rede, representamos a matriz identidade como *Id* ∈ R^(d×d) e definimos os pesos da seguinte forma [^1]:

$$
(W^{(0)}, b^{(0)}),..., (W^{(L)}, b^{(L)}) :=
\begin{pmatrix}
\begin{pmatrix}
I_d \\\\ -I_d
\end{pmatrix}, 0
\end{pmatrix},
(I_{2d}, 0),..., (I_{2d}, 0),
\begin{pmatrix}
(I_d, -I_d), 0
\end{pmatrix}.
$$

Onde a matriz `(I_{2d}, 0)` se repete *L*-1 vezes.

A prova se baseia na propriedade fundamental da função ReLU: *x* = σReLU(*x*) - σReLU(-*x*) para todo *x* ∈ R, e σReLU(*x*) = *x* para todo *x* ≥ 0 [^1]. Com essa construção, fica evidente que a rede neural Φid associada aos pesos definidos acima satisfaz a afirmação do lema [^1].

É importante notar que a capacidade de representar a identidade exatamente não é compartilhada por funções de ativação sigmoidais. No entanto, essa propriedade se mantém para funções de ativação polinomiais [^1].

### Conclusão
A capacidade de expressar a função identidade com precisão em redes neurais ReLU é um resultado fundamental. A construção apresentada no Lemma 5.1 fornece uma rede neural de profundidade *L*, largura 2*d* e tamanho 2*d* ⋅ (*L* + 1) que reproduz exatamente a entrada para todos os *x* ∈ R^d. Esse resultado é crucial para estender redes neurais e facilitar operações de composição eficientes, como será explorado nas seções subsequentes [^2]. A exata representação da função identidade  é um ponto chave para a manipulação e combinação de redes neurais ReLU, abrindo caminho para a construção de redes mais complexas a partir de componentes mais simples [^2].

### Referências
[^1]: Capítulo 5, ReLU neural networks.
[^2]: Seção 5.1, Basic ReLU calculus.

<!-- END -->