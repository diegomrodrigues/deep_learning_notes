## 5.1 Basic ReLU Calculus: Combining and Manipulating ReLU Networks

### Introdução
Este capítulo formaliza como combinar e manipular redes neurais ReLU, expandindo resultados anteriores e adicionando limites ao número de pesos nas redes neurais resultantes [^43]. As quatro operações fundamentais que formam a base de todas as construções são: reprodução de uma identidade, composição, paralelização e combinações lineares [^43].

### Conceitos Fundamentais

#### 5.1.1 Identidade
Começamos expressando a **identidade** em $\mathbb{R}^d$ como uma rede neural de profundidade $L \in \mathbb{N}$ [^44].

**Lemma 5.1 (Identidade)**. Seja $L \in \mathbb{N}$. Então, existe uma rede neural ReLU $\Phi_{id}: \mathbb{R}^d \to \mathbb{R}^d$ tal que $\Phi_{id}(x) = x$ para todo $x \in \mathbb{R}^d$. Além disso, $depth(\Phi_{id}) = L$, $width(\Phi_{id}) = 2d$ e $size(\Phi_{id}) = 2d \cdot (L+1)$ [^44].

*Proof*. Escrevendo $I_d \in \mathbb{R}^{d \times d}$ para a matriz identidade, escolhemos os pesos
$$\
(W^{(0)}, b^{(0)}), \dots, (W^{(L)}, b^{(L)}) := \left( \begin{pmatrix} I_d \\\\ -I_d \end{pmatrix}, 0 \right), \underbrace{(I_{2d}, 0), \dots, (I_{2d}, 0)}_{L-1 \text{ vezes}}, ((I_d, -I_d), 0)\
$$
Usando que $x = \sigma_{ReLU}(x) - \sigma_{ReLU}(-x)$ para todo $x \in \mathbb{R}$ e $\sigma_{ReLU}(x) = x$ para todo $x \ge 0$, é óbvio que a rede neural $\Phi_{id}$ associada aos pesos acima satisfaz a afirmação do lema. $\blacksquare$

É importante notar que a propriedade de representar exatamente a identidade não é compartilhada por funções de ativação sigmoidal. No entanto, vale para funções de ativação polinomial [^44].

#### 5.1.2 Composição
Assumimos que temos duas redes neurais $\Phi_1, \Phi_2$ com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_1+1})$ e $(\sigma_{ReLU}; d_0, \dots, d_{L_2+1})$, respectivamente. Além disso, assumimos que elas têm pesos e biases dados por $(W_1^{(0)}, b_1^{(0)}), \dots, (W_1^{(L_1)}, b_1^{(L_1)})$ e $(W_2^{(0)}, b_2^{(0)}), \dots, (W_2^{(L_2)}, b_2^{(L_2)})$, respectivamente [^44]. Se a dimensão de saída $d_{L_1+1}$ de $\Phi_1$ é igual à dimensão de entrada $d_0$ de $\Phi_2$, podemos definir dois tipos de concatenações:

*   Primeiro, $\Phi_2 \circ \Phi_1$ é a rede neural com pesos e biases dados por
    $$\
    (W_1^{(0)}, b_1^{(0)}), \dots, (W_1^{(L_1-1)}, b_1^{(L_1-1)}), (W_2^{(0)}W_1^{(L_1)}, W_2^{(0)}b_1^{(L_1)} + b_2^{(0)}), (W_2^{(1)}, b_2^{(1)}), \dots, (W_2^{(L_2)}, b_2^{(L_2)}).\
    $$
*   Segundo, $\Phi_2 \bullet \Phi_1$ é a rede neural definida como $\Phi_2 \circ \Phi_{id} \circ \Phi_1$. Em termos de pesos e biases, $\Phi_2 \bullet \Phi_1$ é dado por
    $$\
    (W_1^{(0)}, b_1^{(0)}), \dots, (W_1^{(L_1-1)}, b_1^{(L_1-1)}), \left( \begin{pmatrix} W_1^{(L_1)} \\\\ -W_1^{(L_1)} \end{pmatrix}, \begin{pmatrix} b_1^{(L_1)} \\\\ -b_1^{(L_1)} \end{pmatrix} \right), (W_2^{(0)}, b_2^{(0)}), \dots, (W_2^{(L_2)}, b_2^{(L_2)}).\
    $$

**Lemma 5.2 (Composição)**. Sejam $\Phi_1, \Phi_2$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_1+1})$ e $(\sigma_{ReLU}; d_0, \dots, d_{L_2+1})$. Assumimos que $d_{L_1+1} = d_0$. Então $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^{d_0}$. Além disso,
$$\
width(\Phi_2 \circ \Phi_1) \le \max\\{width(\Phi_1), width(\Phi_2)\\},\
$$
$$\
depth(\Phi_2 \circ \Phi_1) = depth(\Phi_1) + depth(\Phi_2),\
$$
$$\
size(\Phi_2 \circ \Phi_1) \le size(\Phi_1) + size(\Phi_2) + (d_{L_1} + 1)d_{L_2},\
$$
e
$$\
width(\Phi_2 \bullet \Phi_1) \le 2\max\\{width(\Phi_1), width(\Phi_2)\\},\
$$
$$\
depth(\Phi_2 \bullet \Phi_1) = depth(\Phi_1) + depth(\Phi_2) + 1,\
$$
$$\
size(\Phi_2 \bullet \Phi_1) \le 2(size(\Phi_1) + size(\Phi_2)).\
$$

*Proof*. O fato de que $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^{d_0}$ segue imediatamente da construção. O mesmo pode ser dito para os limites de largura e profundidade. Para confirmar o limite de tamanho, observamos que $W_2^{(0)}W_1^{(L_1)} \in \mathbb{R}^{d_{L_2} \times d_{L_1}}$ e, portanto, $W_2^{(0)}W_1^{(L_1)}$ tem no máximo $d_{L_2}^2 \times d_{L_1}$ entradas (não nulas). Além disso, $W_2^{(0)}b_1^{(L_1)} + b_2^{(0)} \in \mathbb{R}^{d_{L_2}}$. Assim, a $L_1$-ésima camada de $\Phi_2 \circ \Phi_1(x)$ tem no máximo $d_{L_2}^2 \times (1 + d_{L_1})$ entradas. O resto é óbvio da construção. $\blacksquare$

Interpretando transformações lineares como redes neurais de profundidade 0, o lema anterior também é válido caso $\Phi_1$ ou $\Phi_2$ sejam um mapeamento linear [^45].

#### 5.1.3 Paralelização
Desejamos colocar redes neurais em paralelo. Sejam $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$, respectivamente. Procedemos para construir uma rede neural $(\Phi_1, \dots, \Phi_m)$ tal que
$$\
(\Phi_1, \dots, \Phi_m): \mathbb{R}^{\sum_{i=1}^m d_0} \to \mathbb{R}^{\sum_{i=1}^m d_{L_i+1}}\
$$
$$\
(x_1, \dots, x_m) \mapsto (\Phi_1(x_1), \dots, \Phi_m(x_m)).\
$$
Para fazer isso, primeiro assumimos que $L_1 = \dots = L_m = L$ e definimos $(\Phi_1, \dots, \Phi_m)$ através da seguinte sequência de tuplas peso-bias:
$$\
\begin{pmatrix} W_1^{(0)} & & \\\\ & \ddots & \\\\ & & W_m^{(0)} \end{pmatrix}, \begin{pmatrix} b_1^{(0)} \\\\ \vdots \\\\ b_m^{(0)} \end{pmatrix}, \dots, \begin{pmatrix} W_1^{(L)} & & \\\\ & \ddots & \\\\ & & W_m^{(L)} \end{pmatrix}, \begin{pmatrix} b_1^{(L)} \\\\ \vdots \\\\ b_m^{(L)} \end{pmatrix},\
$$
onde essas matrizes são entendidas como bloco-diagonal preenchidas com zeros. Para o caso geral onde os $\Phi_j$ podem ter profundidades diferentes, seja $L_{max} := \max_{1 \le i \le m} L_i$ e $I := \\{1 \le i \le m | L_i < L_{max}\\}$. Para $j \in I^c$, defina $\tilde{\Phi}_j := \Phi_j$, e para cada $j \in I$
$$\
\tilde{\Phi}_j := \Phi_{L_{max} - L_j} \circ \Phi_j.\
$$
Finalmente,
$$\
(\Phi_1, \dots, \Phi_m) := (\tilde{\Phi}_1, \dots, \tilde{\Phi}_m).\
$$

**Lemma 5.3 (Paralelização)**. Sejam $m \in \mathbb{N}$ e $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$, respectivamente. Então a rede neural $(\Phi_1, \dots, \Phi_m)$ satisfaz
$$\
(\Phi_1, \dots, \Phi_m)(x) = (\Phi_1(x_1), \dots, \Phi_m(x_m)) \text{ para todo } x \in \mathbb{R}^{\sum_{i=1}^m d_0}.\
$$
Além disso, com $L_{max} := \max_{j \le m} L_j$, vale que
$$\
width((\Phi_1, \dots, \Phi_m)) \le 2 \sum_{j=1}^m width(\Phi_j),\
$$
$$\
depth((\Phi_1, \dots, \Phi_m)) = \max_{j \le m} depth(\Phi_j),\
$$
$$\
size((\Phi_1, \dots, \Phi_m)) \le 2 \sum_{j=1}^m size(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j)d_j + 1.\
$$

*Proof*. Todas as afirmações, exceto o limite no tamanho, seguem imediatamente da construção. Para obter o limite no tamanho, notamos que, por construção, os tamanhos dos $(\tilde{\Phi}_i)$ em (5.1.3) serão simplesmente adicionados. O tamanho de cada $\tilde{\Phi}_i$ pode ser limitado com o Lemma 5.2. $\blacksquare$

Se todas as dimensões de entrada $d_1 = \dots = d_m =: d_0$ são as mesmas, também usaremos a paralelização com entradas compartilhadas para realizar a função $x \mapsto (\Phi_1(x), \dots, \Phi_m(x))$ de $\mathbb{R}^{d_0} \to \mathbb{R}^{d_{L_1+1} + \dots + d_{L_m+1}}$. Em termos da construção (5.1.2), a única mudança necessária é que a matriz bloco-diagonal $diag(W_1^{(0)}, \dots, W_m^{(0)})$ se torna a matriz em $\mathbb{R}^{\sum_{i=1}^m d_i \times d_0}$ que empilha $W_1^{(0)}, \dots, W_m^{(0)}$ um em cima do outro. Da mesma forma, permitiremos que $\Phi_j$ receba apenas algumas das entradas de $x$ como entrada. Para a paralelização com entradas compartilhadas, usaremos a mesma notação $(\Phi_j)_{i=1}^m$ como antes, onde o significado preciso sempre será claro pelo contexto. Note que o Lemma 5.3 permanece válido neste caso [^46].

#### 5.1.4 Combinações Lineares
Sejam $m \in \mathbb{N}$ e $(\Phi_i)_{i=1}^m$ redes neurais ReLU que possuem arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$, respectivamente. Assumimos que $d_{L_1+1} = \dots = d_{L_m+1}$, ou seja, todos $\Phi_1, \dots, \Phi_m$ têm a mesma dimensão de saída. Para escalares $a_j \in \mathbb{R}$, desejamos construir uma rede neural ReLU $\sum_{j=1}^m a_j \Phi_j$ realizando a função
$$\
\mathbb{R}^{\sum_{j=1}^m d_0} \to \mathbb{R}^{d_{L_1+1}}\
$$
$$\
(x_1, \dots, x_m) \mapsto \sum_{j=1}^m a_j \Phi_j(x_j).\
$$
Isto corresponde à paralelização $(\Phi_1, \dots, \Phi_m)$ composta com a transformação linear $(z_1, \dots, z_m) \mapsto \sum_{j=1}^m a_j z_j$. O seguinte resultado se mantém [^47].

**Lemma 5.4 (Combinações Lineares)**. Sejam $m \in \mathbb{N}$ e $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$, respectivamente. Assumimos que $d_{L_1+1} = \dots = d_{L_m+1}$, seja $a \in \mathbb{R}^m$ e defina $L_{max} := \max_{j \le m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m a_j \Phi_j$ tal que $(\sum_{j=1}^m a_j \Phi_j)(x) = \sum_{j=1}^m a_j \Phi_j(x)$ para todo $x = (x_i)_{i=1}^m \in \mathbb{R}^{\sum_{i=1}^m d_0}$. Além disso,
$$\
width \left( \sum_{j=1}^m a_j \Phi_j \right) \le 2 \sum_{j=1}^m width(\Phi_j),\
$$
$$\
depth \left( \sum_{j=1}^m a_j \Phi_j \right) = \max_{j \le m} depth(\Phi_j),\
$$
$$\
size \left( \sum_{j=1}^m a_j \Phi_j \right) \le 2 \sum_{j=1}^m size(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j)d_j + 1.\
$$

*Proof*. A construção de $\sum_{j=1}^m a_j \Phi_j$ é análoga à de $(\Phi_1, \dots, \Phi_m)$, isto é, primeiro definimos a combinação linear de redes neurais com a mesma profundidade. Então, os pesos são escolhidos como em (5.1.2), mas com a última transformação linear substituída por
$$\
(a_1 W_1^{(L)}, \dots, a_m W_m^{(L)}), \sum_{j=1}^m a_j b_j^{(L)}.\
$$
Para profundidades gerais, definimos a soma das redes neurais como sendo a soma das redes neurais estendidas $\tilde{\Phi}_i$ como em (5.1.3). Todas as afirmações do lema seguem imediatamente desta construção. $\blacksquare$

No caso em que $d_0 = \dots = d_m =: d_0$ (todas as redes neurais têm a mesma dimensão de entrada), também consideraremos combinações lineares com entradas compartilhadas, isto é, uma rede neural realizando
$$\
x \mapsto \sum_{j=1}^m a_j \Phi_j(x) \text{ para } x \in \mathbb{R}^{d_0}.\
$$

### Conclusão
Esta seção formalizou as operações básicas para combinar e manipular redes neurais ReLU: reprodução de identidade, composição, paralelização e combinações lineares. Foram adicionados limites precisos ao número de pesos nas redes neurais resultantes, fornecendo um conjunto de ferramentas para construir e analisar redes neurais mais complexas a partir de componentes mais simples.

### Referências
[^43]: In this section, we discuss feedforward neural networks using the ReLU activation function ReLU introduced in Section 2.3. We refer to these functions as ReLU neural networks. Due to its simplicity and the fact that it reduces the vanishing and exploding gradients phenomena, it is one of the most widely used in practice.
[^44]: The goal of this section is to formalize how to combine and manipulate ReLU neural networks.
[^45]: We have seen an instance of such a result already in Proposition 2.3. Now we want to make this result more precise under the assumption that the activation function is the ReLU. We sharpen Proposition 2.3 by adding bounds on the number of weights that the resulting neural networks have. The following four operations form the basis of all constructions in the sequel.
[^46]: • Reproducing an identity: We have seen in Proposition 3.16 that for most activation functions, an approximation to the identity can be built by neural networks. For ReLUs, we can have an even stronger result and reproduce the identity exactly. This identity will play a crucial role in order to extend certain neural networks to deeper neural networks, and to facilitate an efficient composition operation.
[^47]: • Composition: We saw in Proposition 2.3 that we can produce a composition of two neural networks and the resulting function is a neural network as well. There we did not study the size of the resulting neural networks. For ReLU activation functions, this composition can be done in a very efficient way leading to a neural network that has up to a constant not more than the number of weights of the two initial neural networks.
[^48]: • Parallelization: Also the parallelization of two neural networks was discussed in Proposition 2.3. We will refine this notion and make precise the size of the resulting neural networks.
[^49]: • Linear combinations: Similarly, for the sum of two neural networks, we will give precise bounds on the size of the resulting neural network.
[^50]: We start with expressing the identity on Rd as a neural network of depth L∈N.
[^51]: Assume we have two neural networks Φ1, Φ2 with architectures (σReLU; d0, ..., dL1+1) and (σReLU; d0, ..., dL2+1) respectively. Moreover, we assume that they have weights and biases given by (W(0), b(0)), ..., (W(L1), b(L1)), and (W(0), b(0)),..., (W(L2), b(L2)), respectively.
[^52]: Next, we wish to put neural networks in parallel. Let (Φi)i=1m be neural networks with architectures (σReLU; d0,..., dLi+1), respectively. We proceed to build a neural network (Φ1,..., Φm) such that (Φ1,..., Φm): R∑i=1mdi → R∑i=1mdLi+1 (x1,..., xm) → (Φ1(x1),………, Φm(xm)).
[^53]: Let m ∈ N and let (Φi)i=1m be ReLU neural networks that have architectures (σReLU; d0, ..., dLi+1), respectively. Assume that dL1+1 = • = dLm+1, i.e., all Φ1,..., Φm have the same output dimension. For scalars aj ∈ R, we wish to construct a ReLU neural network ∑j=1m αjΦj realizing the function R∑j=1mdi → RdL1+1 (x1,...,xm) → ∑j=1m αjΦj(xj).
<!-- END -->