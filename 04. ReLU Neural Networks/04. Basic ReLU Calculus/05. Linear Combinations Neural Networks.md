## Linear Combinations of ReLU Neural Networks: Size and Complexity Bounds

### Introdução
Este capítulo se dedica ao estudo das combinações lineares de redes neurais ReLU, com o objetivo de estabelecer limites precisos para o tamanho e a complexidade das redes resultantes. Este tópico se situa no contexto mais amplo do "cálculo ReLU básico" [^5], que visa formalizar a combinação e manipulação de redes neurais ReLU. Em continuidade ao conceito de redes neurais ReLU introduzido na Seção 2.3 [^5], exploraremos as operações básicas que formam a base de construções mais complexas. Como vimos anteriormente, a função de ativação ReLU, devido à sua simplicidade e capacidade de mitigar o problema dos gradientes que desaparecem ou explodem, é amplamente utilizada na prática [^5].

### Conceitos Fundamentais
A **combinação linear** de redes neurais é uma operação fundamental na construção de arquiteturas mais complexas. Dada uma coleção de redes neurais ReLU $\Phi_1, ..., \Phi_m$, desejamos construir uma nova rede que realize a função $\sum_{j=1}^m \alpha_j \Phi_j$, onde $\alpha_j$ são escalares [^5]. Formalmente, consideremos $m \in \mathbb{N}$ e redes neurais ReLU $(\Phi_j)_{j=1}^m$ com arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_j+1})$. Assumimos que $d_{L_1+1} = ... = d_{L_m+1}$, ou seja, todas as $\Phi_j$ têm a mesma dimensão de saída [^5]. Para escalares $\alpha_j \in \mathbb{R}$, desejamos construir uma rede neural ReLU $\sum_{j=1}^m \alpha_j \Phi_j$ que realize a função:
$$\
\left(x_1, ..., x_m\right) \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x_j)
$$\
de $\mathbb{R}^{\sum_{i=1}^m d_i}$ para $\mathbb{R}^{d_{L_1+1}}$ [^5].

Esta construção corresponde à **paralelização** das redes neurais $(\Phi_1, ..., \Phi_m)$ composta com uma transformação linear $\left(z_1, ..., z_m\right) \mapsto \sum_{j=1}^m \alpha_j z_j$ [^5].

**Lemma 5.4 (Linear combinations)** [^5]. *Seja $m \in \mathbb{N}$ e $(\Phi_j)_{j=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_j+1})$. Assuma que $d_{L_1+1} = ... = d_{L_m+1}$, seja $\alpha \in \mathbb{R}^m$ e defina $L_{max} := \max_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m \alpha_j \Phi_j$ tal que $\left(\sum_{j=1}^m \alpha_j \Phi_j\right)(x) = \sum_{j=1}^m \alpha_j \Phi_j(x)$ para todo $x = (x_j)_{j=1}^m \in \mathbb{R}^{\sum_{i=1}^m d_i}$. Além disso:*\

$$\
\text{width}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) \leq 2\sum_{j=1}^m \text{width}(\Phi_j) \qquad (5.1.6a)
$$\

$$\
\text{depth}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) = \max_{j \leq m} \text{depth}(\Phi_j) \qquad (5.1.6b)
$$\

$$\
\text{size}\left(\sum_{j=1}^m \alpha_j \Phi_j\right) \leq 2\sum_{j=1}^m \text{size}(\Phi_j) + 2\sum_{j=1}^m (L_{max} - L_j)d_j + 1 \qquad (5.1.6c)
$$\

A construção de $\sum_{j=1}^m \alpha_j \Phi_j$ é análoga à de $(\Phi_1, ..., \Phi_m)$ [^5]. Primeiramente, definimos a combinação linear de redes neurais com a mesma profundidade. Os pesos são então escolhidos como em (5.1.2) [^5], mas com a última transformação linear substituída por:
$$\
\begin{pmatrix}\
\alpha_1 W_1^{(L_1)} & \cdots & \alpha_m W_m^{(L_m)} \\\\\
\end{pmatrix}
\qquad
\begin{pmatrix}\
\alpha_1 b_1^{(L_1)} & \cdots & \alpha_m b_m^{(L_m)} \\\\\
\end{pmatrix}
$$\

Para profundidades gerais, definimos a soma das redes neurais como a soma das redes neurais estendidas $\Phi_j$ como em (5.1.3) [^5]. Todas as afirmações do lema seguem imediatamente dessa construção.

No caso em que $d_1 = ... = d_m =: d_0$ (todas as redes neurais têm a mesma dimensão de entrada), também consideramos combinações lineares com entradas compartilhadas, ou seja, uma rede neural que realiza:
$$\
x \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x) \qquad \text{para } x \in \mathbb{R}^{d_0}
$$\
Este caso requer o mesmo ajuste menor discutido no final da Seção 5.1.3 [^5]. O Lema 5.4 permanece válido neste caso e, novamente, não distinguimos na notação para combinações lineares com ou sem entradas compartilhadas [^5].

### Conclusão
A combinação linear de redes neurais ReLU é uma ferramenta poderosa para construir funções complexas a partir de componentes mais simples. O Lemma 5.4 fornece limites precisos para o tamanho, a profundidade e a largura das redes resultantes, permitindo um controle fino da complexidade do modelo. Esses resultados são fundamentais para o desenvolvimento de arquiteturas de redes neurais eficientes e para a compreensão das capacidades de aproximação das redes neurais ReLU.

### Referências
[^5]: Capítulo 5 do texto fornecido.
<!-- END -->