## Parallelization of ReLU Neural Networks: Size and Complexity

### Introdução
Este capítulo se baseia no cálculo básico de ReLU apresentado anteriormente, especificamente na Proposição 2.3 [^5, ^6, ^7]. Expandindo sobre a capacidade de combinar e manipular redes neurais ReLU, o foco agora se volta para a **paralelização**, uma operação fundamental na construção de redes neurais mais complexas [^3]. Paralelizar redes neurais envolve executar várias redes simultaneamente e combinar suas saídas. Este capítulo irá refinar a noção de paralelização, quantificando precisamente o tamanho das redes neurais resultantes e explorando as implicações na complexidade da rede.

### Conceitos Fundamentais

A **paralelização** de redes neurais permite construir redes maiores e mais poderosas a partir de componentes menores [^2]. A ideia central é executar várias redes neurais independentemente e combinar seus resultados para produzir uma saída mais complexa. Formalmente, dadas *m* redes neurais ReLU $\Phi_1, \Phi_2, ..., \Phi_m$, com arquiteturas (ReLU; $d_0, ..., d_{L_i+1}$), respectivamente, o objetivo é construir uma rede neural (Φ₁, ..., Φm) que realize o seguinte mapeamento [^3]:

$$\
(\Phi_1, ..., \Phi_m) : \mathbb{R}^{\sum_{i=1}^{m} d_0} \rightarrow \mathbb{R}^{\sum_{i=1}^{m} d_{L_i+1}}\
$$

$$\
(x_1, ..., x_m) \rightarrow (\Phi_1(x_1), ..., \Phi_m(x_m))\
$$

Para simplificar a análise inicial, assume-se que todas as redes têm a mesma profundidade, ou seja, $L_1 = ... = L_m = L$ [^3]. A construção da rede paralela é realizada através da seguinte sequência de tuplas peso-bias:

$$\
((W_1^{(0)}, b_1^{(0)}), ..., (W_m^{(0)}, b_m^{(0)})), ..., ((W_1^{(L)}, b_1^{(L)}), ..., (W_m^{(L)}, b_m^{(L)}))\
$$

onde as matrizes são entendidas como blocos diagonais preenchidos com zeros [^3]. No caso geral em que as redes $\Phi_j$ podem ter profundidades diferentes, define-se $L_{max} := max_{1 \leq i \leq m} L_i$ e $I := \\{1 \leq i \leq m | L_i < L_{max}\\}$. Para $j \in I^c$, define-se $\tilde{\Phi}_j := \Phi_j$, e para cada $j \in I$,

$$\
\tilde{\Phi}_j := \Phi_{L_{max} - L_j}^{id} \circ \Phi_j.\
$$

Finalmente, define-se

$$\
(\Phi_1, ..., \Phi_m) := (\tilde{\Phi}_1, ..., \tilde{\Phi}_m).\
$$

O Lemma 5.3 [^3] resume as propriedades da paralelização:

**Lemma 5.3 (Paralelização).** *Seja m ∈ N e (Φᵢ)ᵢ₌₁ᵐ redes neurais com arquiteturas (ReLU; d₀, ..., dᵢ₊₁), respectivamente. Então a rede neural (Φ₁, ..., Φₘ) satisfaz*\

$$\
(\Phi_1, ..., \Phi_m)(x) = (\Phi_1(x_1), ..., \Phi_m(x_m)) \text{ para todo } x \in \mathbb{R}^{\sum_{i=1}^{m} d_0}\
$$

*Além disso, com Lₘₐₓ := maxⱼ≤ₘ Lⱼ, tem-se que*\

*   *width((Φ₁, ..., Φₘ)) ≤ 2∑ⱼ₌₁ᵐ width(Φⱼ),*\
*   *depth((Φ₁, ..., Φₘ)) = maxⱼ≤ₘ depth(Φⱼ),*\
*   *size((Φ₁, ..., Φₘ)) ≤ 2∑ⱼ₌₁ᵐ size(Φⱼ) + 2∑ⱼ₌₁ᵐ(Lₘₐₓ - Lⱼ)dⱼ₊₁*\

A prova do Lemma 5.3 segue diretamente da construção da rede paralela [^3]. É importante notar que, ao confirmar o limite de tamanho, os tamanhos de $\tilde{\Phi}_i$ serão simplesmente adicionados.

Se todas as dimensões de entrada $d_i = ... = d_m =: d_0$ são as mesmas, também será usada paralelização com entradas compartilhadas para realizar a função $x \rightarrow (\Phi_1(x), ..., \Phi_m(x))$ de $\mathbb{R}^{d_0} \rightarrow \mathbb{R}^{d_{L_1+1}+...+d_{L_m+1}}$. Em termos da construção (5.1.2), a única mudança necessária é que a matriz de bloco diagonal $diag(W_1^{(0)}, ..., W_m^{(0)})$ se torna a matriz em $\mathbb{R}^{\sum_{i=1}^{m} d_i \times d_0}$ que empilha $W_1^{(0)}, ..., W_m^{(0)}$ um em cima do outro [^3]. Da mesma forma, permite-se que $\Phi_i$ receba apenas algumas das entradas de $x$ como entrada. Para paralelização com entradas compartilhadas, será usada a mesma notação $(\Phi_i)_{i=1}^{m}$ como antes, onde o significado preciso sempre será claro pelo contexto [^3].

### Conclusão

Este capítulo formalizou a operação de paralelização de redes neurais ReLU, fornecendo limites precisos para o tamanho e a profundidade das redes resultantes [^3]. O Lemma 5.3 [^3] estabelece a base para entender como a complexidade das redes paralelas se relaciona com a complexidade de suas redes componentes. A paralelização com entradas compartilhadas foi brevemente mencionada e será usada em capítulos posteriores [^3].

### Referências
[^2]: A key component of the proofs in the previous chapters was the approximation of derivatives of the activation function to emulate polynomials.
[^3]: Next, we wish to put neural networks in parallel.
[^5]: The goal of this section is to formalise how to combine and manipulate ReLU neural networks.
[^6]: We have seen an instance of such a result already in Proposition 2.3.
[^7]: Now we want to make this result more precise under the assumption that the activation function is the ReLU.
<!-- END -->