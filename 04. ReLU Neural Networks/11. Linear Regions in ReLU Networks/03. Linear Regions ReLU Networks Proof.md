## Capítulo 6: Limites Superiores no Número de Regiões Lineares em Redes ReLU

### Introdução
No capítulo anterior, exploramos as capacidades de aproximação de Redes Neurais ReLU *rasas* [^1]. Observamos que múltiplas camadas são um pré-requisito necessário para que as Redes Neurais ReLU aproximem funções suaves com altas taxas. Neste capítulo, analisaremos qual profundidade é suficiente para alcançar boas taxas de aproximação para funções suaves. Mais especificamente, exploraremos os limites superiores no número de regiões lineares que as redes ReLU podem gerar, estabelecendo uma base teórica para entender o potencial de representação das redes.

### Conceitos Fundamentais
As Redes Neurais ReLU são construídas com base na composição e adição de neurônios. Essas operações aumentam o possível número de regiões de uma maneira muito específica [^65]. A compreensão de como essas operações afetam a complexidade da rede é crucial para analisar suas capacidades de aproximação.

**Definição:** Seja $\sigma$ uma função *cpwl* (continuous piecewise linear) com $p$ regiões [^48, 64]. Então, uma Rede Neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ tem no máximo $(p \cdot width(\Phi))^L$ regiões, onde $width(\Phi)$ representa a largura da rede [^65].

Este resultado é formalizado no seguinte teorema:

**Teorema 6.3:** Seja $L \in \mathbb{N}$. Seja $\sigma$ uma função *cpwl* com $p$ regiões. Então, toda Rede Neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ tem no máximo $(p \cdot width(\Phi))^L$ regiões [^65].

*Prova:* A prova é feita por indução sobre a profundidade $L$.

*Caso Base (L=1):* Seja $L = 1$ e $\Phi: \mathbb{R} \to \mathbb{R}$ uma Rede Neural com arquitetura $(\sigma; 1, d_1, 1)$. Então,
$$\
\Phi(x) = \sum_{k=1}^{d_1} w_k^{(1)} \sigma(w_k^{(0)} x + b_k^{(0)}) + b^{(1)} \quad \text{para } x \in \mathbb{R},\
$$
para certos $w_k^{(0)}, w_k^{(1)}, b_k^{(0)} \in \mathbb{R}^{d_1}$ e $b^{(1)} \in \mathbb{R}$ [^65]. Pela equação (6.1.1) do contexto, $\text{Pieces}(\Phi) \leq p \cdot \text{width}(\Phi)$.

*Passo Indutivo:* Assumimos que a afirmação é verdadeira para $L \in \mathbb{N}$ e seja $\Phi: \mathbb{R} \to \mathbb{R}$ uma Rede Neural com arquitetura $(\sigma; 1, d_1, ..., d_{L+1}, 1)$. Então, podemos escrever
$$\
\Phi(x) = \sum_{j=1}^{d_{L+1}} w_j \sigma(h_j(x)) + b \quad \text{para } x \in \mathbb{R},\
$$
para alguns $w \in \mathbb{R}^{d_{L+1}}$, $b \in \mathbb{R}$, e onde cada $h_j$ é uma Rede Neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ [^65]. Usando a hipótese indutiva, cada $\sigma \circ h_j$ tem no máximo $p \cdot (\text{width}(\Phi))^L$ regiões afins. Portanto, $\Phi$ tem no máximo $\text{width}(\Phi) \cdot p \cdot (p \cdot \text{width}(\Phi))^L = (p \cdot \text{width}(\Phi))^{L+1}$ regiões afins. Isso completa a prova. $\blacksquare$

Este teorema estabelece um limite superior no número de regiões que uma rede ReLU pode criar, o que é crucial para entender as limitações e capacidades de aproximação da rede.

### Conclusão
A análise do número de regiões lineares em Redes Neurais ReLU fornece *insights* valiosos sobre sua complexidade e potencial de aproximação [^65]. O limite superior derivado neste capítulo destaca o papel da profundidade e da largura na determinação da capacidade da rede de representar funções complexas. Compreender esses limites é essencial para projetar arquiteturas de rede eficientes e eficazes para tarefas específicas. A exploração futura pode se concentrar em caracterizar as condições sob as quais esses limites são alcançados e em desenvolver técnicas para melhorar a capacidade de representação das redes ReLU.

### Referências
[^1]: Capítulo anterior
[^48]: Definição 5.5
[^64]: Definition 6.1
[^65]: Theorem 6.3

<!-- END -->