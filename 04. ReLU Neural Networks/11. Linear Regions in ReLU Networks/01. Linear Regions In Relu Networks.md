## Limitações das Redes Neurais ReLU: Uma Análise do Número de Regiões Lineares

### Introdução
Este capítulo explora as limitações das redes neurais ReLU através da análise do número de regiões lineares que essas funções podem gerar [^1]. Em continuidade ao estudo das propriedades das redes neurais ReLU, como visto na seção 2.3 [^1] e no capítulo 5 [^1], focaremos agora na capacidade dessas redes de criar regiões lineares e estabeleceremos um limite superior teórico simples. Investigaremos também sob quais condições esses limites superiores são atingíveis [^1].

### Conceitos Fundamentais
A análise das limitações das redes neurais ReLU é tradicionalmente abordada pelo estudo do número de regiões lineares que essas funções podem gerar [^1]. Seja $d \in \mathbb{N}$, $\Omega \subseteq \mathbb{R}^d$, e $f: \Omega \rightarrow \mathbb{R}$ uma função *cpwl* (continuous, piecewise linear) [^4]. Dizemos que $f$ tem $p \in \mathbb{N}$ *peças* (ou regiões lineares), se $p$ é o menor número de conjuntos abertos conexos $(\Omega_i)_{i=1}^p$ tal que $\bigcup_{i=1}^p \Omega_i = \Omega$, e $f|_{\Omega_i}$ é uma função afim para todo $i = 1, ..., p$. Denotamos *Pieces*(f, $\Omega$) := p [^4]. Para $d = 1$, chamamos cada ponto onde $f$ não é diferenciável de *ponto de quebra* de $f$ [^4].

As redes neurais são baseadas na composição e adição de neurônios. Estas duas operações aumentam o número possível de regiões de uma maneira muito específica [^1]. Elas podem ser descritas da seguinte forma [^1]:

*   **Somatório:** Seja $\Omega \subseteq \mathbb{R}$. A soma de duas funções *cpwl* $f_1, f_2 : \Omega \rightarrow \mathbb{R}$ satisfaz [^1]:
    $$Pieces(f_1 + f_2, \Omega) \leq Pieces(f_1, \Omega) + Pieces(f_2, \Omega) - 1 \qquad (6.1.1)$$
    Isto ocorre porque a soma é afim em todo ponto onde $f_1$ e $f_2$ são afins. Portanto, a soma tem no máximo tantos pontos de quebra quanto $f_1$ e $f_2$ combinados. Além disso, o número de regiões de uma função univariada é igual ao número de seus pontos de quebra mais um [^1].
*   **Composição:** Seja novamente $\Omega \subseteq \mathbb{R}$. A composição de duas funções $f_1 : \mathbb{R}^d \rightarrow \mathbb{R}$ e $f_2: \Omega \rightarrow \mathbb{R}^d$ satisfaz [^1]:
    $$Pieces(f_1 \circ f_2, \Omega) \leq Pieces(f_1, \mathbb{R}^d) \cdot Pieces(f_2, \Omega) \qquad (6.1.2)$$
    Isto ocorre porque para cada uma das regiões afins de $f_2$ (vamos chamar uma dessas regiões de $A \subseteq \mathbb{R}$), temos que $f_2$ é constante ou injetiva em $A$. Se é constante, então $f_1 \circ f_2$ é constante. Se é injetiva, então $Pieces(f_1 \circ f_2, A) = Pieces(f_1, f_2(A)) \leq Pieces(f_1, \mathbb{R}^d)$. Como isto vale para todas as regiões de $f_2$ obtemos (6.1.2) [^1].

Essas considerações dão o seguinte resultado, que segue o argumento de [226, Lemma 2.1] [^1]. Declaramos isso para funções de ativação *cpwl* gerais. A função de ativação ReLU corresponde a $p = 2$ [^1].

**Teorema 6.3.** Seja $L \in \mathbb{N}$. Seja $\sigma$ uma função *cpwl* com $p$ regiões. Então, toda rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ tem no máximo $(p \cdot width(\Phi))^L$ regiões [^1].

**Prova.** A prova é por indução sobre a profundidade $L$. Seja $L = 1$, e seja $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ uma rede neural de arquitetura $(\sigma; 1, d_1, 1)$. Então [^1]
$$\Phi(x) = \sum_{k=1}^{d_1} w_k^{(1)} \sigma(w_k^{(0)}x + b_k^{(0)}) + b^{(1)} \quad \text{para } x \in \mathbb{R},$$
para certos $w^{(0)}, w^{(1)}, b^{(0)} \in \mathbb{R}^{d_1}$ e $b^{(1)} \in \mathbb{R}$. Por (6.1.1), $Pieces(\Phi) \leq p \cdot width(\Phi)$ [^1].

Para o passo de indução, assuma que a declaração vale para $L \in \mathbb{N}$, e seja $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ uma rede neural de arquitetura $(\sigma; 1, d_1, ..., d_{L+1},1)$. Então podemos escrever [^1]
$$\Phi(x) = \sum_{j=1}^{d_{L+1}} w_j \sigma(h_j(x)) + b$$
para $x \in \mathbb{R}$, para alguns $w \in \mathbb{R}^{d_{L+1}}$, $b \in \mathbb{R}$, e onde cada $h_j$ é uma rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$. Usando a hipótese de indução, cada $\sigma \circ h_j$ tem no máximo $p \cdot (p \cdot width(\Phi))^L$ regiões afins. Portanto, $\Phi$ tem no máximo $width(\Phi) \cdot p \cdot (p \cdot width(\Phi))^L = (p \cdot width(\Phi))^{L+1}$ regiões afins. Isto completa a prova. $\blacksquare$

O Teorema 6.3 mostra que existem limites para quantas regiões podem ser criadas com uma certa arquitetura [^1]. É notável que os efeitos da profundidade e da largura de uma rede neural são vastamente diferentes [^1]. Enquanto aumentar a largura pode aumentar polinomialmente o número de regiões, aumentar a profundidade pode resultar em um aumento exponencial [^1]. Esta é uma primeira indicação da proeza da profundidade das redes neurais [^1].

### Conclusão
Este capítulo estabeleceu um limite superior teórico para o número de regiões lineares que uma rede neural ReLU pode gerar [^1]. O teorema 6.3 mostra que o número máximo de regiões lineares cresce exponencialmente com a profundidade da rede, mas apenas polinomialmente com a largura [^1]. Em continuidade a este resultado, o próximo passo é investigar sob quais condições esses limites superiores são atingíveis [^1].

### Referências
[^1]: Capítulo atual do livro.
[^4]: Seção 5.2 do livro.
<!-- END -->