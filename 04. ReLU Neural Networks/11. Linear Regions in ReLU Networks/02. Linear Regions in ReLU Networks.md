## Capítulo 5: Efeito da Composição e Adição de Neurônios em Redes Neurais ReLU

### Introdução
Este capítulo explora como as operações de composição e adição de neurônios afetam o número de regiões lineares em redes neurais ReLU. O foco é entender como essas operações, fundamentais na construção de redes neurais, influenciam a capacidade da rede de aproximar funções complexas [^1].

### Conceitos Fundamentais

Redes neurais são construídas a partir da composição e adição de neurônios. Essas operações afetam diretamente o número de regiões lineares que a rede pode representar [^1].

#### 6.1 Upper bounds
"Neural networks are based on the composition and addition of neurons. These two operations increase the possible number of pieces in a very specific way."[^1] A Figura 6.1 (não presente no contexto atual) ilustra essas operações e seus efeitos. Elas podem ser descritas da seguinte forma:

*   **Summation:** Seja $\Omega \subseteq \mathbb{R}$. A soma de duas funções cpwl (continuous, piecewise linear) $f_1, f_2: \Omega \rightarrow \mathbb{R}$ satisfaz [^1]:
    $$\
    \text{Pieces}(f_1 + f_2, \Omega) \le \text{Pieces}(f_1, \Omega) + \text{Pieces}(f_2, \Omega) - 1. \tag{6.1.1}\
    $$
    Isto ocorre porque a soma é afim em todo ponto onde ambas $f_1$ e $f_2$ são afins. Portanto, a soma tem no máximo tantos pontos de quebra quanto $f_1$ e $f_2$ combinados. Além disso, o número de pedaços de uma função univariada é igual ao número de seus pontos de quebra mais um [^1].

*   **Composition:** Seja novamente $\Omega \subseteq \mathbb{R}$. A composição de duas funções $f_1: \mathbb{R}^d \rightarrow \mathbb{R}$ e $f_2: \Omega \rightarrow \mathbb{R}^d$ satisfaz [^1]:
    $$\
    \text{Pieces}(f_1 \circ f_2, \Omega) \le \text{Pieces}(f_1, \mathbb{R}^d) \cdot \text{Pieces}(f_2, \Omega). \tag{6.1.2}\
    $$
    Isto acontece porque, para cada um dos pedaços afins de $f_2$ (digamos, $A \subset \mathbb{R}$), temos que $f_2$ é constante ou injetiva em $A$. Se for constante, então $f_1 \circ f_2$ é constante. Se for injetiva, então $\text{Pieces}(f_1 \circ f_2, A) = \text{Pieces}(f_1, f_2(A)) \le \text{Pieces}(f_1, \mathbb{R}^d)$. Como isto vale para todos os pedaços de $f_2$, obtemos (6.1.2) [^1].

Essas considerações levam ao seguinte resultado, que segue o argumento de [226, Lemma 2.1]. Declaramos isto para funções cpwl gerais de ativação. A função de ativação ReLU corresponde a $p=2$ [^1].

**Theorem 6.3.** Seja $L \in \mathbb{N}$. Seja $\sigma$ cpwl com $p$ pedaços. Então, toda rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$ tem no máximo $(p \cdot \text{width}(\Phi))^L$ pedaços [^1].

*Proof.* A prova é por indução sobre a profundidade $L$. Seja $L=1$, e seja $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ uma rede neural de arquitetura $(\sigma; 1, d_1, 1)$. Então [^1],
$$\
\Phi(x) = \sum_{k=1}^{d_1} w_k^{(1)}\sigma(w_k^{(0)} x + b_k^{(0)}) + b^{(1)}\
$$
para $x \in \mathbb{R}$. Por (6.1.1), $\text{Pieces}(\Phi) \le p \cdot \text{width}(\Phi)$ [^1].

Para o passo de indução, assuma que a declaração vale para $L \in \mathbb{N}$, e seja $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ uma rede neural de arquitetura $(\sigma; 1, d_1, ..., d_{L+1}, 1)$. Então podemos escrever [^1],
$$\
\Phi(x) = \sum_{j=1}^{d_{L+1}} w_j \sigma(h_j(x)) + b\
$$
para $x \in \mathbb{R}$, para alguns $w \in \mathbb{R}^{d_{L+1}}$, $b \in \mathbb{R}$, e onde cada $h_j$ é uma rede neural com arquitetura $(\sigma; 1, d_1, ..., d_L, 1)$. Usando a hipótese de indução, cada $\sigma \circ h_j$ tem no máximo $p \cdot (p \cdot \text{width}(\Phi))^L$ pedaços afins. Portanto, $\Phi$ tem no máximo width$(\Phi) \cdot p \cdot (p \cdot \text{width}(\Phi))^L = (p \cdot \text{width}(\Phi))^{L+1}$ pedaços afins. Isto completa a prova. $\blacksquare$

Theorem 6.3 mostra que existem limites para quantos pedaços podem ser criados com uma certa arquitetura. É notável que os efeitos da profundidade e da largura de uma rede neural são vastamente diferentes. Enquanto aumentar a largura pode aumentar polinomialmente o número de pedaços, aumentar a profundidade pode resultar em aumento exponencial. Esta é uma primeira indicação da proeza da profundidade das redes neurais [^1].

Para entender o efeito disto no problema de aproximação, aplicamos o limite do Theorem 6.3 ao Theorem 6.2 [^1].

**Theorem 6.4.** Seja $d_0 \in \mathbb{N}$ e $f \in C^3([0,1]^{d_0})$. Assuma que existe um segmento de linha $S \subseteq [0,1]^{d_0}$ de comprimento positivo tal que $0 < c := \int \sqrt{|f\'\'(x)|} dx$. Então existe $C > 0$ dependendo unicamente de $c$, tal que para todas as redes neurais ReLU $\Phi: \mathbb{R}^{d_0} \rightarrow \mathbb{R}$ com $L$ camadas [^1],
$$\
||f - \Phi||_{L^{\infty}([0,1]^{d_0})} \ge c \cdot (2 \text{width}(\Phi))^{-2L}.\
$$

Theorem 6.4 dá um limite inferior sobre as taxas de aproximação alcançáveis em dependência da profundidade $L$. À medida que as funções alvo se tornam mais suaves, esperamos que possamos alcançar taxas de convergência mais rápidas [^1].

### Conclusão

A composição e adição de neurônios são operações fundamentais na construção de redes neurais ReLU, influenciando diretamente o número de regiões lineares que a rede pode representar. A análise apresentada neste capítulo fornece insights sobre como essas operações afetam a capacidade da rede de aproximar funções complexas, destacando a importância da profundidade na criação de modelos mais expressivos [^1].

### Referências
[^1]: Capítulo 5 e 6 do texto fornecido.
<!-- END -->