## ReLU Neural Networks: Activation Function and Basic Calculus

### Introdução
Este capítulo explora as redes neurais *feedforward* que utilizam a função de ativação **ReLU (Rectified Linear Unit)**, denotada como $\sigma_{ReLU}(x) = max(0, x)$ [^1]. As redes ReLU se destacam devido à sua simplicidade e capacidade de mitigar os problemas de gradientes *vanishing* e *exploding* [^1]. Uma característica fundamental das redes ReLU é a sua propriedade de representar exatamente funções contínuas *piecewise linear* [^1]. A análise das redes ReLU difere significativamente das redes com funções de ativação mais suaves, pois as técnicas de aproximação de derivadas não são diretamente aplicáveis devido à natureza *piecewise linear* da ReLU [^1]. Em continuidade ao formalismo introduzido na Seção 2.3 [^1], este capítulo visa formalizar a combinação e manipulação de redes neurais ReLU, refinando resultados anteriores como a Proposição 2.3 [^1] ao adicionar limites no número de pesos das redes resultantes.

### Conceitos Fundamentais

#### 5.1 Basic ReLU calculus
Esta seção tem como objetivo formalizar como combinar e manipular redes neurais ReLU. Já vimos uma instância de tal resultado na Proposição 2.3 [^1]. Agora queremos tornar este resultado mais preciso sob a suposição de que a função de ativação é a ReLU. Aprimoramos a Proposição 2.3 adicionando limites no número de pesos que as redes neurais resultantes possuem [^1]. As quatro operações a seguir formam a base de todas as construções na sequência [^1]:

*   **Reproducing an identity**: Vimos na Proposição 3.16 [^1] que, para a maioria das funções de ativação, uma aproximação da identidade pode ser construída por redes neurais. Para ReLUs, podemos ter um resultado ainda mais forte e reproduzir a identidade exatamente. Essa identidade desempenhará um papel crucial para estender certas redes neurais para redes neurais mais profundas e para facilitar uma operação de composição eficiente [^1].
*   **Composition**: Vimos na Proposição 2.3 [^1] que podemos produzir uma composição de duas redes neurais e a função resultante também é uma rede neural. Lá não estudamos o tamanho das redes neurais resultantes. Para funções de ativação ReLU, essa composição pode ser feita de uma maneira muito eficiente, levando a uma rede neural que tem até uma constante não mais do que o número de pesos das duas redes neurais iniciais [^1].
*   **Parallelization**: Também a paralelização de duas redes neurais foi discutida na Proposição 2.3 [^1]. Refinaremos essa noção e tornaremos preciso o tamanho das redes neurais resultantes [^1].
*   **Linear combinations**: Da mesma forma, para a soma de duas redes neurais, daremos limites precisos no tamanho da rede neural resultante [^1].

#### 5.1.1 Identity
Começamos expressando a identidade em $R^d$ como uma rede neural de profundidade $L \in N$ [^1].

**Lemma 5.1 (Identity)**. Seja $L \in N$. Então, existe uma rede neural ReLU $\Phi_{id}^L$ tal que $\Phi_{id}^L(x) = x$ para todo $x \in R^d$. Além disso, $depth(\Phi_{id}^L) = L$, $width(\Phi_{id}^L) = 2d$ e $size(\Phi_{id}^L) = 2d \cdot (L+1)$ [^1].

*Proof.* Escrevendo $I_d \in R^{d \times d}$ para a matriz identidade, escolhemos os pesos [^1]

$$(W^{(0)}, b^{(0)}), ..., (W^{(L)}, b^{(L)}):=((I_d, -I_d), 0), ..., ((I_d, -I_d), 0), ((I_{2d}, 0), ..., (I_{2d}, 0), ((I_d, -I_d), 0))$$

Usando que $x = \sigma_{ReLU}(x) - \sigma_{ReLU}(-x)$ para todo $x \in R$ e $\sigma_{ReLU}(x) = x$ para todo $x \geq 0$, é óbvio que a rede neural $\Phi_{id}^L$ associada aos pesos acima satisfaz a asserção do lema. $\blacksquare$

Veremos no Exercício 5.23 [^1] que a propriedade de representar exatamente a identidade não é compartilhada por funções de ativação sigmoidal. Ela vale para funções de ativação polinomial, no entanto [^1].

#### 5.1.2 Composition
Assuma que temos duas redes neurais $\Phi_1, \Phi_2$ com arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_1+1})$ e $(\sigma_{ReLU}; d_0, ..., d_{L_2+1})$ respectivamente [^1]. Além disso, assumimos que elas têm pesos e vieses dados por [^1]

$$(W_1^{(0)}, b_1^{(0)}), ..., (W_1^{(L_1)}, b_1^{(L_1)}), e (W_2^{(0)}, b_2^{(0)}), ..., (W_2^{(L_2)}, b_2^{(L_2)}),$$

respectivamente. Se a dimensão de saída $d_{L_1+1}$ de $\Phi_1$ é igual à dimensão de entrada $d_0$ de $\Phi_2$, podemos definir dois tipos de concatenações: Primeiro $\Phi_2 \circ \Phi_1$ é a rede neural com pesos e vieses dados por [^1]

$$(W_1^{(0)}, b_1^{(0)}), ..., (W_1^{(L_1-1)}, b_1^{(L_1-1)}), (W_2^{(0)}W_1^{(L_1)}, W_2^{(0)}b_1^{(L_1)} + b_2^{(0)}), (W_2^{(1)}, b_2^{(1)}), ..., (W_2^{(L_2)}, b_2^{(L_2)}).$$

Segundo, $\Phi_2 \bullet \Phi_1$ é a rede neural definida como $\Phi_2 \circ \Phi_{id} \circ \Phi_1$ [^1]. Em termos de pesos e vieses, $\Phi_2 \bullet \Phi_1$ é dado como [^1]

$$((W_1^{(0)}, b_1^{(0)}), ..., (W_1^{(L_1-1)}, b_1^{(L_1-1)}), (\begin{pmatrix} W_2^{(0)} \\\\ -W_2^{(0)} \end{pmatrix}W_1^{(L_1)}, \begin{pmatrix} W_2^{(0)} \\\\ -W_2^{(0)} \end{pmatrix}b_1^{(L_1)} + \begin{pmatrix} b_2^{(0)} \\\\ -b_2^{(0)} \end{pmatrix}), (W_2^{(1)}, b_2^{(1)}), ..., (W_2^{(L_2)}, b_2^{(L_2)})).$$

O seguinte lema coleta as propriedades da construção acima [^1].

**Lemma 5.2 (Composition)**. Sejam $\Phi_1, \Phi_2$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_1+1})$ e $(\sigma_{ReLU}; d_0, ..., d_{L_2+1})$. Assuma que $d_{L_1+1} = d_0$. Então $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in R^d$. Além disso, [^1]

$$width(\Phi_2 \circ \Phi_1) \leq max\\{width(\Phi_1), width(\Phi_2)\\},$$
$$depth(\Phi_2 \circ \Phi_1) = depth(\Phi_1) + depth(\Phi_2),$$
$$size(\Phi_2 \circ \Phi_1) \leq size(\Phi_1) + size(\Phi_2) + (d_{L_1} + 1)d_0,$$\
\ne [^1]

$$width(\Phi_2 \bullet \Phi_1) \leq 2max\\{width(\Phi_1), width(\Phi_2)\\},$$
$$depth(\Phi_2 \bullet \Phi_1) = depth(\Phi_1) + depth(\Phi_2) + 1,$$
$$size(\Phi_2 \bullet \Phi_1) \leq 2(size(\Phi_1) + size(\Phi_2)).$$

*Proof*. O fato de que $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in R^d$ segue imediatamente da construção. O mesmo pode ser dito para os limites de largura e profundidade. Para confirmar o limite de tamanho, notamos que $W_2^{(0)}W_1^{(L_1)} \in R^{d \times d_{L_1}}$ e, portanto, $W_2^{(0)}W_1^{(L_1)}$ tem no máximo $d^2 \times d_{L_1}$ entradas (não-zero). Além disso, $W_2^{(0)}b_1^{(L_1)} + b_2^{(0)} \in R^d$. Assim, a $L_1$-ésima camada de $\Phi_2 \circ \Phi_1(x)$ tem no máximo $d^2 \times (1 + d_{L_1})$ entradas. O resto é óbvio a partir da construção. $\blacksquare$

Interpretando transformações lineares como redes neurais de profundidade 0, o lema anterior também é válido caso $\Phi_1$ ou $\Phi_2$ seja um mapeamento linear [^1].

#### 5.1.3 Parallelization
Em seguida, desejamos colocar redes neurais em paralelo. Sejam $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0,..., d_{L_i+1})$, respectivamente. Procedemos para construir uma rede neural $(\Phi_1,..., \Phi_m)$ tal que [^1]

$$(\Phi_1,..., \Phi_m): R^{\sum_{i=1}^m d_0} \rightarrow R^{\sum_{i=1}^m d_{L_i+1}}$$\
$$(x_1,..., x_m) \rightarrow (\Phi_1(x_1),..., \Phi_m(x_m)).$$

Para fazer isso, primeiro assumimos $L_1 = ... = L_m = L$, e definimos $(\Phi_1,..., \Phi_m)$ via a seguinte sequência de tuplas de peso-viés [^1]:

$$((W_1^{(0)}, b_1^{(0)}),...,(W_m^{(0)}, b_m^{(0)})),..., ((W_1^{(L)}, b_1^{(L)}),...,(W_m^{(L)}, b_m^{(L)})),$$

onde essas matrizes são entendidas como bloco-diagonal preenchido com zeros. Para o caso geral onde o $\Phi_j$ pode ter diferentes profundidades, seja $L_{max} := max_{1 \leq i \leq m} L_i$ e $I := \\{1 \leq i \leq m | L_i < L_{max}\\}$. Para $j \in I^c$ defina $\tilde{\Phi_j} := \Phi_j$, e para cada $j \in I$ [^1]

$$\tilde{\Phi_j} := \Phi_{L_{max}-L_i}^{id} \circ \Phi_j.$$

Finalmente,

$$(\Phi_1,..., \Phi_m) := (\tilde{\Phi_1},..., \tilde{\Phi_m}).$$

Coletamos as propriedades da paralelização no lema abaixo [^1].

**Lemma 5.3 (Parallelization)**. Seja $m \in N$ e $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0,..., d_{L_i+1})$, respectivamente. Então a rede neural $(\Phi_1,..., \Phi_m)$ satisfaz [^1]

$$(\Phi_1,..., \Phi_m)(x) = (\Phi_1(x_1),..., \Phi_m(x_m)) para todo x \in R^{\sum_{i=1}^m d_0}.$$

Além disso, com $L_{max} := max_{j \leq m} L_j$ vale que [^1]

$$width((\Phi_1,..., \Phi_m)) \leq 2\sum_{j=1}^m width(\Phi_j),$$
$$depth((\Phi_1,..., \Phi_m)) = max_{j \leq m} depth(\Phi_j),$$
$$size((\Phi_1,..., \Phi_m)) \leq 2\sum_{j=1}^m size(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j)d_0 + 1$$

*Proof*. Todas as afirmações, exceto o limite no tamanho, seguem imediatamente da construção. Para obter o limite no tamanho, notamos que, por construção, os tamanhos dos $(\tilde{\Phi_i})_{i=1}^m$ em (5.1.3) simplesmente serão adicionados. O tamanho de cada $\tilde{\Phi_i}$ pode ser limitado com o Lema 5.2 [^1].

Se todas as dimensões de entrada $d_1 = ... = d_m =: d_0$ são as mesmas, também usaremos a paralelização com entradas compartilhadas para realizar a função $x \rightarrow (\Phi_1(x),..., \Phi_m(x))$ de $R^{d_0} \rightarrow R^{d_{L_1+1}+...+d_{L_m+1}}$ [^1]. Em termos da construção (5.1.2), a única mudança necessária é que a matriz de bloco-diagonal $diag(W_1^{(0)},..., W_m^{(0)})$ se torna a matriz em $R^{\sum_{i=1}^m d_0 \times d_0}$ que empilha $W_1^{(0)},..., W_m^{(0)}$ um em cima do outro [^1]. Da mesma forma, permitiremos que $\Phi_i$ apenas pegue algumas das entradas de $x$ como entrada. Para paralelização com entradas compartilhadas, usaremos a mesma notação $(\Phi_j)_{j=1}^m$ como antes, onde o significado preciso sempre estará claro a partir do contexto. Observe que o Lema 5.3 permanece válido neste caso [^1].

#### 5.1.4 Linear combinations
Seja $m \in N$ e sejam $(\Phi_i)_{i=1}^m$ redes neurais ReLU que possuem arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_i+1})$, respectivamente [^1]. Assumimos que $d_{L_1+1} = ... = d_{L_m+1}$, i.e., todos os $\Phi_1,..., \Phi_m$ possuem a mesma dimensão de saída. Para escalares $a_j \in R$, desejamos construir uma rede neural ReLU $\sum_{j=1}^m a_j \Phi_j$ realizando a função [^1]

$$R^{\sum_{j=1}^m d_0} \rightarrow R^{d_{L_1+1}}$$\
$$(x_1,...,x_m) \rightarrow \sum_{j=1}^m a_j \Phi_j(x_j).$$

Isto corresponde à paralelização $(\Phi_1,..., \Phi_m)$ composta com a transformação linear $(z_1,...,z_m) \rightarrow \sum_{j=1}^m a_j z_j$ [^1]. O seguinte resultado se mantém [^1].

**Lemma 5.4 (Linear combinations)**. Seja $m \in N$ e sejam $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, ..., d_{L_i+1})$, respectivamente. Assumimos que $d_{L_1+1} = ... = d_{L_m+1}$, seja $a \in R^m$ e defina $L_{max} := max_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m a_j \Phi_j$ tal que $(\sum_{j=1}^m a_j \Phi_j)(x) = \sum_{j=1}^m a_j \Phi_j(x)$ para todo $x = (x_j)_{j=1}^m \in R^{\sum_{i=1}^m d_0}$. Além disso, [^1]

$$width(\sum_{j=1}^m a_j \Phi_j) \leq 2 \sum_{j=1}^m width(\Phi_j),$$
$$depth(\sum_{j=1}^m a_j \Phi_j) = max_{j \leq m} depth(\Phi_j),$$
$$size(\sum_{j=1}^m a_j \Phi_j) \leq 2\sum_{j=1}^m size(\Phi_j) + 2\sum_{j=1}^m (L_{max} - L_j)d_0 + 1.$$

*Proof*. A construção de $\sum_{j=1}^m a_j \Phi_j$ é análoga à de $(\Phi_1,..., \Phi_m)$, i.e., primeiro definimos a combinação linear de redes neurais com a mesma profundidade. Então os pesos são escolhidos como em (5.1.2), mas com a última transformação linear substituída por [^1]

$$(a_1W_1^{(L)}, ..., a_mW_m^{(L)}, \sum_{j=1}^m a_jb_j^{(L)}).$$

Para profundidades gerais, definimos a soma das redes neurais como sendo a soma das redes neurais estendidas $\tilde{\Phi_i}$ como em (5.1.3). Todas as afirmações do lema seguem imediatamente desta construção [^1].

No caso $d_1 = ... = d_m =: d_0$ (todas as redes neurais possuem a mesma dimensão de entrada), também consideraremos combinações lineares com entradas compartilhadas, i.e., uma rede neural realizando [^1]

$$x \rightarrow \sum_{j=1}^m a_j \Phi_j(x) para x \in R^{d_0}.$$

### Conclusão

Este capítulo estabeleceu as bases para a análise de redes neurais ReLU, explorando a função de ativação ReLU, a manipulação básica dessas redes, e as relações entre elas e funções *piecewise linear*. Os resultados apresentados formam um ponto de partida crucial para entender as capacidades de aproximação das redes ReLU, que serão exploradas em seções posteriores.

### Referências

[^1]: Capítulo 5 do texto fornecido.

<!-- END -->