## Capítulo 5.1: Cálculo Básico com Redes ReLU: Um Framework para Manipulação e Complexidade

### Introdução
Este capítulo explora a combinação e manipulação de redes neurais ReLU, um tópico fundamental para a construção de redes mais complexas a partir de componentes mais simples [^1]. Como vimos anteriormente, a função de ativação ReLU, introduzida na Seção 2.3, é amplamente utilizada devido à sua simplicidade e capacidade de mitigar os problemas de gradientes evanescentes e explosivos [^1]. No entanto, a natureza *piecewise linear* da ReLU exige uma análise diferente daquela utilizada para funções de ativação mais suaves, que dependem da aproximação das derivadas da função de ativação para emular polinômios [^1].

Em continuidade ao trabalho formalizado a partir de [172], este capítulo adota um framework que permite rastrear o número de parâmetros da rede durante manipulações básicas, como somar ou compor duas redes neurais [^1]. Este framework é crucial para limitar a complexidade da rede ao construir arquiteturas mais elaboradas. As operações básicas que sustentam todas as construções são: reprodução de uma identidade, composição, paralelização e combinações lineares [^1].

### Conceitos Fundamentais

#### 5.1.1 Identidade

A capacidade de reproduzir uma identidade é fundamental para estender redes neurais e facilitar operações de composição eficientes. O seguinte lema formaliza essa capacidade para redes ReLU [^1].

**Lema 5.1 (Identidade).** Seja $L \in \mathbb{N}$. Então, existe uma rede neural ReLU $\Phi_{id}: \mathbb{R}^d \to \mathbb{R}^d$ tal que $\Phi_{id}(x) = x$ para todo $x \in \mathbb{R}^d$. Além disso, $depth(\Phi_{id}) = L$, $width(\Phi_{id}) = 2d$ e $size(\Phi_{id}) = 2d \cdot (L+1)$ [^1].

*Prova.* Escrevendo $I_d \in \mathbb{R}^{d \times d}$ para a matriz identidade, escolhemos os pesos
$$
(W^{(0)}, b^{(0)}), \dots, (W^{(L)}, b^{(L)}) := \underbrace{((I_{2d}, 0), \dots, (I_{2d}, 0))}_{L-1 \text{ vezes}}, ((I_d, -I_d), 0)
$$
Usando que $x = \sigma_{ReLU}(x) - \sigma_{ReLU}(-x)$ para todo $x \in \mathbb{R}$ e $\sigma_{ReLU}(x) = x$ para todo $x \geq 0$, é óbvio que a rede neural $\Phi_{id}$ associada aos pesos acima satisfaz a afirmação do lema. $\blacksquare$

<!-- Corolário: A identidade pode ser representada exatamente com redes ReLU -->

#### 5.1.2 Composição

A composição de redes neurais é outra operação fundamental. Como vimos na Proposição 2.3, podemos produzir uma composição de duas redes neurais, e a função resultante também é uma rede neural [^1]. No entanto, a Proposição 2.3 não estudou o tamanho das redes neurais resultantes. Para funções de ativação ReLU, essa composição pode ser feita de forma eficiente, levando a uma rede neural que tem até uma constante a mais do que o número de pesos das duas redes iniciais [^1].

Assuma que temos duas redes neurais $\Phi_1, \Phi_2$ com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_1+1})$ e $(\sigma_{ReLU}; d_0, \dots, d_{L_2+1})$, respectivamente [^1]. Além disso, assumimos que elas têm pesos e *biases* dados por
$$
(W_1^{(0)}, b_1^{(0)}), \dots, (W_1^{(L_1)}, b_1^{(L_1)}), \quad \text{e} \quad (W_2^{(0)}, b_2^{(0)}), \dots, (W_2^{(L_2)}, b_2^{(L_2)}),
$$
respectivamente [^1]. Se a dimensão de saída $d_{L_1+1}$ de $\Phi_1$ é igual à dimensão de entrada $d_0$ de $\Phi_2$, podemos definir dois tipos de concatenações: Primeiro, $\Phi_2 \circ \Phi_1$ é a rede neural com pesos e *biases* dados por
$$
(W_1^{(0)}, b_1^{(0)}), \dots, (W_1^{(L_1-1)}, b_1^{(L_1-1)}), (W_2^{(0)} W_1^{(L_1)}, W_2^{(0)} b_1^{(L_1)} + b_2^{(0)}), (W_2^{(1)}, b_2^{(1)}), \dots, (W_2^{(L_2)}, b_2^{(L_2)})
$$
Segundo, $\Phi_2 \bullet \Phi_1$ é a rede neural definida como $\Phi_2 \circ \Phi_{id} \circ \Phi_1$ [^1]. Em termos de pesos e *biases*, $\Phi_2 \bullet \Phi_1$ é dado por
$$
\left(
\begin{array}{c}
(W_1^{(0)}, b_1^{(0)}), \dots, (W_1^{(L_1-1)}, b_1^{(L_1-1)}),
\begin{pmatrix}
W_2^{(0)} \\\\ -W_2^{(0)}
\end{pmatrix}
W_1^{(L_1)},
\begin{pmatrix}
W_2^{(0)} \\\\ -W_2^{(0)}
\end{pmatrix}
b_1^{(L_1)} +
\begin{pmatrix}
b_2^{(0)} \\\\ b_2^{(0)}
\end{pmatrix}
\right), (W_2^{(1)}, b_2^{(1)}), \dots, (W_2^{(L_2)}, b_2^{(L_2)})
\end{array}
\right)
$$
O seguinte lema coleta as propriedades da construção acima [^1].

**Lema 5.2 (Composição).** Sejam $\Phi_1, \Phi_2$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_1+1})$ e $(\sigma_{ReLU}; d_0, \dots, d_{L_2+1})$. Assuma $d_{L_1+1} = d_0$. Então $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^d$. Além disso,
*   $width(\Phi_2 \circ \Phi_1) \leq max\{width(\Phi_1), width(\Phi_2)\}$,
*   $depth(\Phi_2 \circ \Phi_1) = depth(\Phi_1) + depth(\Phi_2)$,
*   $size(\Phi_2 \circ \Phi_1) \leq size(\Phi_1) + size(\Phi_2) + (d_{L_1} + 1)d_0$,
e
*   $width(\Phi_2 \bullet \Phi_1) \leq 2max\{width(\Phi_1), width(\Phi_2)\}$,
*   $depth(\Phi_2 \bullet \Phi_1) = depth(\Phi_1) + depth(\Phi_2) + 1$,
*   $size(\Phi_2 \bullet \Phi_1) \leq 2(size(\Phi_1) + size(\Phi_2))$ [^1].

*Prova.* O fato de que $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^d$ segue imediatamente da construção. O mesmo pode ser dito para os limites de largura e profundidade. Para confirmar o limite de tamanho, notamos que $W_2^{(0)}W_1^{(L_1)} \in \mathbb{R}^{d \times d_{L_1}}$ e, portanto, $W_2^{(0)}W_1^{(L_1)}$ tem no máximo $d^2 \times d_{L_1}$ entradas (não nulas) [^1]. Além disso, $W_2^{(0)}b_1^{(L_1)} + b_2^{(0)} \in \mathbb{R}^d$. Assim, a $L_1$-ésima camada de $\Phi_2 \circ \Phi_1(x)$ tem no máximo $d^2 \times (1 + d_{L_1})$ entradas. O resto é óbvio a partir da construção. $\blacksquare$

Interpretar transformações lineares como redes neurais de profundidade 0 torna o lema anterior válido também no caso em que $\Phi_1$ ou $\Phi_2$ é um mapeamento linear [^1].

#### 5.1.3 Paralelização

A paralelização de redes neurais permite construir redes que operam em múltiplos inputs simultaneamente [^1]. Dado um conjunto de redes neurais $(\Phi_i)_{i=1}^m$ com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$, o objetivo é construir uma rede neural $(\Phi_1, \dots, \Phi_m)$ tal que
$$
(\Phi_1, \dots, \Phi_m): \mathbb{R}^{\sum_{i=1}^m d_0} \to \mathbb{R}^{\sum_{i=1}^m d_{L_i+1}}
$$
$$
(x_1, \dots, x_m) \mapsto (\Phi_1(x_1), \dots, \Phi_m(x_m))
$$
Para fazer isso, primeiro assumimos que $L_1 = \dots = L_m = L$ e definimos $(\Phi_1, \dots, \Phi_m)$ através da seguinte sequência de *tuples* peso-bias [^1]:

$$
\left(
\begin{array}{c}
\begin{pmatrix}
W_1^{(0)} & & \\
& \ddots & \\
& & W_m^{(0)}
\end{pmatrix},
\begin{pmatrix}
b_1^{(0)} \\
\vdots \\
b_m^{(0)}
\end{pmatrix}
\right), \dots,
\left(
\begin{array}{c}
\begin{pmatrix}
W_1^{(L)} & & \\
& \ddots & \\
& & W_m^{(L)}
\end{pmatrix},
\begin{pmatrix}
b_1^{(L)} \\
\vdots \\
b_m^{(L)}
\end{pmatrix}
\end{array}
\right)
$$
onde estas matrizes são entendidas como bloco-diagonal preenchidas com zeros [^1]. Para o caso geral onde o $\Phi_j$ pode ter profundidades diferentes, seja $L_{max} := max_{1 \leq i \leq m} L_i$ e $I := \{1 \leq i \leq m | L_i < L_{max}\}$. Para $j \in I^c$ set $\tilde{\Phi}_j := \Phi_j$, e para cada $j \in I$
$$
\tilde{\Phi}_j := \Phi_{L_{max}-L_i} \circ \Phi_j
$$
Finalmente,
$$
(\Phi_1, \dots, \Phi_m) := (\tilde{\Phi}_1, \dots, \tilde{\Phi}_m)
$$
O seguinte lema coleta as propriedades da paralelização [^1].

**Lema 5.3 (Paralelização).** Seja $m \in \mathbb{N}$ e $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$, respectivamente. Então a rede neural $(\Phi_1, \dots, \Phi_m)$ satisfaz
$$
(\Phi_1, \dots, \Phi_m)(x) = (\Phi_1(x_1), \dots, \Phi_m(x_m)) \quad \text{para todo} \quad x \in \mathbb{R}^{\sum_{i=1}^m d_0}
$$
Além disso, com $L_{max} := max_{j \leq m} L_j$ vale que
*   $width((\Phi_1, \dots, \Phi_m)) \leq 2 \sum_{j=1}^m width(\Phi_j)$,
*   $depth((\Phi_1, \dots, \Phi_m)) = max_{j \leq m} depth(\Phi_j)$,
*   $size((\Phi_1, \dots, \Phi_m)) \leq 2 \sum_{j=1}^m size(\Phi_j) + 2\sum_{j=1}^m (L_{max} - L_j)d_0 + 1$.

*Prova.* Todas as afirmações, exceto para a delimitação do tamanho, seguem imediatamente da construção. Para obter a delimitação do tamanho, notamos que, por construção, os tamanhos de $(\tilde{\Phi}_i)$, em (5.1.3) serão simplesmente adicionados. O tamanho de cada $\tilde{\Phi}_i$ pode ser delimitado com o Lema 5.2. $\blacksquare$

Se todas as dimensões de entrada $d_i = \dots = d_m =: d_0$ são as mesmas, também usaremos a paralelização com entradas compartilhadas para perceber a função $x \to (\Phi_1(x), \dots, \Phi_m(x))$ de $\mathbb{R}^{d_0} \to \mathbb{R}^{d_{L_1+1} + \dots + d_{L_m+1}}$. Em termos da construção (5.1.2), a única mudança necessária é que a matriz bloco-diagonal $diag(W_1^{(0)}, \dots, W_m^{(0)})$ torna-se a matriz em $\mathbb{R}^{\sum_{i=1}^m d_i \times d_0}$ que empilha $W_1^{(0)}, \dots, W_m^{(0)}$ um em cima do outro [^1]. Da mesma forma, permitiremos que $\Phi_i$ tome apenas algumas das entradas de $x$ como entrada [^1]. Para paralelização com entradas compartilhadas, usaremos a mesma notação $(\Phi_j)_{j=1}^m$ como antes, onde o significado preciso sempre será claro a partir do contexto. Observe que o Lema 5.3 permanece válido neste caso [^1].

#### 5.1.4 Combinações Lineares

As combinações lineares de redes neurais permitem construir novas redes que realizam somas ponderadas das saídas das redes originais [^1]. Dado um conjunto de redes neurais $(\Phi_i)_{i=1}^m$ com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$, o objetivo é construir uma rede neural $\sum_{j=1}^m \alpha_j \Phi_j$ que realize a função
$$
(x_1, \dots, x_m) \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x_j)
$$
Isso corresponde à paralelização $(\Phi_1, \dots, \Phi_m)$ composta com a transformação linear $(z_1, \dots, z_m) \mapsto \sum_{j=1}^m \alpha_j z_j$ [^1].

**Lema 5.4 (Combinações Lineares).** Seja $m \in \mathbb{N}$ e $(\Phi_i)_{i=1}^m$ redes neurais com arquiteturas $(\sigma_{ReLU}; d_0, \dots, d_{L_i+1})$. Assuma que $d_{L_1+1} = \dots = d_{L_m+1}$, seja $\alpha \in \mathbb{R}^m$ e defina $L_{max} := max_{j \leq m} L_j$. Então, existe uma rede neural $\sum_{j=1}^m \alpha_j \Phi_j$ tal que $(\sum_{j=1}^m \alpha_j \Phi_j)(x) = \sum_{j=1}^m \alpha_j \Phi_j(x)$ para todo $x = (x_j)_{j=1}^m \in \mathbb{R}^{\sum_{j=1}^m d_0}$. Além disso,
*   $width(\sum_{j=1}^m \alpha_j \Phi_j) \leq 2 \sum_{j=1}^m width(\Phi_j)$,
*   $depth(\sum_{j=1}^m \alpha_j \Phi_j) = max_{j \leq m} depth(\Phi_j)$,
*   $size(\sum_{j=1}^m \alpha_j \Phi_j) \leq 2 \sum_{j=1}^m size(\Phi_j) + 2 \sum_{j=1}^m (L_{max} - L_j) d_0 + 1$.

*Prova.* A construção de $\sum_{j=1}^m \alpha_j \Phi_j$ é análoga à de $(\Phi_1, \dots, \Phi_m)$, isto é, primeiro definimos a combinação linear de redes neurais com a mesma profundidade. Então, os pesos são escolhidos como em (5.1.2), mas com a última transformação linear substituída por
$$
(\alpha_1 W_1^{(L_1)} \dots \alpha_m W_m^{(L_m)}), \quad \sum_{j=1}^m \alpha_j b_j^{(L_j)}
$$
Para profundidades gerais, definimos a soma das redes neurais como a soma das redes neurais estendidas $\tilde{\Phi}_i$ como em (5.1.3). Todas as afirmações do lema seguem imediatamente desta construção. $\blacksquare$

No caso de $d_1 = \dots = d_m =: d_0$ (todas as redes neurais têm a mesma dimensão de entrada), também consideramos combinações lineares com entradas compartilhadas, isto é, uma rede neural que realiza
$$
x \mapsto \sum_{j=1}^m \alpha_j \Phi_j(x) \quad \text{para} \quad x \in \mathbb{R}^{d_0}
$$

### Conclusão

Este capítulo formalizou as operações básicas para manipular redes neurais ReLU e forneceu limites sobre a complexidade das redes resultantes. A reprodução da identidade, composição, paralelização e combinações lineares formam um conjunto fundamental de ferramentas para construir redes neurais ReLU complexas a partir de componentes mais simples. Os lemas apresentados neste capítulo fornecem uma base teórica para entender e controlar a complexidade de redes neurais ReLU, que é crucial para garantir a generalização e evitar o *overfitting*.

### Referências
[^1]: Trecho relevante do contexto fornecido.
<!-- END -->