## Aproximação da Função Quadrada e Multiplicação com Redes Neurais ReLU Profundas

### Introdução
Este capítulo explora a capacidade das redes neurais ReLU profundas em aproximar funções suaves com altas taxas de convergência, complementando as observações do capítulo anterior [^1]. O foco principal é demonstrar como as redes ReLU profundas podem aproximar eficientemente a função quadrada ($x \rightarrow x^2$) e, consequentemente, a multiplicação, abrindo caminho para a aproximação de polinômios e funções suaves [^77].

### Conceitos Fundamentais

#### Aproximação da Função Quadrada
A função quadrada $x \rightarrow x^2$ pode ser aproximada eficientemente por uma rede neural profunda. Define-se uma função linear por partes $s_n(x)$ com pontos de quebra $x_{n,j} = j2^{-n}$ [^77]. A proposição 7.1 [^77] introduz a função $s_n(x)$:\n
$$ s_n(x) = \sum_{j=1}^{n} \frac{h_j(x)}{2^{2j}} $$

onde $s_n(x)$ é uma função linear por partes em $[0,1]$ com pontos de quebra $x_{n,j} = j2^{-n}$, $j = 0, ..., 2^n$. Além disso, $s_n(x_{n,k}) = x_{n,k}^2$ para todo $k = 0, ..., 2^n$.

**Lemma 7.2** [^78]: Para $n \in \mathbb{N}$, tem-se que:\n
$$ \sup_{x \in [0,1]} |x^2 - s_n(x)| \leq 2^{-2n-1} $$
Além disso, $s_n \in \mathcal{N}_1(\sigma_{ReLU}; n, 3)$, com $\text{size}(s_n) \leq 7n$ e $\text{depth}(s_n) = n$.

A prova [^78] demonstra que $s_n$ aproxima $x^2$ uniformemente em $[0,1]$ com erro exponencialmente decrescente em relação ao tamanho da rede neural.

#### Multiplicação
A aproximação eficiente da multiplicação permite a aproximação eficiente de polinômios. Isso se baseia na identidade de polarização para expressar o produto de dois números como uma soma de quadrados [^79]:

$$ xy = \frac{(x+y)^2 - (x-y)^2}{4} $$

A Proposição 7.3 [^80] estabelece que para todo $\epsilon > 0$, existe uma rede neural ReLU $\Phi_{\epsilon}: [-1, 1]^2 \rightarrow [-1, 1]$ tal que:\n
$$ \sup_{x,y \in [-1, 1]} |xy - \Phi_{\epsilon}(x, y)| \leq \epsilon $$

e $\text{size}(\Phi_{\epsilon}) < C \cdot (1 + |\log(\epsilon)|)$ e $\text{depth}(\Phi_{\epsilon}) < C \cdot (1 + |\log(\epsilon)|)$ para uma constante $C > 0$ independente de $\epsilon$. Além disso, $\Phi_{\epsilon}(x, y) = 0$ se $x=0$ ou $y=0$.

A Proposição 7.4 [^80] generaliza este resultado para $n$ entradas, mostrando que a multiplicação de $n$ números pode ser aproximada por uma rede neural ReLU com tamanho e profundidade que crescem logaritmicamente com $n$ e o erro de aproximação.

### Conclusão
Este capítulo demonstra que as redes neurais ReLU profundas possuem a capacidade de aproximar eficientemente a função quadrada e a multiplicação. Essa capacidade é fundamental para a aproximação de polinômios e, consequentemente, para a aproximação de funções suaves, abrindo caminho para a construção de redes neurais que podem atingir altas taxas de convergência [^81]. A profundidade da rede desempenha um papel crucial na obtenção dessas taxas de convergência, em contraste com as limitações das redes neurais rasas [^86].

### Referências
[^1]: Capítulo anterior
[^77]: Seção 7.1
[^78]: Lemma 7.2
[^79]: Seção 7.2
[^80]: Lemma 7.3 e 7.4
[^81]: Seção 7.3
[^86]: Seção 7.3

<!-- END -->