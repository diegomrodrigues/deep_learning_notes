## Capítulo 7: Aproximação de Funções Suaves com Redes Neurais Profundas ReLU

### Introdução
Como vimos anteriormente no Capítulo 6, redes neurais ReLU profundas são um pré-requisito necessário para aproximar funções suaves com altas taxas de convergência [^77]. Expandindo o conceito apresentado, este capítulo analisa qual profundidade é suficiente para alcançar boas taxas de aproximação para funções suaves. Em continuidade ao Capítulo 4, onde reconstruímos funções baseadas em polinômios, como B-splines de ordem superior, exploraremos como as redes ReLU profundas podem aproximar polinômios de forma eficiente. Essa abordagem contorna a limitação inerente da natureza *piecewise linear* da ReLU [^1]. Notavelmente, redes ReLU profundas se mostram eficientes na produção das funções *sawtooth* discutidas no Lemma 6.5 [^77]. Este capítulo é baseado no artigo seminal de 2017 de Dmitry Yarotsky [^77].

### Conceitos Fundamentais

A principal observação é que a representação eficiente de funções *sawtooth* está intimamente ligada à aproximação da **função quadrática**, o que permite aproximações muito eficientes de funções polinomiais [^77].

#### 7.1 A Função Quadrática
Nesta seção, demonstraremos como a função quadrática $x \rightarrow x^2$ pode ser aproximada de forma eficiente por uma rede neural profunda [^77].

**Proposição 7.1.** Seja $n \in \mathbb{N}$. Então,
$$\ns_n(x) = x - \sum_{j=1}^{n} \frac{h_j(x)}{2^{2j}}\n$$
é uma função linear *piecewise* em $[0,1]$ com pontos de quebra $x_{n,j} = j2^{-n}$, $j = 0, \dots, 2^n$. Além disso, $s_n(x_{n,k}) = x_{n,k}^2$ para todo $k = 0, \dots, 2^n$, ou seja, $s_n$ é a interpolante linear *piecewise* de $x^2$ em $[0,1]$ [^77].

*Prova.* A prova se dá por indução. O caso $n=1$ pode ser verificado diretamente. Assumindo que o resultado vale para $s_n$ e seja $k \in \\{0, \dots, 2^{n+1}\\}$. Pelo Lemma 6.5, $h_{n+1}(x_{n+1, k})=0$ sempre que $k$ é par. Logo, para $k$ par:
$$\ns_{n+1}(x_{n+1, k}) = x - \sum_{j=1}^{n+1} \frac{h_j(x_{n+1, k})}{2^{2j}} = s_n(x_{n+1, k}) - \frac{h_{n+1}(x_{n+1, k})}{2^{2(n+1)}} = s_n(x_{n+1, k}) = x_{n+1, k}^2\n$$
onde usamos que $s_n(x_{n+1, k})=s_n(x_{n, k/2}) = x_{n, k/2}^2 = k^2 2^{-2(n+1)}$. Agora, seja $k \in \\{1, \dots, 2^{n+1} - 1\\}$ ímpar. Então, pelo Lemma 6.5, $h_{n+1}(x_{n+1, k}) = 1$. Além disso, como $s_n$ é linear em $[x_{n, (k-1)/2}, x_{n, (k+1)/2}] = [x_{n+1, k-1}, x_{n+1, k+1}]$ e $x_{n+1, k}$ é o ponto médio deste intervalo:
$$\ns_{n+1}(x_{n+1, k}) = s_n(x_{n+1, k}) - \frac{h_{n+1}(x_{n+1, k})}{2^{2(n+1)}} = \frac{1}{2} (x_{n+1, k-1}^2 + x_{n+1, k+1}^2) - \frac{1}{2^{2(n+1)}} = \frac{1}{2} \left( \frac{(k-1)^2}{2^{2(n+1)}} + \frac{(k+1)^2}{2^{2(n+1)}} \right) - \frac{1}{2^{2(n+1)}} = \frac{k^2}{2^{2(n+1)}} = x_{n+1, k}^2\n$$
Isto completa a prova. $\blacksquare$

**Lemma 7.2.** Para $n \in \mathbb{N}$, temos
$$\n\sup_{x \in [0,1]} |x^2 - s_n(x)| \leq 2^{-2n-1}.\n$$
Ademais, $s_n \in \mathcal{N}_1(\sigma_{\text{ReLU}}; n, 3)$, e $\text{size}(s_n) \leq 7n$ e $\text{depth}(s_n) = n$ [^78].

*Prova.* Seja $e_n(x) := x^2 - s_n(x)$. Seja $x$ no intervalo $[x_{n,k}, x_{n,k+1}] = [k2^{-n}, (k+1)2^{-n}]$ de comprimento $2^{-n}$. Como $s_n$ é a interpolante linear de $x^2$ neste intervalo, temos
$$\n|e_n'(x)| = \left| 2x - \frac{x_{n,k+1}^2 - x_{n,k}^2}{x_{n,k+1} - x_{n,k}} \right| = \left| 2x - \frac{(k+1)^2 2^{-2n} - k^2 2^{-2n}}{2^{-n}} \right| = \left| 2x - \frac{2k+1}{2^n} \right| \leq \frac{1}{2^n}\n$$
Assim, $e_n: [0,1] \rightarrow \mathbb{R}$ tem constante de Lipschitz $2^{-n}$. Como $e_n(x_{n,k}) = 0$ para todo $k = 0, \dots, 2^n$, e o comprimento do intervalo $[x_{n,k}, x_{n,k+1}]$ é igual a $2^{-n}$, obtemos
$$\n\sup_{x \in [0,1]} |e_n(x)| \leq \frac{1}{2} 2^{-n} 2^{-n} = 2^{-2n-1}.\n$$
[^78]

#### 7.2 Multiplicação

De acordo com o Lemma 7.2, a profundidade pode auxiliar na aproximação de $x \rightarrow x^2$, o que, à primeira vista, parece um exemplo específico [^79]. No entanto, como discutiremos a seguir, isso abre um caminho para uma aproximação rápida de funções com alta regularidade, por exemplo, $C^k([0,1]^d)$ para algum $k > 1$ [^79]. A observação crucial é que, através da identidade de polarização, podemos escrever o produto de dois números como uma soma de quadrados:
$$\nxy = \frac{(x+y)^2 - (x-y)^2}{4}\n$$
para todo $x, y \in \mathbb{R}$. A aproximação eficiente da operação de multiplicação permite a aproximação eficiente de polinômios [^79].

#### 7.3 Funções $C^{k,s}$

Vamos agora discutir as implicações de nossas observações nas seções anteriores para a aproximação de funções na classe $C^{k,s}$ [^81].

**Definição 7.5.** Sejam $k \in \mathbb{N}_0$, $s \in [0,1]$ e $\Omega \subseteq \mathbb{R}^d$. Então,
$$\n||f||_{C^{k,s}(\Omega)} := \sup_{\substack{x \in \Omega \\\\ \alpha \in \mathbb{N}_0^d \\\\ ||\alpha|| \leq k}} |D^\alpha f(x)| + \sup_{\substack{x,y \in \Omega \\\\ \alpha \in \mathbb{N}_0^d \\\\ ||\alpha|| = k}} \frac{|D^\alpha f(x) - D^\alpha f(y)|}{||x-y||^s}\n$$
e denotamos por $C^{k,s}(\Omega)$ o conjunto de funções $f \in C^k(\Omega)$ para as quais $||f||_{C^{k,s}(\Omega)} < \infty$ [^81].

### Conclusão
Este capítulo demonstra que as redes neurais ReLU profundas são capazes de aproximar funções suaves com altas taxas de convergência [^86]. A representação eficiente da função quadrática, juntamente com a identidade de polarização, permite a aproximação eficiente de polinômios, que são ferramentas poderosas para aproximar funções suaves [^77]. Os resultados apresentados aqui fornecem uma base teórica para entender a eficácia das redes neurais profundas na aproximação de funções suaves e destacam a importância da profundidade para alcançar altas taxas de convergência [^86].

### Referências
[^1]: Capítulo 5: ReLU neural networks
[^77]: Yarotsky, D. (2017). Error bounds for approximations with deep ReLU networks. *Neural Networks*, *94*, 103-114.
[^78]: Lemma 7.2
[^79]: Section 7.2
[^81]: Section 7.3
[^86]: Remark 7.8
<!-- END -->