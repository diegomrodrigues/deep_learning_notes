## Composição de Redes Neurais ReLU

### Introdução
Este capítulo aprofunda a análise da composição de redes neurais, concentrando-se especificamente em redes neurais ReLU. Como vimos anteriormente, a composição de redes neurais é uma operação fundamental que permite a construção de modelos mais complexos a partir de blocos básicos [^43]. Aqui, exploraremos em detalhes como essa composição se manifesta em redes ReLU, analisando as propriedades das redes resultantes em termos de arquitetura e tamanho, expandindo o conceito apresentado na Proposition 2.3 [^44].

### Conceitos Fundamentais
A composição de redes neurais ReLU oferece uma maneira eficiente de construir modelos mais profundos. Considere duas redes neurais, $\Phi_1$ e $\Phi_2$, com arquiteturas (ReLU; $d_0, ..., d_{L_1+1}$) e (ReLU; $d_0, ..., d_{L_2+1}$), respectivamente [^44]. Assumimos que essas redes possuem pesos e biases dados por $(W^{(0)}_1, b^{(0)}_1), ..., (W^{(L_1)}_1, b^{(L_1)}_1)$ e $(W^{(0)}_2, b^{(0)}_2), ..., (W^{(L_2)}_2, b^{(L_2)}_2)$, respectivamente. Para que a composição seja possível, a dimensão de saída de $\Phi_1$ deve ser igual à dimensão de entrada de $\Phi_2$, ou seja, $d_{L_1+1} = d_0$ [^44].

Podemos definir duas formas de concatenar essas redes: $\Phi_2 \circ \Phi_1$ e $\Phi_2 \bullet \Phi_1$. A primeira, $\Phi_2 \circ \Phi_1$, representa a composição direta, enquanto a segunda, $\Phi_2 \bullet \Phi_1$, introduz uma camada de identidade intermediária [^44].

**Composição Direta ($\Phi_2 \circ \Phi_1$):**
Nesta forma de composição, a saída de $\Phi_1$ é diretamente alimentada como entrada para $\Phi_2$. Os pesos e biases da rede resultante são dados por:

$$(W^{(0)}_1, b^{(0)}_1), ..., (W^{(L_1-1)}_1, b^{(L_1-1)}_1), (W^{(0)}_2W^{(L_1)}_1, W^{(0)}_2b^{(L_1)}_1 + b^{(0)}_2), (W^{(1)}_2, b^{(1)}_2), ..., (W^{(L_2)}_2, b^{(L_2)}_2)$$

**Composição com Identidade Intermediária ($\Phi_2 \bullet \Phi_1$):**
Esta composição insere uma rede de identidade entre $\Phi_1$ e $\Phi_2$. A rede resultante tem pesos e biases dados por:

$$(W^{(0)}_1, b^{(0)}_1), ..., (W^{(L_1-1)}_1, b^{(L_1-1)}_1), ((W^{(L_1)}_1, -W^{(L_1)}_1), b^{(L_1)}_1 - b^{(L_1)}_1), (W^{(0)}_2, b^{(0)}_2), ..., (W^{(L_2)}_2, b^{(L_2)}_2)$$

A introdução da identidade intermediária permite um controle mais preciso sobre a arquitetura da rede resultante [^44].

**Lemma 5.2 (Composition)** [^45]: *Sejam $\Phi_1, \Phi_2$ redes neurais com arquiteturas (ReLU; $d_0, ..., d_{L_1+1}$) e (ReLU; $d_0, ..., d_{L_2+1}$). Assuma que $d_{L_1+1} = d_0$. Então $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^{d_0}$. Além disso,*\n\n*   *width($\Phi_2 \circ \Phi_1$) ≤ max{width($\Phi_1$), width($\Phi_2$)},*\n*   *depth($\Phi_2 \circ \Phi_1$) = depth($\Phi_1$) + depth($\Phi_2$),*\n*   *size($\Phi_2 \circ \Phi_1$) < size($\Phi_1$) + size($\Phi_2$) + ($d_{L_1} + 1$)d,*\n\n*e*\n\n*   *width($\Phi_2 \bullet \Phi_1$) ≤ 2max{width($\Phi_1$), width($\Phi_2$)},*\n*   *depth($\Phi_2 \bullet \Phi_1$) = depth($\Phi_1$) + depth($\Phi_2$) + 1,*\n*   *size($\Phi_2 \bullet \Phi_1$) ≤ 2(size($\Phi_1$) + size($\Phi_2$)).*\n\n**Proof**: A igualdade $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ segue diretamente da construção. As relações para width e depth também são imediatas. Para confirmar a relação para size, observe que $W^{(L_1)}_2W^{(L_1)}_1 \in \mathbb{R}^{d_2 \times d_{L_1}}$ e, portanto, possui no máximo $d_2^2 \times d_{L_1}$ entradas não nulas. Além disso, $W^{(0)}_2b^{(L_1)}_1 + b^{(0)}_2 \in \mathbb{R}^{d_0}$. Assim, a $L_1$-ésima camada de $\Phi_2 \circ \Phi_1(x)$ possui no máximo $d_2^2 \times (1 + d_{L_1})$ entradas. O restante segue da construção. $\blacksquare$

### Conclusão
A composição de redes neurais ReLU, seja de forma direta ou com a introdução de uma camada de identidade, é uma técnica poderosa para a construção de modelos complexos. O Lemma 5.2 [^45] fornece limites precisos para a arquitetura e o tamanho das redes resultantes, permitindo um controle refinado sobre a complexidade do modelo. Este conhecimento é fundamental para o desenvolvimento de redes neurais ReLU eficientes e com bom desempenho.
<!-- END -->