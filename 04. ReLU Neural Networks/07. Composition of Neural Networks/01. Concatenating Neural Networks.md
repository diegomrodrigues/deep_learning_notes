## Capítulo 5.1.2: Composição de Redes Neurais ReLU

### Introdução
Em continuidade ao estudo da composição de redes neurais, este capítulo aprofunda a análise da composição de duas redes neurais ReLU, explorando a arquitetura resultante e suas propriedades, como largura, profundidade e tamanho. Este tópico se baseia nos conceitos de redes neurais ReLU introduzidos na Seção 2.3 [^1] e nas operações básicas de cálculo ReLU apresentadas na Seção 5.1 [^1]. A composição de redes neurais é uma operação fundamental para construir modelos mais complexos a partir de componentes mais simples, permitindo a criação de funções de aproximação mais sofisticadas.

### Conceitos Fundamentais
Consideremos duas redes neurais, **Φ1** e **Φ2**, com arquiteturas *(σReLU; d0, ..., dL1+1)* e *(σReLU; d0, ..., dL2+1)*, respectivamente [^2]. Assumimos que a dimensão de saída *dL1+1* de **Φ1** é igual à dimensão de entrada *d0* de **Φ2**. Além disso, as redes possuem pesos e bias dados por *(W(0)1, b(0)1), ..., (W(L1)1, b(L1)1)* e *(W(0)2, b(0)2), ..., (W(L2)2, b(L2)2)*, respectivamente [^2].

Podemos definir dois tipos de concatenação:

1.  **Concatenação Direta (Φ2 ◦ Φ1):** Esta é a rede neural obtida pela composição direta das funções representadas por **Φ1** e **Φ2**. Os pesos e biases da rede resultante são dados por:

    $$(W_1^{(0)}, b_1^{(0)}),..., (W_1^{(L_1-1)}, b_1^{(L_1-1)}), (W_2^{(0)}W_1^{(L_1)}, W_2^{(0)}b_1^{(L_1)} + b_2^{(0)}), (W_2^{(1)}, b_2^{(1)}),..., (W_2^{(L_2)}, b_2^{(L_2)}).$$ [^2]
2.  **Concatenação com Identidade (Φ2 • Φ1):** Esta é a rede neural definida como **Φ2 ◦ Φid ◦ Φ1**, onde **Φid** representa a função identidade. Em termos de pesos e biases, esta composição é dada por:

    $$\left( \begin{array}{c} (W_1^{(0)}, b_1^{(0)}),..., (W_1^{(L_1-1)}, b_1^{(L_1-1)}), \\\\ \left( \left( \begin{array}{c} W_2^{(0)} \\\\ -W_2^{(0)} \end{array} \right), b_2^{(0)} \right), (W_2^{(1)}, b_2^{(1)}),..., (W_2^{(L_2)}, b_2^{(L_2)}) \end{array} \right).$$ [^3]

O **Lemma 5.2** [^3] sumariza as propriedades das construções acima, conforme segue:

**Lemma 5.2 (Composition).** *Sejam Φ1, Φ2 redes neurais com arquiteturas (σReLU; d0, ..., dL1+1) e (σReLU; d0, ..., dL2+1). Assuma que dL1+1 = d0. Então Φ2 ◦ Φ1(x) = Φ2 • Φ1(x) = Φ2(Φ1(x)) para todo x ∈ Rd. Além disso,*\
*width(Φ2 ◦ Φ1) ≤ max{width(Φ1), width(Φ2)},*\

*depth(Φ2 ◦ Φ1) = depth(Φ1) + depth(Φ2),*\

*size(Φ2 ◦ Φ1) ≤ size(Φ1) + size(Φ2) + (dL1 + 1)d0,*\

*e*\

*width(Φ2 • Φ1) ≤ 2max{width(Φ1), width(Φ2)},*\

*depth(Φ2 • Φ1) = depth(Φ1) + depth(Φ2) + 1,*\

*size(Φ2 • Φ1) ≤ 2(size(Φ1) + size(Φ2)).*\

*Proof:* O fato de que Φ2 ◦ Φ1(x) = Φ2 • Φ1(x) = Φ2(Φ1(x)) para todo *x ∈ R<sup>d</sup>* segue imediatamente da construção. O mesmo pode ser dito para os limites de largura e profundidade. Para confirmar o limite de tamanho, notamos que *W(0)2W(L1)1 ∈ R<sup>d x dL1</sup>* e, portanto, *W(0)2W(L1)1* tem no máximo *d<sup>2</sup> x dL1* entradas (não nulas). Além disso, *W(0)2b(L1)1 + b(0)2 ∈ R<sup>d</sup>*. Assim, a *L1*-ésima camada de *Φ2 ◦ Φ1(x)* tem no máximo *d<sup>2</sup> x (1 + dL1)* entradas. O resto é óbvio a partir da construção. $\blacksquare$

O Lemma 5.2 também é válido se *Φ1* ou *Φ2* for um mapeamento linear, interpretando transformações lineares como redes neurais de profundidade 0 [^3].

### Conclusão
A composição de redes neurais ReLU permite a construção de modelos mais complexos, mas é crucial controlar o crescimento do número de parâmetros e a complexidade da rede. As concatenações direta e com identidade oferecem diferentes maneiras de combinar redes, com diferentes implicações para a largura, profundidade e tamanho da rede resultante. Este entendimento é fundamental para o design eficiente de arquiteturas de redes neurais.

### Referências
[^1]: Capítulo 5: ReLU neural networks.
[^2]: Section 5.1.2 Composition.
[^3]: Lemma 5.2 (Composition).
<!-- END -->