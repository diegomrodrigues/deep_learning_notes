## Composição Eficiente de Redes Neurais ReLU e Limites de Complexidade

### Introdução
Este capítulo explora a composição de redes neurais utilizando a função de ativação ReLU, um tópico introduzido no Capítulo 2, Seção 2.3 [^1]. A simplicidade da ReLU e sua capacidade de mitigar o problema do desaparecimento e explosão dos gradientes a tornam uma escolha popular na prática [^1]. Como vimos anteriormente, um componente chave nas provas dos capítulos anteriores era a aproximação das derivadas da função de ativação para emular polinômios. No entanto, essa técnica não é diretamente aplicável à ReLU, pois ela é *piecewise linear* [^1]. Este capítulo se concentra em formalizar como combinar e manipular redes neurais ReLU de forma eficiente, expandindo sobre resultados preliminares como a Proposição 2.3 [^1].

### Conceitos Fundamentais
**Composição de Redes Neurais:**

Anteriormente, foi estabelecido na Proposição 2.3 que a composição de duas redes neurais resulta em outra rede neural [^2]. No entanto, a complexidade da rede resultante não foi explorada em detalhes. Para funções de ativação ReLU, a composição pode ser realizada de maneira eficiente, resultando em uma rede neural que possui um número de pesos não superior, até uma constante, à soma dos pesos das duas redes neurais iniciais [^2].

**Lemma 5.2 (Composition)** [^3]: Sejam $\Phi_1$ e $\Phi_2$ redes neurais com arquiteturas (ReLU; $d_0, ..., d_{L_1+1}$) e (ReLU; $d_0', ..., d_{L_2+1}$), respectivamente. Assumindo que $d_{L_1+1} = d_0'$, então $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^d$. Além disso,
$$
width(\Phi_2 \circ \Phi_1) \leq max\{width(\Phi_1), width(\Phi_2)\},
$$
$$
depth(\Phi_2 \circ \Phi_1) = depth(\Phi_1) + depth(\Phi_2),
$$
$$
size(\Phi_2 \circ \Phi_1) < size(\Phi_1) + size(\Phi_2) + (d_{L_1} + 1)d,
$$
e
$$
width(\Phi_2 \bullet \Phi_1) \leq 2max\{width(\Phi_1), width(\Phi_2)\},
$$
$$
depth(\Phi_2 \bullet \Phi_1) = depth(\Phi_1) + depth(\Phi_2) + 1,
$$
$$
size(\Phi_2 \bullet \Phi_1) \leq 2(size(\Phi_1) + size(\Phi_2)).
$$

**Prova do Lemma 5.2:**

A igualdade $\Phi_2 \circ \Phi_1(x) = \Phi_2 \bullet \Phi_1(x) = \Phi_2(\Phi_1(x))$ segue diretamente da construção das redes compostas [^3]. As relações de *width* e *depth* também são evidentes. Para confirmar os limites de *size*, observe que $W_2^{(0)}W_1^{(L_1)} \in \mathbb{R}^{d \times d_{L_1}}$ e, portanto, $W_2^{(0)}W_1^{(L_1)}$ tem no máximo $d^2 \times d_{L_1}$ entradas não nulas. Similarmente, $W_2^{(0)}b_1^{(L_1)} + b_2^{(0)} \in \mathbb{R}^d$. Portanto, a $L_1$-ésima camada de $\Phi_2 \circ \Phi_1(x)$ tem no máximo $d^2 \times (1 + d_{L_1})$ entradas. O restante segue da construção [^3]. $\blacksquare$

**Tipos de Concatenação:**

Dado que a dimensão de saída $d_{L_1+1}$ de $\Phi_1$ é igual à dimensão de entrada $d_0'$ de $\Phi_2$, podemos definir dois tipos de concatenações [^2]:

1.  **$\Phi_2 \circ \Phi_1$**: A rede neural com pesos e *biases* dados por
    $$
    (W_1^{(0)}, b_1^{(0)}), ..., (W_1^{(L_1-1)}, b_1^{(L_1-1)}), (W_2^{(0)}W_1^{(L_1)}, W_2^{(0)}b_1^{(L_1)} + b_2^{(0)}), (W_2^{(1)}, b_2^{(1)}), ..., (W_2^{(L_2)}, b_2^{(L_2)}).
    $$
2.  **$\Phi_2 \bullet \Phi_1$**: A rede neural definida como $\Phi_2 \circ \Phi_{id} \circ \Phi_1$, onde $\Phi_{id}$ representa a função identidade em $\mathbb{R}^d$. Seus pesos e *biases* são dados por
    $$
    \left(\begin{array}{cc}
    W_1^{(0)}, & b_1^{(0)}
    \end{array}\right), \ldots,\left(\begin{array}{cc}
    W_1^{\left(L_1-1\right)}, & b_1^{\left(L_1-1\right)}
    \end{array}\right),\left(\begin{array}{cc}
    W_2^{(0)}W_1^{\left(L_1\right)}, & W_2^{(0)} b_1^{\left(L_1\right)}+b_2^{(0)}
    \end{array}\right),\left(\begin{array}{cc}
    W_2^{(1)}, & b_2^{(1)}
    \end{array}\right), \ldots,\left(\begin{array}{cc}
    W_2^{\left(L_2\right)}, & b_2^{\left(L_2\right)}
    \end{array}\right)
    $$

**Interpretação com Mapeamentos Lineares:**

Interpretar transformações lineares como redes neurais de profundidade 0 torna o Lemma 5.2 válido mesmo quando $\Phi_1$ ou $\Phi_2$ são mapeamentos lineares [^3].

### Conclusão

Este capítulo demonstrou que a composição de redes neurais ReLU pode ser realizada de forma eficiente, com limites bem definidos para a profundidade, largura e tamanho da rede resultante. O Lemma 5.2 fornece uma base teórica para compreender como a complexidade das redes compostas se relaciona com a complexidade de suas redes constituintes. Além disso, a capacidade de interpretar transformações lineares como redes neurais de profundidade zero estende a aplicabilidade deste lema a uma gama mais ampla de arquiteturas de redes neurais.

### Referências
[^1]: Capítulo 5, p. 43
[^2]: Capítulo 5, p. 44
[^3]: Capítulo 5, p. 45
<!-- END -->