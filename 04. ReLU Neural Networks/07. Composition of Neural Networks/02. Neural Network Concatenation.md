## 5.1.2 Concatenação de Redes Neurais ReLU

### Introdução
Expandindo o conceito de composição de redes neurais apresentado em proposições anteriores [^2], este capítulo se aprofunda em duas formas específicas de concatenar redes neurais ReLU, explorando suas propriedades e implicações para a arquitetura e complexidade das redes resultantes. O foco é em como a estrutura piecewise linear da função de ativação ReLU permite composições eficientes e controladas, um contraste com funções de ativação mais suaves onde a análise pode ser fundamentalmente diferente [^1].

### Conceitos Fundamentais
Consideremos duas redes neurais, $\Phi_1$ e $\Phi_2$, com arquiteturas ReLU definidas como (ReLU; $d_0^1$, ..., $d_{L_1+1}^1$) e (ReLU; $d_0^2$, ..., $d_{L_2+1}^2$), respectivamente [^2]. Seus pesos e biases são representados por $(W_1^{(0)}, b_1^{(0)})$, ..., $(W_1^{(L_1)}, b_1^{(L_1)})$ e $(W_2^{(0)}, b_2^{(0)})$, ..., $(W_2^{(L_2)}, b_2^{(L_2)})$, respectivamente [^2]. Uma condição essencial para a concatenação é que a dimensão de saída de $\Phi_1$ seja igual à dimensão de entrada de $\Phi_2$, ou seja, $d_{L_1+1}^1 = d_0^2$ [^2]. Sob essa condição, duas formas de concatenação são definidas [^2]:

1.  **Concatenação Direta ($\Phi_2 \circ \Phi_1$)**:
    *   A primeira forma, denotada por $\Phi_2 \circ \Phi_1$, resulta em uma rede neural cujos pesos e biases são construídos combinando as camadas de $\Phi_1$ e $\Phi_2$ [^2]. Especificamente, as primeiras $L_1$ camadas são herdadas de $\Phi_1$, e a $(L_1 + 1)$-ésima camada é uma combinação das últimas camadas de $\Phi_1$ e a primeira camada de $\Phi_2$. As camadas restantes são de $\Phi_2$. Matematicamente, os pesos e biases são dados por [^2]:
        $$ (W_1^{(0)}, b_1^{(0)}), ..., (W_1^{(L_1-1)}, b_1^{(L_1-1)}), (W_2^{(0)}W_1^{(L_1)}, W_2^{(0)}b_1^{(L_1)} + b_2^{(0)}), (W_2^{(1)}, b_2^{(1)}), ..., (W_2^{(L_2)}, b_2^{(L_2)}) $$
    *   Essa concatenação essencialmente alimenta a saída de $\Phi_1$ diretamente para $\Phi_2$, com uma transformação linear intermediária [^2].

2.  **Concatenação com Identidade ($\Phi_2 \bullet \Phi_1$)**:
    *   A segunda forma, denotada por $\Phi_2 \bullet \Phi_1$, introduz uma camada de identidade intermediária [^2]. Essa concatenação é definida como $\Phi_2 \circ \Phi_{id} \circ \Phi_1$, onde $\Phi_{id}$ é uma rede neural de identidade [^2].
    *   Em termos de pesos e biases, $\Phi_2 \bullet \Phi_1$ é dado por [^2]:
        $$\left(\n        \begin{array}{ccccccccc}\n             (W_1^{(0)}, b_1^{(0)}), & ... , & (W_1^{(L_1-1)}, b_1^{(L_1-1)}), &\n             \begin{pmatrix}\n                  W_2^{(0)}W_{id}^{(L_1)} \\\\ -W_{2}^{(0)}W_{id}^{(L_1)}\n             \end{pmatrix}, &\n             \begin{pmatrix}\n                  W_2^{(0)}b_{id}^{(L_1)} + b_2^{(0)} \\\\ -W_{2}^{(0)}b_{id}^{(L_1)} - b_2^{(0)}\n             \end{pmatrix}, &\n             (W_2^{(1)}, b_2^{(1)}), & ... , & (W_2^{(L_2)}, b_2^{(L_2)})\n        \end{array}\n        \right)$$\n        onde $\Phi_{id}$ é a rede identidade.

O Lemma 5.2 [^2] sumariza as propriedades dessas construções, fornecendo limites para a largura, profundidade e tamanho das redes resultantes. Especificamente, se $d_{L_1+1}^1 = d_0^2$, então $\Phi_2 \circ \Phi_1(x) = \Phi_2(\Phi_1(x))$ para todo $x \in \mathbb{R}^{d_0^1}$ [^2]. Além disso, [^2]:

*   $width(\Phi_2 \circ \Phi_1) \leq max\\{width(\Phi_1), width(\Phi_2)\\}$
*   $depth(\Phi_2 \circ \Phi_1) = depth(\Phi_1) + depth(\Phi_2)$
*   $size(\Phi_2 \circ \Phi_1) < size(\Phi_1) + size(\Phi_2) + (d_{L_1+1}^1 + 1)d_0^2$

E também [^2]:

*   $width(\Phi_2 \bullet \Phi_1) \leq 2max\\{width(\Phi_1), width(\Phi_2)\\}$
*   $depth(\Phi_2 \bullet \Phi_1) = depth(\Phi_1) + depth(\Phi_2) + 1$
*   $size(\Phi_2 \bullet \Phi_1) \leq 2(size(\Phi_1) + size(\Phi_2))$

A prova do Lemma 5.2 [^2] demonstra como essas propriedades emergem diretamente da construção das redes concatenadas e da análise do número de parâmetros introduzidos em cada camada.

### Conclusão
A concatenação de redes neurais ReLU oferece duas abordagens distintas para construir modelos mais complexos. A concatenação direta (${\Phi_2 \circ \Phi_1}$) preserva a largura máxima das redes originais, enquanto a concatenação com identidade (${\Phi_2 \bullet \Phi_1}$) dobra a largura máxima, mas introduz uma camada adicional. Compreender essas nuances é crucial para projetar arquiteturas ReLU eficientes e controlar sua complexidade. Os limites estabelecidos no Lemma 5.2 [^2] fornecem ferramentas para prever o tamanho e a profundidade das redes resultantes, permitindo uma otimização informada do projeto da rede.

### Referências
[^1]: Chapter 5, ReLU neural networks.
[^2]: Section 5.1.2 Composition.
<!-- END -->