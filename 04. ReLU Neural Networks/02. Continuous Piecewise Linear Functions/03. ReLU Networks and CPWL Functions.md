## 5.1 Basic ReLU Calculus: Min/Max Operations

### Introdução
Este capítulo explora as capacidades fundamentais das redes neurais ReLU, especificamente como operações básicas como mínimo e máximo podem ser construídas usando um número limitado de funções de ativação ReLU [^43]. Como as redes ReLU são *piecewise linear*, as técnicas de aproximação usadas para funções de ativação mais suaves não se aplicam diretamente [^43]. No entanto, mesmo com essa simplicidade, as redes ReLU exibem notáveis capacidades de aproximação [^43]. Esta seção se concentra em como combinar e manipular redes neurais ReLU para construir funções contínuas *piecewise linear* mais complexas [^43].

### Conceitos Fundamentais
As operações de **mínimo** e **máximo** são blocos de construção essenciais para funções *continuous piecewise linear (CPWL)* mais complexas. O contexto fornece representações explícitas dessas operações usando a função de ativação ReLU [^50].

Para todos $x, y \in \mathbb{R}$, as seguintes identidades se mantêm:

*   Mínimo: $$ \min\{x, y\} = \text{ReLU}(x) - \text{ReLU}(-x) + \text{ReLU}(y) - \text{ReLU}(-y) - \text{ReLU}(x - y) $$
*   Máximo: $$ \max\{x, y\} = \text{ReLU}(x) - \text{ReLU}(-x) + \text{ReLU}(y) - \text{ReLU}(-y) + \text{ReLU}(x - y) $$

onde $\text{ReLU}(x) = \max\{0, x\}$ [^50]. Note que a representação original no texto contém um erro de digitação. As equações acima estão corrigidas.

**Lemma 5.10** Para cada $x, y \in \mathbb{R}$, é verdade que [^50]:

$$ \min\{x,y\} = \text{ReLU}(y) - \text{ReLU}(-y) - \text{ReLU}(y - x) \in \mathcal{N}(\text{ReLU}; 1,3) $$
$$ \max\{x,y\} = \text{ReLU}(y) - \text{ReLU}(-y) + \text{ReLU}(x - y) \in \mathcal{N}(\text{ReLU}; 1,3) $$

*Proof.* Dado que $\text{max}\{x,y\} = y + \text{ReLU}(x - y)$ [^50], e usando a identidade $y = \text{ReLU}(y) - \text{ReLU}(-y)$ [^44], a expressão para o máximo é derivada diretamente. Para o mínimo, observe que $\min\{x,y\} = - \max\{-x,-y\}$ [^50]. $\blacksquare$

As representações acima mostram que tanto a operação de mínimo quanto a de máximo podem ser implementadas com uma rede neural ReLU de uma única camada e três neurônios [^50].
A operação de mínimo de $n \geq 2$ entradas pode ser computada aplicando-se repetidamente a construção do **Lemma 5.10**. A rede neural resultante é descrita no próximo lema [^50].

**Lemma 5.11** Para todo $n \geq 2$ existe uma rede neural $\Phi_{n}^{min}: \mathbb{R}^n \rightarrow \mathbb{R}$ com [^51]:

$size(\Phi_{n}^{min}) < 16n$
$width(\Phi_{n}^{min}) \leq 3n$
$depth(\Phi_{n}^{min}) \leq \lceil \log_2(n) \rceil$

Tal que $\Phi_{n}^{min}(x_1, ..., x_n) = \min_{1 \leq j \leq n} x_j$. Similarmente, existe uma rede neural $\Phi_{n}^{max}: \mathbb{R}^n \rightarrow \mathbb{R}$ realizando o máximo e satisfazendo os mesmos limites de complexidade [^51].

### Conclusão
A capacidade de construir operações de mínimo e máximo com um número limitado de funções ReLU demonstra o poder expressivo das redes neurais ReLU, apesar de sua simplicidade [^50]. Esses blocos de construção permitem a representação de funções *continuous piecewise linear* mais complexas, conforme explorado nas seções subsequentes [^43]. A representação exata da identidade por redes ReLU, conforme demonstrado no Lema 5.1 [^44], é uma propriedade crucial que facilita a composição eficiente de redes [^44].

### Referências
[^43]: Page 43, Paragraphs 1-5
[^44]: Page 44, Paragraph 1-3
[^50]: Page 50, Lemma 5.10 and preceding paragraph
[^51]: Page 51, Lemma 5.11 and preceding paragraph
<!-- END -->