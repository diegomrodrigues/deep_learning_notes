## Capítulo 5.2: Funções Lineares Contínuas por Partes e Redes Neurais ReLU

### Introdução
Em continuidade ao estudo das funções lineares contínuas por partes (CPWL), este capítulo explora a relação intrínseca entre essas funções e as redes neurais ReLU [^48]. Como vimos anteriormente, a simplicidade e a capacidade de mitigar o problema dos gradientes evanescentes e explosivos tornam a função de ativação ReLU amplamente utilizada [^1]. Apesar de sua natureza piecewise linear, a ReLU permite a construção de classes ricas de funções com notáveis capacidades de aproximação [^1]. Este capítulo demonstra que redes neurais ReLU podem representar qualquer função CPWL, e vice-versa, estabelecendo uma equivalência fundamental entre essas duas classes de funções [^48].

### Conceitos Fundamentais
**Definição de Funções CPWL:**
Uma função $f: \Omega \rightarrow \mathbb{R}$, onde $\Omega \subseteq \mathbb{R}^d$, é dita *contínua, piecewise linear (CPWL)* se $f \in C^0(\Omega)$ e existem $n \in \mathbb{N}$ funções afins $g_j: \mathbb{R}^d \rightarrow \mathbb{R}$, $g_j(x) = w_j^T x + b_j$, tais que para cada $x \in \Omega$, existe pelo menos um $j \in \\{1, ..., n\\}$ para o qual $f(x) = g_j(x)$ [^48]. O número de regiões onde $f$ coincide com uma função $g_j$ é denotado por $q$, e o número de funções afins $n$ satisfaz $n \leq q$ [^48].

**Relação com Redes Neurais ReLU:**
A função $x \rightarrow \sigma_{ReLU}(w^T x + b)$, que representa uma rede neural ReLU com um único neurônio, é uma função CPWL com duas regiões [^48]. Consequentemente, qualquer rede neural ReLU é uma composição repetida de combinações lineares de funções CPWL [^48].

**Teorema da Representação:**
O teorema central deste capítulo estabelece a equivalência entre redes neurais ReLU e funções CPWL [^48].
**Teorema 5.7.** Seja $d \in \mathbb{N}$, seja $\Omega \subseteq \mathbb{R}^d$ convexo, e seja $f: \Omega \rightarrow \mathbb{R}$ uma função CPWL com $n \in \mathbb{N}$ como na Definição 5.5 [^48]. Então, existe uma rede neural ReLU $\Phi_f$ tal que $\Phi_f(x) = f(x)$ para todo $x \in \Omega$, e
$$size(\Phi_f) = O(dn2^n), \quad width(\Phi_f) = O(dn2^n), \quad depth(\Phi_f) = O(n).$$ [^48]

**Demonstração:**
A demonstração deste teorema se baseia na proposição de que qualquer função CPWL pode ser expressa como o máximo finito de um mínimo finito de funções afins [^48]. Mais precisamente:

**Proposição 5.8.** Seja $d \in \mathbb{N}$, $\Omega \subseteq \mathbb{R}^d$ convexo, e seja $f: \Omega \rightarrow \mathbb{R}$ uma função CPWL com $n \in \mathbb{N}$ funções afins como na Definição 5.5 [^49]. Então, existem $m \in \mathbb{N}$ e conjuntos $s_j \subseteq \\{1, ..., n\\}$ para $j \in \\{1, ..., m\\}$, tais que
$$f(x) = \max_{1 \leq j \leq m} \min_{i \in s_j} (g_i(x)) \quad \text{para todo} \\ x \in \Omega.$$ [^49]

A prova desta proposição é feita indutivamente, começando com o caso $d = 1$, onde $\Omega \subseteq \mathbb{R}$ é um intervalo (possivelmente não limitado) [^49]. O caso geral é então estendido para $d \in \mathbb{N}$ [^49].

Para completar a prova do Teorema 5.7, basta mostrar que as operações de mínimo e máximo podem ser expressas por redes neurais ReLU [^50].

**Lema 5.10.** Para todo $x, y \in \mathbb{R}$, valem as seguintes identidades:
$$\min\\{x, y\\} = \sigma_{ReLU}(y) - \sigma_{ReLU}(-y) - \sigma_{ReLU}(y - x) \in \mathcal{N}(\sigma_{ReLU}; 1, 3)$$\
$$\max\\{x, y\\} = \sigma_{ReLU}(y) - \sigma_{ReLU}(-y) + \sigma_{ReLU}(x - y) \in \mathcal{N}(\sigma_{ReLU}; 1, 3)$$ [^50]

Este lema demonstra que as operações de mínimo e máximo de dois inputs podem ser realizadas por redes neurais ReLU com um único layer e três neurônios [^50]. A operação de mínimo de $n \geq 2$ inputs pode ser computada repetidamente aplicando a construção do Lema 5.10 [^50]. A rede neural resultante é descrita no próximo lema.

**Lema 5.11.** Para todo $n \geq 2$, existe uma rede neural $\Phi_{min}: \mathbb{R}^n \rightarrow \mathbb{R}$ com
$$size(\Phi_{min}) < 16n, \quad width(\Phi_{min}) < 3n, \quad depth(\Phi_{min}) \leq [\log_2(n)]$$
tal que $\Phi_{min}(x_1, ..., x_n) = \min_{1 \leq j \leq n} x_j$ [^51]. Similarmente, existe uma rede neural $\Phi_{max}: \mathbb{R}^n \rightarrow \mathbb{R}$ realizando o máximo e satisfazendo os mesmos limites de complexidade [^51].

Com estes resultados, o Teorema 5.7 segue diretamente, mostrando que qualquer função CPWL pode ser representada por uma rede neural ReLU com as características especificadas [^52].

### Conclusão
Este capítulo estabeleceu uma conexão fundamental entre funções lineares contínuas por partes e redes neurais ReLU [^48]. Ao demonstrar que cada função CPWL pode ser representada por uma rede neural ReLU e vice-versa, este capítulo fornece uma base teórica sólida para entender as capacidades de aproximação das redes neurais ReLU [^48]. Os resultados apresentados aqui se baseiam nos conceitos e ferramentas desenvolvidos nos capítulos anteriores, como a definição da função de ativação ReLU e as operações básicas de combinação e manipulação de redes neurais [^1, ^43].

### Referências
[^1]: Página 1, ReLU neural networks
[^43]: Página 43, ReLU neural networks
[^48]: Página 48, Continuous piecewise linear functions
[^49]: Página 49, Continuous piecewise linear functions
[^50]: Página 50, Continuous piecewise linear functions
[^51]: Página 51, Continuous piecewise linear functions
[^52]: Página 52, Continuous piecewise linear functions
<!-- END -->