## 0-1 Loss Function in Binary Classification

### Introdução
Este capítulo explora a função de perda 0-1, uma escolha comum para problemas de classificação binária. Dada a sua simplicidade e interpretabilidade, é fundamental compreender as suas propriedades e limitações, especialmente no contexto de modelos mais complexos como redes neurais profundas. A função de perda é uma medida de discrepância entre o valor predito e o valor real, e sua escolha impacta diretamente o desempenho do modelo [^1, ^2].

### Conceitos Fundamentais
Em problemas de **classificação binária** com um conjunto discreto de cardinalidade dois como espaço de rótulos $Y$, a **função de perda 0-1** é uma escolha comum [^3]. Ela atribui uma perda de 1 se a previsão estiver incorreta e 0 se estiver correta. Matematicamente, a função de perda 0-1, denotada como $L_{0-1}(y, y')$, é definida como:

$$
L_{0-1}(y, y') =
\begin{cases}
1 & \text{se } y \neq y' \\
0 & \text{se } y = y'
\end{cases}
$$

onde $y$ é o rótulo verdadeiro e $y'$ é a previsão.

A função de perda 0-1 é intuitiva e fácil de entender, pois penaliza igualmente todos os erros de classificação. No entanto, ela apresenta algumas desvantagens significativas. Uma delas é que não é *diferenciável*, o que a torna inadequada para otimização direta usando métodos de gradiente descendente, que são amplamente utilizados no treinamento de redes neurais [^3]. Além disso, a função de perda 0-1 não fornece informações sobre a "confiança" da previsão. Por exemplo, uma previsão incorreta com uma probabilidade alta e uma previsão incorreta com uma probabilidade baixa são penalizadas da mesma forma.

Em contraste, a **binary cross-entropy loss**, também conhecida como log loss, é uma alternativa comum para problemas de classificação binária, especialmente quando se deseja prever probabilidades [^3]. Ela é definida como:

$$
L_{ce}(y, y') = -(y \log(y') + (1 - y) \log(1 - y'))
$$

onde $y$ é o rótulo verdadeiro (0 ou 1) e $y'$ é a probabilidade prevista de que o rótulo seja 1. A binary cross-entropy loss é *diferenciável* e penaliza previsões incorretas com alta confiança de forma mais severa do que previsões incorretas com baixa confiança [^3].

**Exemplo:** No contexto do exemplo de qualidade do café [^1], se o objetivo fosse classificar o café como "bom" ou "ruim" (uma classificação binária), a função de perda 0-1 poderia ser usada. No entanto, como a qualidade é avaliada em uma escala de 0 a 10, usar uma função de perda como a square loss poderia ser mais apropriado, pois penaliza erros maiores de forma mais severa [^3].

### Conclusão
A função de perda 0-1 é uma escolha natural e intuitiva para problemas de classificação binária, mas sua falta de diferenciabilidade e a ausência de informações sobre a confiança da previsão limitam sua aplicabilidade em cenários de aprendizado de máquina mais complexos. A binary cross-entropy loss oferece uma alternativa diferenciável que considera a probabilidade da previsão, tornando-se uma escolha mais adequada para o treinamento de redes neurais profundas [^3]. A seleção da função de perda ideal depende das características específicas do problema e dos requisitos de otimização do modelo.

### Referências
[^1]: Example 14.1 (Coffee Quality), p. 188
[^2]: Definition 14.2, p. 189
[^3]: Example 14.3 (Loss functions), p. 190
<!-- END -->