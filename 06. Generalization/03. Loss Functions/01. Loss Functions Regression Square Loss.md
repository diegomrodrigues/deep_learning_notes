## Funções de Perda Quadrática para Problemas de Regressão

### Introdução
Este capítulo explora em detalhes a escolha de funções de perda, focando na função de perda quadrática ($L_2$) para problemas de regressão. A escolha da função de perda é crucial para o sucesso do aprendizado de máquina, pois ela define o objetivo que o modelo deve otimizar. Como mencionado no Exemplo 14.3 [^1], a seleção de uma função de perda $L$ depende da aplicação em questão. Em continuidade ao contexto de *Learning Setup* apresentado na seção 14.1 [^2], onde se busca identificar uma conexão entre *features* $x$ e *labels* $y$ modelada por uma distribuição de probabilidade $D$ sobre $X \times Y$, a função de perda quantifica a discrepância entre a predição do modelo e o valor real, guiando o processo de otimização para encontrar o melhor preditor possível.

### Conceitos Fundamentais

Para problemas de regressão, onde o espaço de *labels* $Y$ é um subconjunto não discreto de um espaço Euclidiano, a função de perda quadrática ($L_2$) é uma escolha comum [^1]. Essa função é definida como:

$$L_2(y, y') = ||y - y'||^2$$

onde:
- $y$ representa o valor real (ground truth) do *label*.
- $y'$ representa a predição do modelo.
- $|| \cdot ||$ denota a norma Euclidiana (magnitude) do vetor diferença $y - y'$.

A norma Euclidiana ao quadrado calcula a soma dos quadrados das diferenças entre cada componente dos vetores $y$ e $y'$. Matematicamente, se $y = (y_1, y_2, ..., y_n)$ e $y' = (y'_1, y'_2, ..., y'_n)$, então:

$$L_2(y, y') = \sum_{i=1}^{n} (y_i - y'_i)^2$$

**Justificativa e Interpretação:**

A popularidade da função de perda quadrática em problemas de regressão se deve a várias razões:

1.  **Simplicidade e Facilidade de Cálculo:** A função é simples de calcular e sua derivada também é direta, facilitando a implementação de algoritmos de otimização baseados em gradiente.

2.  **Convexidade:** Para muitos modelos de regressão, a função de perda quadrática resulta em uma função de custo convexa. Funções convexas possuem um único mínimo global, o que garante que os algoritmos de otimização convergirão para a solução ótima.

3.  **Interpretação Estatística:** Sob a suposição de que os erros são independentes, identicamente distribuídos e seguem uma distribuição normal (Gaussiana) com média zero, a minimização da função de perda quadrática é equivalente à estimação de máxima verossimilhança (MLE).

**Exemplo:**

Considere um problema de regressão onde o objetivo é prever o preço de uma casa ($y$) com base em suas características (tamanho, localização, número de quartos, etc.). Suponha que o modelo preveja um preço de $y' = \\$280,000$ para uma casa cujo preço real é $y = \\$300,000$. A perda quadrática seria:

$$L_2(y, y') = (300,000 - 280,000)^2 = (20,000)^2 = 400,000,000$$

Essa perda quantifica o quão distante a predição está do valor real. O objetivo do treinamento do modelo é minimizar essa perda (e a perda média sobre todo o conjunto de treinamento) para obter predições mais precisas.

**Considerações Adicionais:**

*   **Sensibilidade a Outliers:** A função de perda quadrática é sensível a *outliers* (valores atípicos). Como a perda é proporcional ao quadrado da diferença, *outliers* podem ter um impacto desproporcionalmente grande na função de custo, influenciando a direção da otimização e prejudicando a performance do modelo em dados típicos.
*   **Alternativas:** Em situações onde *outliers* são comuns, outras funções de perda, como a perda absoluta (L1) ou a perda de Huber, podem ser mais robustas.

### Conclusão

A função de perda quadrática ($L_2$) é uma ferramenta fundamental em problemas de regressão, oferecendo simplicidade, convexidade e uma interpretação estatística clara. No entanto, é crucial estar ciente de sua sensibilidade a *outliers* e considerar alternativas mais robustas quando apropriado. A escolha da função de perda deve ser guiada pelas características específicas do problema e pelos objetivos do modelador. Em continuidade com a discussão sobre *Empirical Risk Minimization* (Seção 14.2 [^3]), a função de perda quadrática é frequentemente utilizada para calcular o risco empírico $R_S(h)$ [^3], que é então minimizado para encontrar o melhor modelo dentro de um conjunto de hipóteses $H$.

### Referências
[^1]: Exemplo 14.3 do texto fornecido.
[^2]: Seção 14.1 do texto fornecido.
[^3]: Seção 14.2 do texto fornecido.
<!-- END -->