## 14.4 Generalização via Números de Cobertura L∞

### Introdução
Este capítulo explora as propriedades de generalização de redes neurais profundas utilizando números de cobertura. Especificamente, focaremos em como a estimativa dos números de cobertura $L^\infty$ é crucial para entender o erro de generalização. Os números de cobertura fornecem uma medida de quão bem um conjunto finito de funções pode aproximar todo o espaço de funções, influenciando diretamente a capacidade de generalização do modelo. Conforme mencionado na introdução do Capítulo 14, o objetivo é discutir a generalização através do uso de números de cobertura.

### Conceitos Fundamentais

**Números de Cobertura**

Para derivar um limite de generalização para classes de redes neurais, começamos introduzindo a noção de números de cobertura.

**Definição 14.10** Seja *A* um subconjunto relativamente compacto de um espaço métrico (*X*, *d*). Para ε > 0, chamamos:

$$\
\mathcal{G}(A, \varepsilon, (X, d)) := \min \left\{m \in \mathbb{N} \mid \exists (x_i)_{i=1}^m \subset X \text{ s.t. } A \subseteq \bigcup_{i=1}^m B_\varepsilon(x_i) \right\},
$$

o número de ε-cobertura de *A* em *X*. Aqui, $B_\varepsilon(x) = \{z \in X \mid d(z, x) \leq \varepsilon\}$ é a bola de raio ε centrada em *x*. Em caso de o espaço *X* ou a métrica *d* serem claros pelo contexto, podemos escrever $\mathcal{G}(A, \varepsilon, d)$ ou $\mathcal{G}(A, \varepsilon, X)$ em vez de $\mathcal{G}(A, \varepsilon, (X, d))$. Uma visualização da Definição 14.10 é dada na Figura 14.2.

É possível limitar superiormente os números de ε-cobertura de redes neurais como um subconjunto de $L^\infty([0, 1]^d)$, assumindo que os pesos são confinados a um conjunto limitado fixo. As estimativas precisas são adiadas para a Seção 14.5. Antes disso, mostramos como um número de cobertura finito facilita um limite de generalização. Consideramos apenas espaços de características euclidianas *X* no seguinte resultado. Uma versão mais geral poderia ser facilmente derivada.

**Teorema 14.11** Sejam $C_Y, C_\mathcal{L} > 0$ e $\alpha > 0$. Seja $Y \subseteq [-C_Y, C_Y]$, $X \subseteq \mathbb{R}^d$ para algum $d \in \mathbb{N}$, e $H \subseteq \{h: X \rightarrow Y\}$. Além disso, seja $\mathcal{L}: Y \times Y \rightarrow \mathbb{R}$ ser $C_\mathcal{L}$-Lipschitz.

Então, para cada distribuição *D* em $X \times Y$ e cada $m \in \mathbb{N}$, vale com probabilidade de pelo menos $1 - \delta$ sobre a amostra $S \sim D^m$ que para todo $h \in H$:

$$\
|\mathcal{R}(h) - \mathcal{R}_S(h)| \leq 4 C_Y C_\mathcal{L} \sqrt{\frac{\log(\mathcal{G}(H, m^{-\alpha}, L^{\infty}(X))) + \log(2/\delta)}{m}} + \frac{2C_\mathcal{L}}{m^{\alpha}}
$$

*Prova*: Seja $M = \mathcal{G}(H, m^{-\alpha}, L^{\infty}(X))$. Seja $H_M = (h_i)_{i=1}^M \subseteq H$ tal que para cada $h \in H$ existe $h_i \in H_M$ com $||h - h_i||_{L^{\infty}(X)} \leq 1/m^{\alpha}$. A existência de $H_M$ segue pela Definição 14.10.

Fixe por enquanto tal $h \in H$ e $h_i \in H_M$. Pela desigualdade triangular reversa e normal, temos:

$$\
|\mathcal{R}(h) - \mathcal{R}_S(h)| - |\mathcal{R}(h_i) - \mathcal{R}_S(h_i)| \leq |\mathcal{R}(h) - \mathcal{R}(h_i)| + |\mathcal{R}_S(h) - \mathcal{R}_S(h_i)|
$$

Além disso, da monotonicidade do valor esperado e da propriedade de Lipschitz de $\mathcal{L}$, segue que:

$$\
|\mathcal{R}(h) - \mathcal{R}(h_i)| \leq \mathbb{E} |\mathcal{L}(h(x), y) - \mathcal{L}(h_i(x), y)| \leq C_\mathcal{L} \mathbb{E} |h(x) - h_i(x)| \leq \frac{C_\mathcal{L}}{m^{\alpha}}
$$

Uma estimativa similar produz $|\mathcal{R}_S(h) - \mathcal{R}_S(h_i)| \leq \frac{C_\mathcal{L}}{m^{\alpha}}$.

Assim, concluímos que para cada $\varepsilon > 0$:

$$\
\mathbb{P}_{S \sim D^m} \left[ \exists h \in H: |\mathcal{R}(h) - \mathcal{R}_S(h)| \geq \varepsilon \right] \leq \mathbb{P}_{S \sim D^m} \left[ \exists h_i \in H_M: |\mathcal{R}(h_i) - \mathcal{R}_S(h_i)| \geq \varepsilon - \frac{2C_\mathcal{L}}{m^{\alpha}} \right]
$$

Pela Proposição 14.9, sabemos que para $\varepsilon > 0$ e $\delta \in (0, 1)$:

$$\
\mathbb{P}_{S \sim D^m} \left[ \exists h_i \in H_M: |\mathcal{R}(h_i) - \mathcal{R}_S(h_i)| \geq \varepsilon - \frac{2C_\mathcal{L}}{m^{\alpha}} \right] < \delta
$$

contanto que

$$\
\varepsilon - \frac{2C_\mathcal{L}}{m^{\alpha}} > C_Y \sqrt{\frac{\log(M) + \log(2/\delta)}{2m}}
$$

onde *C* é tal que $\mathcal{L}(Y \times Y) \subseteq [c_1, c_2]$ com $c_2 - c_1 < C$. Pela propriedade de Lipschitz de $\mathcal{L}$, podemos escolher $C = 2\sqrt{2}C_\mathcal{L}C_Y$. Portanto, a definição de *M* em (14.4.1) juntamente com (14.4.2) e (14.4.3) dá que com probabilidade de pelo menos $1 - \delta$, vale para todo $h \in H$:

$$\
|\mathcal{R}(h) - \mathcal{R}_S(h)| \leq 2\sqrt{2}C_\mathcal{L}C_Y \sqrt{\frac{\log(\mathcal{G}(H, m^{-\alpha}, L^{\infty})) + \log(2/\delta)}{2m}} + \frac{2C_\mathcal{L}}{m^{\alpha}}
$$

$\blacksquare$

### Conclusão
A estimativa dos números de cobertura $L^\infty$ é crucial para entender o erro de generalização. O Teorema 14.11 demonstra como os números de cobertura podem ser usados para derivar limites de generalização. A próxima etapa é determinar como estimar esses números de cobertura para redes neurais, o que será abordado na Seção 14.5.

### Referências
[^1]: Page 1, Chapter 14
[^6]: Page 6, Chapter 14
[^7]: Page 7, Chapter 14
[^8]: Page 8, Chapter 14
<!-- END -->