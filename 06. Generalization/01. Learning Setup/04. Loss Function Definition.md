## 14.1.1 Loss Functions: Quantifying Prediction Error

### Introdução
No contexto de aprendizado de máquina, o objetivo é criar modelos que generalizem bem para dados não vistos. Para isso, é fundamental quantificar o quão "boa" é uma previsão feita pelo modelo. A **loss function** (função de perda) desempenha esse papel crucial, medindo a discrepância entre a previsão do modelo e o valor real [^1, ^2]. Este capítulo explora a definição, propriedades e exemplos de loss functions, estabelecendo a base para entender como otimizar modelos de aprendizado de máquina.

### Conceitos Fundamentais

A **loss function** $L: Y \times Y \rightarrow \mathbb{R}^+$ é uma função mensurável que quantifica a discrepância entre o rótulo previsto $h(x)$ e o rótulo verdadeiro $y$ [^2]. Formalmente, ela atribui um valor real não negativo a cada par de rótulos previstos e verdadeiros, representando o custo do erro de previsão.

**Definição Formal:** Uma loss function $L$ mapeia um par de valores, o valor predito e o valor real, para um número real não negativo.
$$L(h(x), y) \in \mathbb{R}^+$$
onde:
*   $h(x)$ é a previsão do modelo para a entrada $x$.
*   $y$ é o rótulo verdadeiro correspondente à entrada $x$.

**Exemplos de Loss Functions:**

1.  **Square Loss (Erro Quadrático)**: Utilizada em problemas de regressão, onde o objetivo é prever um valor contínuo. Ela calcula o quadrado da diferença entre o valor previsto e o valor real [^3].
    $$L_2(y, y') = ||y - y'||^2$$
2.  **0-1 Loss:** Comum em problemas de classificação binária, onde o objetivo é classificar uma entrada em uma de duas classes. Ela atribui um custo de 1 se a previsão estiver incorreta e 0 se estiver correta [^3].
    $$L_{0-1}(y, y') = \begin{cases} 1, & \text{se } y \neq y' \\ 0, & \text{se } y = y' \end{cases}$$
3.  **Binary Cross-Entropy Loss:** Também utilizada em classificação binária, especialmente quando se deseja prever probabilidades. Ela mede a diferença entre a distribuição de probabilidade prevista e a distribuição real [^3].
    $$L_{ce}(y, y') = -(y \log(y') + (1 - y) \log(1 - y'))$$

**Risco (Risk):** Dado uma loss function $L$ e uma distribuição de probabilidade $D$ sobre o espaço de entrada e saída $X \times Y$, o risco de uma função mensurável $h: X \rightarrow Y$ é definido como o valor esperado da loss function [^2]:
$$R(h) = \mathbb{E}_{(x,y) \sim D}[L(h(x), y)]$$
O risco representa o erro médio que o modelo $h$ comete ao prever os rótulos com base na distribuição $D$.

**Bayes Risk:** O objetivo do aprendizado é encontrar uma função $h$ que minimize o risco. O menor risco possível é chamado de Bayes risk [^2]:
$$R^* = \inf_{h: X \rightarrow Y} R(h)$$
O Bayes risk representa o limite inferior do erro que qualquer modelo pode alcançar, dada a distribuição de dados.

**Loss functions no exemplo da Qualidade do Café:**

No exemplo da qualidade do café [^1], onde o objetivo é prever a qualidade do café com base em seis features (acidez, teor de cafeína, preço, aftertaste, nível de torra e origem), a escolha da loss function é crucial. A qualidade é dada como uma fração $k/10$, onde $k = 0, ..., 10$.  Neste caso, a 0-1 loss não seria ideal, pois penalizaria igualmente previsões próximas e distantes do valor real. Uma square loss seria mais apropriada, penalizando mais fortemente previsões mais erradas [^3].

### Conclusão

A loss function é um componente essencial no aprendizado de máquina, fornecendo uma medida quantitativa do erro de previsão. A escolha da loss function depende do tipo de problema (regressão ou classificação) e das características dos dados. Minimizar o risco, ou seja, o valor esperado da loss function, é o objetivo central do processo de treinamento do modelo. O conceito de Bayes risk define o limite inferior do erro que pode ser alcançado. Compreender e selecionar a loss function apropriada é crucial para o sucesso de qualquer projeto de aprendizado de máquina.

### Referências
[^1]: Capítulo 14, Seção 14.1
[^2]: Capítulo 14, Definição 14.2
[^3]: Capítulo 14, Exemplo 14.3
<!-- END -->