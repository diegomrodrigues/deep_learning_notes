## O Fenômeno da Explosão dos Pesos em Redes Neurais

### Introdução
Como discutido anteriormente, a não convexidade do espaço de redes neurais tem sérias implicações para a aproximação e o aprendizado [^1]. Este capítulo explora uma dessas consequências, o fenômeno da explosão dos pesos, que surge devido à não existência de melhores aproximações [^11]. Em outras palavras, se buscamos aprender uma função $f$ usando redes neurais com uma arquitetura fixa $N(A; \sigma, \infty)$ e a função de perda $L$ é o erro quadrático, então, sob certas condições, os pesos da rede neural devem divergir [^11]. Isso pode ser indesejável para a generalização e para a obtenção de limites no espaço de parâmetros [^11].

### Conceitos Fundamentais
O fenômeno da explosão dos pesos está intrinsecamente ligado à inexistência de uma melhor aproximação para uma função $f$ dentro do conjunto de redes neurais $N(A; \sigma, \infty)$ [^11]. Para formalizar essa ideia, consideremos um problema de regressão onde o objetivo é aprender uma função $f$ utilizando redes neurais com uma arquitetura fixa $N(A; \sigma, \infty)$ [^11]. Suponha que se busca produzir uma sequência de redes neurais $(\Phi_n)_{n=1}^\infty$ tal que o risco, definido em (1.2.4) (não fornecido no contexto, mas presumivelmente definido em um capítulo anterior), convirja para 0 [^11]. Se a função de perda $L$ é o erro quadrático e $\mu$ é uma medida de probabilidade em $[-1,1]^{d_0}$, e os dados são dados por $(x, f(x))$ para $x \sim \mu$, então [^11]:

$$R(\Phi_n) = ||\Phi_n - f||_{L^2([-1,1]^{d_0}, \mu)}^2 = \int_{[-1,1]^{d_0}} |\Phi_n(x) - f(x)|^2 d\mu(x) \rightarrow 0 \quad \text{para } n \rightarrow \infty$$ [^11]

No entanto, de acordo com a Proposição 13.12 [^11], para uma dada arquitetura $A$ e função de ativação $\sigma$, é possível que a condição acima (convergência do risco para zero) seja satisfeita, mas $f \notin N(\sigma; A, \infty)$. O resultado a seguir demonstra que, nessa situação, os pesos de $\Phi_n$ devem divergir [^11].

**Proposição 13.14:** Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua com $C_0 \geq 1$, e $|\sigma(x)| \leq C_0|x|$ para todo $x \in \mathbb{R}$, e seja $\mu$ uma medida em $[-1,1]^{d_0}$ [^11]. Assuma que existe uma sequência $\Phi_n \in N(\sigma; A, \infty)$ e $f \in L^2([-1,1]^{d_0}, \mu) \setminus N(\sigma; A, \infty)$ tal que [^11]:

$$||\Phi_n - f||_{L^2([-1,1]^{d_0}, \mu)} \rightarrow 0$$ [^11]

Então,
$$\limsup_{n \rightarrow \infty} \max \{||W_n^{(l)}||_\infty, ||b_n^{(l)}||_\infty | l = 0, ..., L\} = \infty$$ [^11]

*Prova:* Assuma, por contradição, que o lado esquerdo de (13.3.7) é finito [^11]. Como resultado, existe $C > 0$ tal que $\Phi_n \in N(\sigma; A, C)$ para todo $n \in \mathbb{N}$ [^11]. Pela Proposição 13.1 (não fornecida no contexto, mas referenciada), conclui-se que $N(\sigma; A, C)$ é a imagem de um conjunto compacto sob uma aplicação contínua e, portanto, é um conjunto compacto em $L^2([-1,1]^{d_0}, \mu)$ [^11]. Em particular, temos que $N(\sigma; A, C)$ é fechado [^11]. Assim, (13.3.6) implica $f \in N(\sigma; A, C)$, o que gera uma contradição [^11]. $\blacksquare$

A Proposição 13.14 pode ser estendida para todas as funções $f$ para as quais não existe uma melhor aproximação em $N(\sigma; A, \infty)$, como pode ser visto no Exercício 13.18 [^11]. Esses resultados implicam que, para funções que desejamos aprender e que não possuem uma melhor aproximação dentro de um conjunto de redes neurais, devemos esperar que os pesos das redes neurais que as aproximam cresçam para o infinito [^11].

### Conclusão
O fenômeno da explosão dos pesos é uma consequência direta da não existência de melhores aproximações em espaços de redes neurais [^11]. A Proposição 13.14 formaliza essa ideia, demonstrando que, quando uma função $f$ não pode ser representada por uma rede neural com uma arquitetura fixa, mas ainda assim buscamos aproximá-la, os pesos da rede neural devem divergir [^11]. Este comportamento pode ser indesejável, pois um espaço de parâmetros limitado facilita muitos limites de generalização, como será visto nas seções seguintes [^11]. Em resumo, este capítulo demonstra que a falta de melhores aproximações pode levar a comportamentos inesperados e potencialmente prejudiciais no treinamento de redes neurais [^11].

### Referências
[^1]: Capítulo 13. Shape of neural network spaces.
[^11]: Seção 13.3.3 Exploding weights phenomenon.
<!-- END -->