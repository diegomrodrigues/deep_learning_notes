## Propriedades de Aproximação e Fechamento em Espaços de Redes Neurais

### Introdução
Em continuidade ao capítulo anterior, que abordou a complexidade do *loss landscape* das redes neurais e sua natureza tipicamente não-convexa, este capítulo explora propriedades de aproximação e fechamento dos espaços de redes neurais [^1]. A análise da capacidade de aproximação e das características de fechamento é crucial para entender o comportamento das redes neurais durante o treinamento e sua capacidade de generalização. Investigaremos como a não-convexidade dos espaços de redes neurais impacta a existência de melhores aproximações e a continuidade das seleções, e como a falta dessas propriedades pode levar a comportamentos indesejáveis, como o aumento descontrolado dos pesos da rede.

### Conceitos Fundamentais
Um espaço de rede neural $N(\sigma; A, \infty)$ pode exibir propriedades como a **melhor aproximação**, a **melhor aproximação única** ou a **seleção contínua**. Essas propriedades são determinantes para avaliar quão bem as funções podem ser aproximadas por redes neurais dentro desse espaço.

*   **Propriedade da Melhor Aproximação:** Garante que, para qualquer função $h$ em um espaço de função normado $H$, existe uma rede neural $\Phi$ em $N(\sigma; A, \infty)$ que minimiza a distância até $h$ [^1]. Formalmente:

$$
\exists \Phi \in N(\sigma; A, \infty) \text{ tal que } ||\Phi - h||_H = \inf_{\Phi^* \in N(\sigma; A, \infty)} ||\Phi^* - h||_H
$$

*   **Propriedade da Melhor Aproximação Única:** Reforça a propriedade anterior, exigindo que exista exatamente uma rede neural $\Phi$ que minimize a distância até $h$ [^1].

*   **Propriedade da Seleção Contínua:** Requer a existência de uma função contínua $\varphi$ que mapeia funções $h$ de $H$ para redes neurais $\Phi$ em $N(\sigma; A, \infty)$, de forma que $\Phi = \varphi(h)$ satisfaça a condição de melhor aproximação para todo $h$ em $H$ [^1]. A existência de uma seleção contínua implica a existência de um algoritmo de seleção estável, onde problemas similares levam a redes neurais similares que satisfazem a condição de melhor aproximação [^8].

#### Convexidade dos Espaços de Redes Neurais
O capítulo anterior já indicava que os *loss landscapes* das redes neurais são tipicamente não-convexos [^1]. Aqui, exploramos a convexidade dos espaços de redes neurais em si. Um conjunto $Z$ é chamado de **star-shaped** se existe um ponto $x \in Z$ tal que para todo $y \in Z$, o segmento de reta $\{tx + (1-t)y | t \in [0,1]\}$ está contido em $Z$ [^4]. Um ponto $x$ é chamado de **centro** de $Z$ se essa condição é satisfeita para todo $y \in Z$ [^4].

**Proposição 13.5:** Para $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$, o espaço $N(\sigma; A, \infty)$ é *scaling invariant*, ou seja, para todo $\lambda \in \mathbb{R}$, $\lambda f \in N(\sigma; A, \infty)$ se $f \in N(\sigma; A, \infty)$, e portanto $0 \in N(\sigma; A, \infty)$ é um centro de $N(\sigma; A, \infty)$ [^4].

Essa proposição implica que o espaço de redes neurais é star-shaped com centro em 0, mas a questão é se ele possui outros centros. O próximo teorema estabelece um limite superior para o número de centros linearmente independentes.

**Teorema 13.6:** Para $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua, $N(\sigma; A, \infty)$ contém no máximo $n_A = \sum_{l=0}^{L} (d_l + 1)d_{l+1}$ centros linearmente independentes [^4].

**Corolário 13.7:** Se $N(\sigma; A, \infty)$ contém mais do que $n_A = \sum_{l=0}^{L} (d_l + 1)d_{l+1}$ funções linearmente independentes, então $N(\sigma; A, \infty)$ não é convexo [^5].

Esse corolário sugere que não podemos esperar conjuntos convexos de redes neurais, especialmente se o conjunto contém muitos elementos linearmente independentes [^5].

#### ɛ-Convexidade
Para quantificar a não-convexidade dos espaços de redes neurais, introduzimos a noção de ɛ-convexidade. Um subconjunto $A$ de um espaço vetorial normado $X$ é dito ser ɛ-convexo se o *convex hull* de $A$, denotado por $co(A)$, está contido em $A + B_{\epsilon}(0)$, onde $B_{\epsilon}(0)$ é uma bola de raio ɛ centrada em 0 [^6]. Intuitivamente, um conjunto é ɛ-convexo se ele preenche todos os "buracos" menores que ɛ.

O seguinte teorema demonstra que, sob certas condições, não existe um ɛ > 0 tal que $N(\sigma; A, \infty)$ seja ɛ-convexo.

**Teorema 13.9:** Sejam $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$. Seja $K \subseteq \mathbb{R}^{d_0}$ compacto e $\sigma \in M$, com $M$ como em (3.1.1), e assuma que $\sigma$ não é um polinômio. Além disso, suponha que existe um conjunto aberto onde $\sigma$ é diferenciável e não constante. Se existe um ɛ > 0 tal que $N(\sigma; A, \infty)$ é ɛ-convexo, então $N(\sigma; A, \infty)$ é denso em $C(K)$ [^7].

Esse teorema implica que conjuntos de redes neurais com arquiteturas fixas possuem buracos arbitrariamente grandes [^8].

#### Seleção Contínua
A seleção contínua é uma propriedade desejável, pois implica a existência de algoritmos de seleção estáveis. No entanto, como demonstrado no Teorema 13.10, subconjuntos de $L^p$ com a propriedade de melhor aproximação única são convexos [^8]. Isso leva à seguinte proposição:

**Proposição 13.11:** Sejam $L \in \mathbb{N}$, $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua e não um polinômio, e $p \in (1, \infty)$. Então, $N(\sigma; A, \infty) \subseteq L^p([-1,1]^{d_0})$ não possui a propriedade de seleção contínua [^9].

Essa proposição implica que, sob condições bastante gerais, os espaços de redes neurais não admitem a propriedade de seleção contínua [^9].

#### Existência de Melhores Aproximações
Além da seleção contínua, a própria existência de melhores aproximações nem sempre é garantida. O seguinte resultado, uma versão simplificada de [171, Theorem 3.1], demonstra que, em muitos casos, a propriedade de melhor aproximação falha.

**Proposição 13.12:** Seja $A = (1, 2, 1)$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua. Assuma que existem $r > 0$ e $\alpha' \neq \alpha$ tais que $\sigma$ é diferenciável para todo $|x| > r$ e $\sigma'(x) \rightarrow \alpha$ para $x \rightarrow \infty$, $\sigma'(x) \rightarrow \alpha'$ para $x \rightarrow -\infty$. Então, existe uma sequência em $N(\sigma; A, \infty)$ que converge em $L^p([-1,1]^{d_0})$ para todo $p \in (1, \infty)$, e o limite dessa sequência é descontínuo. Em particular, o limite da sequência não está em $N(\sigma; A', \infty)$ para nenhum $A'$ [^10].

Essa proposição implica que a sequência pode convergir para uma função que não pode ser representada por uma rede neural com a arquitetura dada [^10].

#### Fenômeno da Explosão dos Pesos
A não-existência de melhores aproximações pode levar a um fenômeno conhecido como "explosão dos pesos". Considere um problema de regressão onde o objetivo é aprender uma função $f$ usando redes neurais com uma arquitetura fixa $N(A; \sigma, \infty)$ [^11]. Se a sequência de redes neurais $(\Phi_n)_{n=1}^{\infty}$ converge para $f$ em $L^2$, mas $f$ não pertence a $N(\sigma; A, \infty)$, então os pesos de $\Phi_n$ divergem [^11].

**Proposição 13.14:** Sejam $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua com $C_\sigma \geq 1$, e $|\sigma(x)| \leq C_\sigma |x|$ para todo $x \in \mathbb{R}$, e seja $\mu$ uma medida em $[-1,1]^{d_0}$. Assuma que existe uma sequência $\Phi_n \in N(\sigma; A, \infty)$ e $f \in L^2([-1,1]^{d_0}, \mu) \setminus N(\sigma; A, \infty)$ tal que $||\Phi_n - f||_{L^2([-1,1]^{d_0}, \mu)} \rightarrow 0$ [^11]. Então, $\limsup_{n \rightarrow \infty} \max\{||W_n^{(l)}||_\infty, ||b_n^{(l)}||_\infty | l = 0, ..., L\} = \infty$ [^11].

Essa proposição implica que, para funções que não possuem uma melhor aproximação dentro de um conjunto de redes neurais, devemos esperar que os pesos das redes neurais aproximantes cresçam para o infinito [^12].

### Conclusão
Este capítulo explorou as propriedades de aproximação e fechamento dos espaços de redes neurais, demonstrando que, sob condições bastante gerais, esses espaços não são convexos, não possuem a propriedade de seleção contínua e nem sempre garantem a existência de melhores aproximações [^1, 5, 9, 10]. A falta dessas propriedades pode levar a comportamentos indesejáveis durante o treinamento, como a explosão dos pesos [^12]. Esses resultados têm implicações significativas para a compreensão da capacidade de generalização das redes neurais e a necessidade de técnicas de regularização para controlar o comportamento dos pesos durante o treinamento.

### Referências
[^1]: Introdução do capítulo.
[^2]: Definição da propriedade da melhor aproximação.
[^3]: Definição da propriedade da melhor aproximação única.
[^4]: Teorema 13.5, Teorema 13.6, Definição 13.4.
[^5]: Corolário 13.7.
[^6]: Definição 13.8.
[^7]: Teorema 13.9.
[^8]: Seção 13.3.1
[^9]: Proposição 13.11.
[^10]: Proposição 13.12.
[^11]: Seção 13.3.3 e Proposição 13.14.
[^12]: Conclusão do capítulo.
<!-- END -->