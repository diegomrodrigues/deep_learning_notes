## Capítulo 13: Forma dos Espaços de Redes Neurais

### Introdução
Como mencionado anteriormente, a paisagem de perda das redes neurais pode ser intrincada e tipicamente não convexa. Este capítulo explora a forma dos espaços de redes neurais, considerando-os como subconjuntos de outros espaços de funções. Investigaremos a convexidade e propriedades relacionadas, como a presença de centros e a invariância de escala, que influenciam a capacidade de aproximação e otimização das redes neurais [^1].

### Conceitos Fundamentais

#### Espaços Estrela e Centros
Um conceito fundamental para entender a geometria dos espaços de redes neurais é a noção de um **conjunto estrela** (star-shaped set). Um conjunto $Z$ em um espaço linear é dito *estrela* se existe um ponto $x \in Z$, chamado **centro** de $Z$, tal que para todo $y \in Z$, o segmento de reta que conecta $x$ a $y$ está contido em $Z$ [^4]. Formalmente,

$$
\{tx + (1-t)y \mid t \in [0,1]\} \subseteq Z.
$$

#### Invariância de Escala e o Centro Zero
Uma propriedade importante dos espaços de redes neurais $N(\sigma; A, \infty)$ é a **invariância de escala** [^4]. Isso significa que se uma função $f$ pertence ao espaço da rede neural, então qualquer versão escalada dessa função ($\lambda f$) também pertence ao mesmo espaço. Matematicamente, para todo $\lambda \in \mathbb{R}$, se $f \in N(\sigma; A,infty)$, então $\lambda f \in N(\sigma; A, \infty)$.

Essa propriedade tem implicações significativas. Em particular, implica que a **função zero** (0) é um centro do espaço da rede neural $N(\sigma; A, \infty)$ [^4]. Isso decorre diretamente da definição de invariância de escala, pois $\lambda \cdot f = 0$ quando $\lambda = 0$, independentemente de qual seja a função $f$. Além da função zero, toda função constante também é um centro do espaço [^4].

**Proposição 13.5:** Seja $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. Então, $N(\sigma; A, \infty)$ é invariante por escala, i.e., para todo $\lambda \in \mathbb{R}$, vale que $\lambda f \in N(\sigma; A, \infty)$ se $f \in N(\sigma; A, \infty)$, e portanto $0 \in N(\sigma; A, \infty)$ é um centro de $N(\sigma; A, \infty)$ [^4].

#### Não Convexidade e Implicações
Embora os espaços de redes neurais sejam *estrela*, eles tipicamente **não são convexos** [^1]. A convexidade implica que para quaisquer dois pontos em um conjunto, o segmento de reta que os conecta também está contido no conjunto. A não convexidade dos espaços de redes neurais pode levar a dificuldades na otimização, como a presença de mínimos locais espúrios [^1].

**Corolário 13.7:** Seja $A = (d_0, d_1, ..., d_{L+1})$ e seja $\sigma : \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua. Se $N(\sigma; A, \infty)$ contém mais do que $n_A = \sum_{l=0}^{L} (d_l + 1)d_{l+1}$ funções linearmente independentes, então $N(\sigma; A, \infty)$ não é convexo [^5].

Essa não convexidade severa implica que a otimização direta sobre o espaço das funções da rede neural, em vez do espaço de parâmetros, ainda pode ser desafiadora [^1].

#### Lipschitz Parameterizations
Para entender melhor a geometria dos espaços de redes neurais, é útil analisar a **realização mapeada** $R_\theta$, que mapeia os parâmetros da rede neural para a função que ela representa [^1]. Se a função de ativação $\sigma$ é Lipschitz contínua, então o conjunto de redes neurais é a imagem de $P_N(A,\infty)$ sob um mapa localmente Lipschitz [^1].

**Proposição 13.1:** Sejam $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ ser $C_0$-Lipschitz contínua com $C_0 \geq 1$, seja $|\sigma(x)| \leq C_0|x|$ para todo $x \in \mathbb{R}$, e seja $B > 1$. Então, para todo $\theta, \theta' \in P_N(A, B)$,

$$
||R_\theta(\theta) - R_\theta(\theta')||_{L^\infty([-1,1]^{d_0})} \leq (2C_0Bd_{max})^Ln_A||\theta - \theta'||_\infty,
$$

onde $d_{max} = \max_{l=0, ..., L+1} d_l$ e $n_A = \sum_{l=0}^{L} d_{l+1}(d_l + 1)$ [^2].

#### Convexidade e $\epsilon$-convexidade
Embora os espaços de redes neurais não sejam convexos, podemos considerar uma noção mais fraca de convexidade, chamada **$\epsilon$-convexidade**. Um conjunto $A$ em um espaço vetorial normado $X$ é dito $\epsilon$-convexo se o fecho convexo de $A$ está contido em $A + B_\epsilon(0)$, onde $B_\epsilon(0)$ é uma bola de raio $\epsilon$ centrada na origem [^6].

**Definição 13.8:** Para $\epsilon > 0$, dizemos que um subconjunto $A$ de um espaço vetorial normado $X$ é $\epsilon$-convexo se $co(A) \subseteq A + B_\epsilon(0)$, onde $co(A)$ denota o fecho convexo de $A$ e $B_\epsilon(0)$ é uma bola $\epsilon$ ao redor de 0 com respeito à norma de $X$ [^6].

Intuitivamente, um conjunto é $\epsilon$-convexo se ele se torna convexo ao preencher todos os buracos menores que $\epsilon$. No entanto, mesmo essa condição mais fraca geralmente não é satisfeita pelos espaços de redes neurais [^6].

**Teorema 13.9:** Sejam $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$. Seja $K \subseteq \mathbb{R}^{d_0}$ compacto e seja $\sigma \in M$, com $M$ como em (3.1.1), e assuma que $\sigma$ não é um polinômio. Além disso, assuma que existe um conjunto aberto onde $\sigma$ é diferenciável e não constante. Se existe um $\epsilon > 0$ tal que $N(\sigma; A, \infty)$ é $\epsilon$-convexo, então $N(\sigma; A, \infty)$ é denso em $C(K)$ [^7].

#### Propriedades de Aproximação
A não convexidade dos espaços de redes neurais tem implicações importantes para a capacidade de aproximação de funções. Em particular, pode não existir uma **melhor aproximação** para uma dada função em um espaço de rede neural [^8]. Isso significa que não existe uma rede neural dentro do espaço que minimize a distância até a função alvo.

Além disso, a **propriedade de seleção contínua** (continuous selection property) geralmente não é satisfeita [^8]. Isso significa que não existe uma função contínua que mapeie uma função alvo para a melhor aproximação correspondente no espaço da rede neural.

**Proposição 13.11:** Sejam $L \in \mathbb{N}$, $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua e não um polinômio, e seja $p \in (1, \infty)$. Então, $N(\sigma; A, \infty) \subseteq L^p([-1,1]^{d_0})$ não tem a propriedade de seleção contínua [^9].

A ausência da propriedade de melhor aproximação pode levar a problemas durante o treinamento de redes neurais. Em particular, os pesos da rede neural podem tender ao infinito para aproximar funções que não pertencem ao espaço da rede neural [^8].

**Proposição 13.14:** Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua com $C_0 \geq 1$, e $|\sigma(x)| \leq C_0|x|$ para todo $x \in \mathbb{R}$, e seja $\mu$ uma medida em $[-1,1]^{d_0}$. Assuma que existe uma sequência $\Phi_n \in N(\sigma; A, \infty)$ e $f \in L^2([-1,1]^{d_0}, \mu) \setminus N(\sigma; A, \infty)$ tal que $||\Phi_n - f||_{L^2([-1,1]^{d_0}, \mu)} \rightarrow 0$. Então, $\limsup_{n \rightarrow \infty} \max\{||W_n^{(l)}||_\infty, ||b_n^{(l)}||_\infty \mid l = 0, ..., L\} = \infty$ [^11].

### Conclusão
Este capítulo explorou a forma dos espaços de redes neurais, destacando sua não convexidade e as implicações dessas propriedades para a aproximação de funções e otimização. A invariância de escala garante que a função zero seja um centro do espaço, mas a não convexidade e a falta de melhores aproximações podem levar a dificuldades no treinamento e na generalização [^1]. A análise da realização mapeada e das propriedades de Lipschitz fornece insights adicionais sobre a geometria desses espaços [^1].

### Referências
[^1]: Capítulo 13, "Shape of neural network spaces", página 175.
[^2]: Proposition 13.1, página 175.
[^3]: Lemma 13.2, página 176.
[^4]: Definition 13.4, Proposition 13.5, página 178.
[^5]: Corollary 13.7, página 179.
[^6]: Definition 13.8, página 180.
[^7]: Theorem 13.9, página 181.
[^8]: Seção 13.3, página 182.
[^9]: Proposition 13.11, página 183.
[^10]: Proposition 13.12, página 184.
[^11]: Proposition 13.14, página 185.
<!-- END -->