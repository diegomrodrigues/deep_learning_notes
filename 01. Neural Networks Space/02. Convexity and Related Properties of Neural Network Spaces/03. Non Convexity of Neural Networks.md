## Não Convexidade e Buracos Arbitrariamente Grandes em Espaços de Redes Neurais

### Introdução
Este capítulo aprofunda a análise da geometria dos espaços de redes neurais, focando especificamente na sua não convexidade e na existência de "buracos" de tamanho arbitrário. Como vimos anteriormente [^1], a superfície de perda de redes neurais é intrincada e tipicamente não convexa. Aqui, exploraremos como essa não convexidade se manifesta na estrutura do espaço de funções representadas por redes neurais de arquitetura fixa, e como isso afeta a capacidade de aproximação e otimização. Em particular, analisaremos o conceito de $\epsilon$-convexidade e demonstraremos que, sob certas condições, conjuntos de redes neurais de arquiteturas fixas podem ter buracos arbitrariamente grandes.

### Conceitos Fundamentais

**Não Convexidade e Centros:** O conjunto de redes neurais $N(\sigma; A, \infty)$ geralmente não é convexo [^1]. No entanto, ele é *star-shaped*, com a função nula (0) e funções constantes sendo centros [^2]. Um conjunto $Z$ é *star-shaped* se existe um ponto $x \in Z$ (chamado centro) tal que para todo $y \in Z$, o segmento de reta que conecta $x$ e $y$ está contido em $Z$ [^2]. Formalmente, $\\{tx + (1-t)y | t \in [0,1]\\} \subseteq Z$. A não convexidade implica que algoritmos de otimização podem falhar ao encontrar o mínimo global [^1].

**$\epsilon$-Convexidade:** Para quantificar o quão "perto" um conjunto está de ser convexo, introduzimos a noção de $\epsilon$-convexidade [^1]. Um conjunto $A$ em um espaço vetorial normado $X$ é $\epsilon$-convexo se seu *convex hull* (co(A)) está contido em uma $\epsilon$-neighborhood de $A$. Formalmente,
$$co(A) \subseteq A + B_{\epsilon}(0),$$
onde $B_{\epsilon}(0)$ é uma bola de raio $\epsilon$ centrada na origem [^6]. Intuitivamente, um conjunto é $\epsilon$-convexo se, ao preencher todos os buracos menores que $\epsilon$, ele se torna convexo [^6].

**Teorema da Densidade e Buracos Arbitrariamente Grandes:** O teorema central deste capítulo demonstra que, sob certas condições, o conjunto de redes neurais de arquitetura fixa não é $\epsilon$-convexo para nenhum $\epsilon > 0$ [^1, 7]. Além disso, se $N(\sigma; A, \infty)$ fosse $\epsilon$-convexo, então $N(\sigma; A, \infty)$ seria denso em $C(K)$, onde $K$ é um conjunto compacto e $\sigma$ satisfaz certas condições [^1, 7]. Isso implica que conjuntos de redes neurais de arquiteturas fixas têm buracos arbitrariamente grandes [^1].

**Teorema 13.9 [^7]:** Seja $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$. Seja $K \subseteq \mathbb{R}^{d_0}$ compacto e seja $\sigma \in M$, com $M$ como em (3.1.1) e assuma que $\sigma$ não é um polinômio. Assuma também que existe um conjunto aberto onde $\sigma$ é diferenciável e não constante. Se existe um $\epsilon > 0$ tal que $N(\sigma; A, \infty)$ é $\epsilon$-convexo, então $N(\sigma; A, \infty)$ é denso em $C(K)$.

*Prova:* A prova deste teorema envolve três passos principais:

1.  **$\epsilon$-convexidade implica convexidade:** Demonstra-se que se $N(\sigma; A, \infty)$ é $\epsilon$-convexo para algum $\epsilon > 0$, então ele é convexo [^7]. Isso é feito utilizando a invariância de escala de $N(\sigma; A, \infty)$ (Proposição 13.5 [^4]) e mostrando que o convex hull também é invariante de escala.

2.  **$N_1(\sigma; 1) \subseteq \overline{N(\sigma; A, \infty)}$:** Mostra-se que o conjunto de redes neurais rasas (shallow) com largura arbitrária, $N_1(\sigma; 1)$, está contido no fecho de $N(\sigma; A, \infty)$. Isso é feito utilizando a convexidade de $N(\sigma; A, \infty)$ e a Proposição 3.16, que garante a existência de funções em $N(\sigma; A, \infty)$ que se comportam como $\sigma(w^T x + b)$ para certos $w$ e $b$ [^7].

3.  **Densidade:** Conclui-se que $N(\sigma; A, \infty)$ é denso em $C(K)$ [^7]. Isso é feito utilizando o Teorema 3.8, que afirma que $N_1(\sigma; 1)$ é denso em $C(K)$, e o fato de que $N_1(\sigma; 1)$ está contido no fecho de $N(\sigma; A, \infty)$.

$\blacksquare$

**Implicações:** Este teorema tem implicações significativas para a aproximação e otimização em redes neurais. Ele demonstra que, em geral, não podemos esperar que o conjunto de redes neurais de arquitetura fixa seja "bem comportado" em termos de convexidade. A existência de buracos arbitrariamente grandes sugere que a otimização pode ser extremamente difícil, pois algoritmos podem ficar presos em regiões do espaço de parâmetros longe de soluções ótimas [^1].

### Conclusão
Este capítulo explorou a natureza não convexa e a presença de buracos arbitrariamente grandes nos espaços de redes neurais de arquitetura fixa. A não $\epsilon$-convexidade, combinada com o teorema da densidade, revela a complexidade inerente à geometria desses espaços. A otimização nesses espaços é, portanto, um desafio significativo, exigindo o desenvolvimento de técnicas robustas que possam lidar com a não convexidade e a presença de "buracos" de tamanho arbitrário. Os resultados apresentados aqui fornecem uma base teórica para entender as dificuldades práticas encontradas no treinamento de redes neurais e motivam a busca por novas abordagens para a otimização e generalização.

### Referências
[^1]: Capítulo 13 do texto fornecido.
[^2]: Definition 13.4 [^4].
[^3]: Proposition 13.5 [^4].
[^4]: Página 178 [^4].
[^5]: Página 177 [^3].
[^6]: Definition 13.8 [^6].
[^7]: Theorem 13.9 [^7].

<!-- END -->