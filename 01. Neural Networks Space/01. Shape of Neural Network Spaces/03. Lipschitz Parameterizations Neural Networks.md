## 13.1 Lipschitz Parameterizations and Sensitivity of Neural Networks

### Introdução
Este capítulo explora a geometria do espaço de redes neurais, focando em como a parametrização de Lipschitz afeta a sensibilidade e estabilidade das redes neurais. Como vimos anteriormente, a paisagem de perda de redes neurais pode ser intrincada e tipicamente não convexa [^1]. Investigaremos o mapa de realização $R_\theta$ e sua relação com a continuidade de Lipschitz das redes neurais. Em particular, analisaremos como mudanças nos parâmetros da rede afetam a função resultante, um aspecto crucial para entender a sensibilidade e estabilidade das redes neurais [^1].

### Conceitos Fundamentais

#### O Mapa de Realização e Continuidade de Lipschitz
O estudo das parametrizações de Lipschitz através do mapa de realização $R_\theta$ oferece *insights* sobre como as mudanças nos parâmetros da rede afetam a função resultante [^1]. Este é um ponto central para entender a sensibilidade e estabilidade das redes neurais. O mapa de realização $R_\theta$, introduzido na Definição 12.1, associa os parâmetros de uma rede neural à função que ela representa [^1].

**Proposição 13.1** [^2]: Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, onde $d_i$ representa a dimensão da *i*-ésima camada, e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ uma função Lipschitz $C_\sigma$-contínua com $C_\sigma \geq 1$. Além disso, suponha que $|\sigma(x)| \leq C_\sigma |x|$ para todo $x \in \mathbb{R}$, e seja $B > 1$. Então, para todo $\theta, \theta' \in P_N(A, B)$, onde $P_N(A, B)$ denota o conjunto de redes neurais com arquitetura $A$ e pesos limitados por $B$, temos:
$$
||R_\theta(\theta) - R_\theta(\theta')||_{L^\infty([-1,1]^{d_0})} \leq (2C_\sigma B d_{max})^L n_A ||\theta - \theta'||_\infty
$$
onde $d_{max} = \max_{l=0,...,L+1} d_l$ e $n_A = \sum_{l=0}^{L} d_{l+1}(d_l + 1)$.

**Prova:** A prova desta proposição envolve o uso repetido da desigualdade triangular para encontrar uma sequência $(\theta_j)_{j=0}^{n_A}$ tal que $\theta_0 = \theta$, $\theta_{n_A} = \theta'$, $||\theta_j - \theta_{j+1}||_\infty \leq \delta$, onde $\delta := ||\theta - \theta'||_\infty$, e $\theta_j$ e $\theta_{j+1}$ diferem em apenas uma entrada para todo $j = 0, ..., n_A - 1$ [^2]. Concluímos que para todo $x \in [-1,1]^{d_0}$:

$$
||R_\theta(\theta)(x) - R_\theta(\theta')(x)||_{L^\infty} \leq \sum_{j=0}^{n_A - 1} ||R_\theta(\theta_j)(x) - R_\theta(\theta_{j+1})(x)||_{\infty} \qquad (13.1.1)
$$
Para limitar superiormente (13.1.1), precisamos entender o efeito de mudar um peso em uma rede neural por $\delta$ [^2]. Antes de completar a prova, precisamos de dois lemas auxiliares [^2].

**Lema 13.2** [^2]: Sob as hipóteses da Proposição 13.1, mas com $B$ sendo permitido ser arbitrário positivo, temos para todo $\Phi \in N(\sigma; A, B)$:
$$
||\Phi(x) - \Phi(x')||_\infty \leq C_\sigma (B d_{max})^{L+1} ||x - x'||_\infty \qquad (13.1.2)
$$
para todo $x, x' \in \mathbb{R}^{d_0}$.

**Prova:** Começamos com o caso onde $L = 1$. Então, para $(d_0, d_1, d_2) = A$, temos que:
$$
\Phi(x) = W^{(1)}(\sigma(W^{(0)}x + b^{(0)})) + b^{(1)}
$$
para certos $W^{(0)}, W^{(1)}, b^{(0)}, b^{(1)}$ com todas as entradas limitadas por $B$ [^2]. Como consequência, podemos estimar:
$$
\begin{aligned}
||\Phi(x) - \Phi(x')||_\infty &= ||W^{(1)}(\sigma(W^{(0)}x + b^{(0)})) - \sigma(W^{(0)}x' + b^{(0)}))||_\infty \\
&\leq d_1 B ||\sigma(W^{(0)}x + b^{(0)}) - \sigma(W^{(0)}x' + b^{(0)})||_\infty \\
&\leq d_1 B C_\sigma ||W^{(0)}(x - x')||_\infty \\
&\leq d_1 d_0 B^2 C_\sigma ||x - x'||_\infty \leq C_\sigma (d_{max}B)^2 ||x - x'||_\infty
\end{aligned}
$$
onde usamos a propriedade de Lipschitz de $\sigma$ e o fato de que $||Ax||_\infty \leq n \max_{i,j} |A_{ij}| ||x||_\infty$ para toda matriz $A = (A_{ij})_{i=1, j=1}^{m,n} \in \mathbb{R}^{m \times n}$. O passo de indução de $L$ para $L+1$ segue similarmente [^2]. Isso conclui a prova do lema. $\blacksquare$

**Lema 13.3** [^3]: Sob as hipóteses da Proposição 13.1, temos que:
$$
||x^{(l)}||_\infty \leq (2C_\sigma B d_{max})^l
$$
para todo $x \in [-1,1]^{d_0}$.

**Prova:** Pelas Definições (2.1.1b) e (2.1.1c), temos que para $l = 1, ..., L+1$:
$$
||x^{(l)}||_\infty \leq C_\sigma ||W^{(l-1)}x^{(l-1)} + b^{(l-1)}||_\infty \leq C_\sigma B d_{max} ||x^{(l-1)}||_\infty + BC_\sigma
$$
onde usamos a desigualdade triangular e a estimativa $||Ax||_\infty \leq n \max_{i,j} |A_{ij}| ||x||_\infty$, que vale para toda matriz $A \in \mathbb{R}^{m \times n}$ [^3]. Obtemos que:

$$
||x^{(l)}||_\infty \leq C_\sigma B d_{max} (1 + ||x^{(l-1)}||_\infty) \leq 2 C_\sigma B d_{max} (\max\{1, ||x^{(l-1)}||_\infty\})
$$

Resolvendo a estimativa recursiva de $||x^{(l)}||_\infty$ por $2C_\sigma B d_{max} (\max\{1, ||x^{(l-1)}||_\infty\})$, concluímos que:

$$
||x^{(l)}||_\infty \leq (2 C_\sigma B d_{max}) \max\{1, ||x^{(0)}||_\infty\} = (2 C_\sigma B d_{max})^l
$$
Isto conclui a prova do lema. $\blacksquare$

Agora podemos prosseguir com a prova da Proposição 13.1 [^3]. Assumimos que $\theta_{j+1}$ e $\theta_j$ diferem apenas em uma entrada [^3]. Assumimos que esta entrada está na *l*-ésima camada, e começamos com o caso $l < L$ [^3]. Vale que:
$$
|R_\theta(\theta_j)(x) - R_\theta(\theta_{j+1})(x)| = |\Phi^l(\sigma(W^{(l)}x^{(l)} + b^{(l)})) - \Phi^l(\sigma(W'^{(l)}x^{(l)} + b'^{(l)}))|
$$
onde $\Phi^l \in N(\sigma; A', B)$ para $A' = (d_{l+1}, ..., d_{L+1})$ e $(W^{(l)}, b^{(l)}), (W'^{(l)}, b'^{(l)})$ diferem em apenas uma entrada [^3]. Usando a continuidade de Lipschitz de $\Phi^l$ do Lema 13.2, temos:
$$
\begin{aligned}
|R_\theta(\theta_j)(x) - R_\theta(\theta_{j+1})(x)| &\leq C^{L-l-1}(Bd_{max})^{L-l} |\sigma(W^{(l)}x^{(l)} + b^{(l)}) - \sigma(W'^{(l)}x^{(l)} + b'^{(l)})| \\
&\leq C^{L-l}(Bd_{max})^{L-l} |W^{(l)}x^{(l)} + b^{(l)} - W'^{(l)}x^{(l)} - b'^{(l)}| \\
&\leq C^{L-l}(Bd_{max})^{L-l} d_{max} \delta \max\{1, ||x^{(l)}||\}
\end{aligned}
$$
onde $\delta := ||\theta - \theta'||_{max}$ [^3]. Invocando (13.3), concluímos que:
$$
|R_\theta(\theta_j)(x) - R_\theta(\theta_{j+1})(x)| \leq (2C_\sigma B d_{max})^l C^{L-l} (Bd_{max})^{L-l} \delta \leq (2C_\sigma B d_{max})^L ||\theta - \theta'||_{max}
$$
Para o caso $l = L$, uma estimativa similar pode ser mostrada [^3]. Combinando isso com (13.1.1) resulta no resultado [^3]. $\blacksquare$

Usando a Proposição 13.1, podemos agora considerar o conjunto de redes neurais com uma arquitetura fixa $N(\sigma; A, \infty)$ como um subconjunto de $L^\infty([-1,1]^{d_0})$ [^3]. Além disso, $N(\sigma; A, \infty)$ é a imagem de $P_N(A, \infty)$ sob um mapa localmente Lipschitz [^3].

### Conclusão
Este capítulo estabeleceu uma base para entender a sensibilidade das redes neurais através da análise das parametrizações de Lipschitz e do mapa de realização [^1]. A Proposição 13.1 fornece um limite para a diferença entre as funções produzidas por duas redes neurais com parâmetros próximos [^2]. Os Lemas 13.2 e 13.3 fornecem ferramentas auxiliares para a prova da proposição principal [^2, 3]. Este resultado é crucial para garantir a estabilidade e generalização das redes neurais, pois limita a influência de pequenas variações nos parâmetros da rede sobre a função aprendida. Os resultados apresentados abrem caminho para a investigação da convexidade e das propriedades de aproximação do espaço de redes neurais, que exploraremos nas próximas seções.

### Referências
[^1]: Capítulo 13, Shape of Neural Network Spaces, p. 175.
[^2]: Capítulo 13, Shape of Neural Network Spaces, p. 176.
[^3]: Capítulo 13, Shape of Neural Network Spaces, p. 177.
<!-- END -->