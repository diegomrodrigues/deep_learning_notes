## Não Convexidade, Não-Fechamento e Propriedades de Aproximação em Espaços de Redes Neurais

### Introdução
Este capítulo investiga as propriedades geométricas dos espaços de redes neurais, complementando a discussão anterior sobre a intrincada topologia do *loss landscape* [^1]. Enquanto o capítulo anterior focava na não-convexidade do *loss landscape* em termos dos parâmetros da rede, este capítulo aborda a forma do espaço de redes neurais como subconjuntos de espaços de funções [^1]. Exploraremos como a não-convexidade, a falta de fechamento e a ausência de propriedades de melhor aproximação afetam a capacidade de aproximação e o comportamento do treinamento de redes neurais [^1]. Em particular, examinaremos a propriedade de seleção contínua e a propriedade de melhor aproximação, demonstrando que, sob certas condições, os espaços de redes neurais não satisfazem nenhuma delas [^1].

### Conceitos Fundamentais

#### Propriedade de Seleção Contínua

A **propriedade de seleção contínua** é uma característica desejável em espaços de aproximação, pois garante a existência de um algoritmo de seleção estável [^8]. Formalmente, um espaço de redes neurais $N(\sigma; A, \infty) \subseteq H$, onde $H$ é um espaço de funções normado, satisfaz a propriedade de seleção contínua se existe uma função contínua $\varphi: H \rightarrow N(\sigma; A, \infty)$ tal que $\Phi = \varphi(h)$ satisfaz [^1]:

$$||\Phi - h||_H = \inf_{\Phi^* \in N(\sigma; A, \infty)} ||\Phi^* - h||_H \quad \forall h \in H$$

Em outras palavras, $\varphi$ seleciona continuamente a melhor aproximação em $N(\sigma; A, \infty)$ para cada função $h \in H$ [^8]. No entanto, como mencionado em [^8] e demonstrado no contexto fornecido pela Proposição 13.11 [^2, 9], os espaços de redes neurais geralmente não possuem essa propriedade.

**Proposição 13.11**: Seja $L \in \mathbb{N}$, $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua e não um polinômio, e seja $p \in (1, \infty)$. Então, $N(\sigma; A, \infty) \subseteq L^p([-1, 1]^{d_0})$ não possui a propriedade de seleção contínua [^9].

*Prova*: A prova desta proposição demonstra que, sob as condições especificadas, $N(\sigma; A, \infty)$ não é convexo [^9]. Consequentemente, $N(\sigma; A, \infty)$ não possui a propriedade de melhor aproximação única [^9]. Ademais, se $N(\sigma; A, \infty)$ não possui a propriedade de melhor aproximação, então trivialmente não possui a propriedade de seleção contínua [^9]. Assim, pode-se assumir, sem perda de generalidade, que $N(\sigma; A, \infty)$ possui a propriedade de melhor aproximação, e que existe um ponto $h \in L^p([-1, 1]^{d_0})$ e duas funções diferentes $\Phi_1, \Phi_2$ tais que [^9]:

$$||\Phi_1 - h||_{L^p} = ||\Phi_2 - h||_{L^p} = \inf_{\Phi^* \in N(\sigma; A, \infty)} ||\Phi^* - h||_{L^p}$$

A prova então constrói um caminho contínuo $P(\lambda)$ em $L^p$, onde $\lambda \in [-1, 1]$, que interpola entre $\Phi_1$ e $\Phi_2$ passando por $h$ [^9]. Ao assumir, por contradição, que existe uma seleção contínua $\Phi^*$, a prova demonstra que $\Phi_1$ deve ser o minimizador único para $P(\lambda)$ para $\lambda < 0$, e similarmente $\Phi_2$ para $\lambda > 0$ [^9]. Isso contradiz a continuidade da função de seleção $\varphi$ [^9]. $\blacksquare$

#### Propriedade de Melhor Aproximação

A **propriedade de melhor aproximação** garante que, para qualquer função alvo em um espaço de funções, existe pelo menos uma rede neural no espaço da rede neural que a aproxima melhor [^8]. Formalmente, $N(\sigma; A, \infty) \subseteq H$ possui a propriedade de melhor aproximação se, para todo $h \in H$, existe pelo menos um $\Phi \in N(\sigma; A, \infty)$ tal que [^8]:

$$||\Phi - h||_H = \inf_{\Phi^* \in N(\sigma; A, \infty)} ||\Phi^* - h||_H$$

No entanto, em muitos casos, essa propriedade não se mantém para espaços de redes neurais [^10]. A Proposição 13.12 demonstra que, sob certas condições, existe uma sequência em $N(\sigma; A, \infty)$ que converge em $L^p([-1, 1]^{d_0})$ para todo $p \in (1, \infty)$, mas o limite dessa sequência é descontínuo [^10].

**Proposição 13.12**: Seja $A = (1, 2, 1)$ e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua. Assuma adicionalmente que existem $r > 0$ e $\alpha' \neq \alpha$ tais que $\sigma$ é diferenciável para todo $|x| > r$ e $\sigma'(x) \rightarrow \alpha$ para $x \rightarrow \infty$, $\sigma'(x) \rightarrow \alpha'$ para $x \rightarrow -\infty$ [^10].

Então, existe uma sequência em $N(\sigma; A, \infty)$ que converge em $L^p([-1, 1]^{d_0})$ para todo $p \in (1, \infty)$, e o limite desta sequência é descontínuo. Em particular, o limite da sequência não reside em $N(\sigma; A', \infty)$ para nenhum $A'$ [^10].

*Prova*: A prova constrói uma sequência de funções $f_n(x) = \sigma(nx + 1) - \sigma(nx)$ que podem ser escritas como redes neurais com arquitetura (1, 2, 1) [^10]. Demonstra-se que $f_n(x)$ converge pontualmente para $\alpha$ para $x > 0$ e para $\alpha'$ para $x < 0$ [^10]. Usando o teorema da convergência dominada, conclui-se que $f_n$ converge para uma função descontínua em $L^p$ [^10]. Como o limite é descontínuo, ele não pode pertencer a nenhum espaço de redes neurais $N(\sigma; A', \infty)$ [^10]. $\blacksquare$

#### Implicações da Ausência de Melhores Aproximações

A ausência da propriedade de melhor aproximação tem implicações significativas para o treinamento de redes neurais [^1]. Em particular, a Proposição 13.14 demonstra que, se uma função $f$ não possui uma melhor aproximação em $N(\sigma; A, \infty)$ e uma sequência de redes neurais $\Phi_n$ converge para $f$ no espaço $L^2$, então as normas dos pesos de $\Phi_n$ devem divergir para infinito [^11].

**Proposição 13.14**: Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua com $C_\sigma \geq 1$ e $|\sigma(x)| \leq C_\sigma |x|$ para todo $x \in \mathbb{R}$, e seja $\mu$ uma medida em $[-1, 1]^{d_0}$ [^11].

Assuma que existe uma sequência $\Phi_n \in N(\sigma; A, \infty)$ e $f \in L^2([-1, 1]^{d_0}, \mu) \setminus N(\sigma; A, \infty)$ tal que [^11]:

$$||\Phi_n - f||_{L^2([-1, 1]^{d_0}, \mu)} \rightarrow 0$$

Então [^11]:

$$\lim_{n \rightarrow \infty} \sup \{||W_n^{(l)}||_\infty, ||b_n^{(l)}||_\infty : l = 0, ..., L\} = \infty$$

*Prova*: A prova assume, por contradição, que os pesos permanecem limitados [^11]. Nesse caso, a sequência $\Phi_n$ pertenceria a um conjunto compacto, o que implicaria que o limite $f$ também pertenceria a $N(\sigma; A, \infty)$, contradizendo a hipótese [^11]. $\blacksquare$

Esse resultado sugere que, ao tentar aprender funções que estão fora do espaço de aproximação da rede neural, os pesos da rede tendem a crescer indefinidamente, o que pode levar a problemas de generalização [^12].

#### Convexidade e Não Convexidade

Como vimos na proposição 13.5, $N(\sigma; A, \infty)$ é invariante à escala, ou seja, para todo $\lambda \in \mathbb{R}$, $\lambda f \in N(\sigma; A, \infty)$ se $f \in N(\sigma; A, \infty)$ [^4]. Consequentemente, $0 \in N(\sigma; A, \infty)$ é um centro de $N(\sigma; A, \infty)$ [^4].

A não convexidade dos espaços de redes neurais tem implicações importantes para algoritmos de otimização [^5]. Ao contrário de conjuntos convexos, onde uma linha reta entre dois pontos permanece dentro do conjunto, os espaços de redes neurais podem ter "buracos" ou regiões onde essa propriedade não se mantém [^5]. Isso pode dificultar a convergência de algoritmos de otimização, pois eles podem ficar presos em mínimos locais ou ter dificuldade em explorar o espaço de busca de forma eficiente [^5].

### Conclusão

Este capítulo apresentou uma análise detalhada das propriedades geométricas dos espaços de redes neurais, com foco na propriedade de seleção contínua, propriedade de melhor aproximação, e convexidade. Demonstramos que, sob certas condições, os espaços de redes neurais não satisfazem essas propriedades, o que tem implicações significativas para a capacidade de aproximação e o comportamento do treinamento de redes neurais. Em particular, a ausência da propriedade de melhor aproximação pode levar à divergência dos pesos da rede durante o treinamento, enquanto a não convexidade pode dificultar a convergência de algoritmos de otimização.

### Referências

[^1]: Capítulo 13, "Shape of neural network spaces", p. 175.
[^2]: Capítulo 13, "Shape of neural network spaces", p. 176.
[^3]: Capítulo 13, "Shape of neural network spaces", p. 177.
[^4]: Capítulo 13, "Shape of neural network spaces", p. 178.
[^5]: Capítulo 13, "Shape of neural network spaces", p. 179.
[^6]: Capítulo 13, "Shape of neural network spaces", p. 180.
[^7]: Capítulo 13, "Shape of neural network spaces", p. 181.
[^8]: Capítulo 13, "Shape of neural network spaces", p. 182.
[^9]: Capítulo 13, "Shape of neural network spaces", p. 183.
[^10]: Capítulo 13, "Shape of neural network spaces", p. 184.
[^11]: Capítulo 13, "Shape of neural network spaces", p. 185.
[^12]: Capítulo 13, "Shape of neural network spaces", p. 186.
<!-- END -->