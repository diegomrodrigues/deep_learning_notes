## Aprofundando na Não-Convexidade e Aproximação em Espaços de Redes Neurais

### Introdução
Como discutido no capítulo anterior, o espaço de perda de redes neurais é tipicamente não-convexo [^1]. Este capítulo explora as consequências dessa não-convexidade na capacidade de aproximar funções arbitrárias e no comportamento do treinamento de redes neurais [^1]. Investigaremos a estrutura do espaço de redes neurais, particularmente a presença de "buracos" arbitrários e a falta de melhores aproximações, o que pode levar ao crescimento ilimitado dos pesos durante o treinamento [^1]. Estudaremos a convexidade de conjuntos de redes neurais e propriedades de melhor aproximação [^1].

### Conceitos Fundamentais

#### Não-Convexidade e "Buracos"
A não-convexidade dos espaços de redes neurais é um problema central [^1]. Intuitivamente, isso significa que, dados dois modelos de redes neurais, a combinação linear desses modelos não necessariamente resulta em outro modelo válido dentro do mesmo espaço. Mais grave ainda, esses espaços podem conter "buracos" arbitrários, o que implica que grandes regiões do espaço de funções não podem ser representadas por nenhuma rede neural da arquitetura especificada [^1].

#### Star-Shape e Centros
Apesar da não-convexidade, os conjuntos de redes neurais exibem algumas propriedades geométricas interessantes [^4]. O conceito de **star-shape** é introduzido: um conjunto *Z* é star-shaped se existe um ponto *x* ∈ *Z* (um **centro** de *Z*) tal que, para todo *y* ∈ *Z*, o segmento de reta que conecta *x* a *y* está contido em *Z* [^4].

**Definição 13.4 [^4].** Seja *Z* um subconjunto de um espaço linear. Um ponto *x* ∈ *Z* é chamado de centro de *Z* se, para todo *y* ∈ *Z*, vale que $\\{tx + (1-t)y \mid t \in [0,1]\\} \subseteq Z$. Um conjunto é chamado star-shaped se ele tem pelo menos um centro.

Uma propriedade importante é que $\mathcal{N}(\sigma; A, \infty)$ é *scaling invariant*, o que significa que se $f \in \mathcal{N}(\sigma; A, \infty)$, então $\lambda f \in \mathcal{N}(\sigma; A, \infty)$ para todo $\lambda \in \mathbb{R}$ [^4]. Como consequência, a função nula (0) é um centro de $\mathcal{N}(\sigma; A, \infty)$ [^4]. Além disso, funções constantes também são centros [^4].

#### $\epsilon$-Convexidade
Para quantificar a não-convexidade, introduzimos o conceito de $\epsilon$-convexidade [^5].

**Definição 13.8 [^5].** Para $\epsilon > 0$, dizemos que um subconjunto *A* de um espaço vetorial normado *X* é $\epsilon$-convexo se $\text{co}(A) \subseteq A + B_{\epsilon}(0)$, onde $\text{co}(A)$ denota o *convex hull* de *A* e $B_{\epsilon}(0)$ é uma bola de raio $\epsilon$ centrada em 0 com respeito à norma de *X*.

Intuitivamente, um conjunto é $\epsilon$-convexo se ele "preenche" todos os "buracos" menores que $\epsilon$ [^5]. No entanto, para redes neurais, demonstra-se que não existe um $\epsilon > 0$ tal que $\mathcal{N}(\sigma; A, \infty)$ seja $\epsilon$-convexo [^5]. Isso implica que os "buracos" no espaço de redes neurais podem ser arbitrariamente grandes [^5].

**Teorema 13.9 [^7].** Seja $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$. Seja $K \subseteq \mathbb{R}^{d_0}$ compacto e seja $\sigma \in \mathcal{M}$, com $\mathcal{M}$ como em (3.1.1) e assuma que $\sigma$ não é um polinômio. Além disso, assuma que existe um conjunto aberto, onde $\sigma$ é diferenciável e não constante. Se existe um $\epsilon > 0$ tal que $\mathcal{N}(\sigma; A, \infty)$ é $\epsilon$-convexo, então $\mathcal{N}(\sigma; A, \infty)$ é denso em $C(K)$.

#### Propriedades de Aproximação
A não-convexidade e a presença de "buracos" têm implicações diretas na capacidade de aproximar funções arbitrárias com redes neurais [^8]. Definimos as seguintes propriedades [^8]:

*   **Melhor Aproximação:** Para toda função *h* em um espaço funcional *H*, existe uma rede neural $\Phi \in \mathcal{N}(\sigma; A, \infty)$ que minimiza a distância entre $\Phi$ e *h*.
*   **Aproximação Única:** Para toda função *h* em *H*, existe uma *única* rede neural $\Phi \in \mathcal{N}(\sigma; A, \infty)$ que minimiza a distância entre $\Phi$ e *h*.
*   **Seleção Contínua:** Existe uma função contínua $\phi: H \to \mathcal{N}(\sigma; A, \infty)$ que mapeia cada função *h* em sua melhor aproximação $\Phi = \phi(h)$.

Em geral, as classes de redes neurais não satisfazem a propriedade de seleção contínua nem a propriedade de melhor aproximação [^8].

#### Seleção Contínua
A propriedade de seleção contínua é particularmente interessante porque implica a existência de um algoritmo de seleção estável: algoritmos que produzem redes neurais similares para problemas similares [^8]. No entanto, como demonstrado em [109], espaços de redes neurais raramente admitem a propriedade de seleção contínua [^8].

**Teorema 13.10 [^8].** Seja $p \in (1, \infty)$. Todo subconjunto de $L^p([-1,1]^{d_0})$ com a propriedade de melhor aproximação única é convexo.

**Proposição 13.11 [^9].** Sejam $L \in \mathbb{N}$, $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, $\sigma: \mathbb{R} \to \mathbb{R}$ Lipschitz contínua e não um polinômio, e $p \in (1, \infty)$. Então, $\mathcal{N}(\sigma; A, \infty) \subseteq L^p([-1,1]^{d_0})$ não possui a propriedade de seleção contínua.

#### Existência de Melhores Aproximações
Em muitos casos, a propriedade de melhor aproximação também não é satisfeita [^10]. Isso significa que, para algumas funções, não existe uma rede neural que minimize a distância entre a função e o conjunto de redes neurais [^10].

**Proposição 13.12 [^10].** Seja $A = (1,2,1)$ e seja $\sigma: \mathbb{R} \to \mathbb{R}$ Lipschitz contínua. Adicionalmente, assuma que existem $r > 0$ e $\alpha\' \neq \alpha$ tais que $\sigma$ é diferenciável para todo $|x| > r$ e $\sigma\'(x) \to \alpha$ para $x \to \infty$, $\sigma\'(x) \to \alpha\'$ para $x \to -\infty$. Então, existe uma sequência em $\mathcal{N}(\sigma; A, \infty)$ que converge em $L^p([-1,1]^{d_0})$, para todo $p \in (1, \infty)$, e o limite dessa sequência é descontínuo. Em particular, o limite da sequência não está em $\mathcal{N}(\sigma; A\', \infty)$ para nenhum $A\'$.

#### Fenômeno da Explosão de Pesos
A falta de melhores aproximações pode levar ao fenômeno da "explosão de pesos" durante o treinamento [^11]. Se uma função *f* não pode ser representada por uma rede neural da arquitetura especificada, mas o treinamento busca aproximar *f*, os pesos da rede neural podem crescer indefinidamente [^11].

**Proposição 13.14 [^11].** Sejam $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, $\sigma: \mathbb{R} \to \mathbb{R}$ Lipschitz contínua com $C_0 \geq 1$ e $|\sigma(x)| \leq C_0 |x|$ para todo $x \in \mathbb{R}$, e seja $\mu$ uma medida em $[-1,1]^{d_0}$. Assuma que existe uma sequência $\Phi_n \in \mathcal{N}(\sigma; A, \infty)$ e $f \in L^2([-1,1]^{d_0}, \mu) \setminus \mathcal{N}(\sigma; A, \infty)$ tais que $||\Phi_n - f||_{L^2([-1,1]^{d_0}, \mu)} \to 0$. Então, $\limsup_{n \to \infty} \max \\{||W_n^{(l)}||_\infty, ||b_n^{(l)}||_\infty \mid l = 0, ..., L\\} = \infty$.

### Conclusão
A não-convexidade e a presença de "buracos" nos espaços de redes neurais têm implicações significativas para a teoria da aproximação e o treinamento de redes neurais [^1]. A falta de melhores aproximações pode levar ao crescimento descontrolado dos pesos, o que pode ser indesejável em aplicações práticas [^11]. Compreender a geometria desses espaços é crucial para desenvolver algoritmos de treinamento mais robustos e eficazes [^1].

### Referências
[^1]: Capítulo 13, Shape of neural network spaces.
[^4]: Definição 13.4 e Proposição 13.5, Capítulo 13, Shape of neural network spaces.
[^5]: Definição 13.8, Capítulo 13, Shape of neural network spaces.
[^7]: Teorema 13.9, Capítulo 13, Shape of neural network spaces.
[^8]: Seção 13.3, Capítulo 13, Shape of neural network spaces.
[^9]: Proposição 13.11, Capítulo 13, Shape of neural network spaces.
[^10]: Proposição 13.12, Capítulo 13, Shape of neural network spaces.
[^11]: Proposição 13.14, Capítulo 13, Shape of neural network spaces.
<!-- END -->