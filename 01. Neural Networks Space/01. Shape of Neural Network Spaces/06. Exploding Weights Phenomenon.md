## 13.3.3 Exploding Weights Phenomenon

### Introdução
Em continuidade com a discussão sobre as propriedades dos espaços de redes neurais e suas implicações para o aprendizado, este capítulo foca no fenômeno do "exploding weights" (pesos explosivos). Como vimos anteriormente, a não convexidade e a falta da propriedade de melhor aproximação em espaços de redes neurais podem levar a comportamentos indesejáveis durante o treinamento. Este fenômeno se manifesta quando os pesos da rede neural tendem ao infinito, o que pode prejudicar a generalização e a estabilidade do modelo [^1]. Nesta seção, exploraremos as condições sob as quais esse fenômeno ocorre e suas implicações práticas.

### Conceitos Fundamentais
O fenômeno dos pesos explosivos surge particularmente quando se tenta aprender funções que não possuem uma melhor aproximação dentro do espaço da rede neural [^1]. Em problemas de regressão, o objetivo é encontrar uma sequência de redes neurais ($\Phi_n$) que minimize o risco, definido como o erro quadrático médio entre a saída da rede e a função alvo [^1]. Matematicamente, o risco é dado por:

$$\
R(\Phi_n) = \mathbb{E}[( \Phi_n(x) - f(x) )^2]
$$

onde $f(x)$ é a função alvo e $\mathbb{E}$ denota a esperança matemática sobre a distribuição dos dados.

Se a sequência de redes neurais ($\Phi_n$) converge em $L^2$ para uma função $f$, mas $f$ não pertence ao espaço da rede neural $N(\sigma; A, \infty)$, os pesos de $\Phi_n$ divergirão [^1]. Isso significa que, para alcançar uma aproximação cada vez melhor, a rede neural precisa ajustar seus pesos para valores cada vez maiores, o que pode levar a instabilidades e dificuldades na generalização.

**Proposição 13.14** formaliza essa ideia [^1]:

> Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua com $C_0 \geq 1$, e $|\sigma(x)| \leq C_0|x|$ para todo $x \in \mathbb{R}$, e seja $\mu$ uma medida em $[-1,1]^{d_0}$. Assuma que existe uma sequência $\Phi_n \in N(\sigma; A, \infty)$ e $f \in L^2([-1,1]^{d_0}, \mu) \setminus N(\sigma; A, \infty)$ tal que
$$\
||\Phi_n - f||_{L^2([-1,1]^{d_0}, \mu)} \rightarrow 0.
$$
Então,
$$\
\limsup_{n \rightarrow \infty} \max \\{ ||W_n^{(l)}||_\infty, ||b_n^{(l)}||_\infty | l = 0, ..., L \\} = \infty.
$$

Essa proposição afirma que, se uma sequência de redes neurais se aproxima de uma função que não pertence ao espaço da rede neural, então os pesos da rede devem tender ao infinito [^1].

**Demonstração (Prova por Contradição):**

Assumimos, por contradição, que o lado esquerdo de (13.3.7) é finito [^1]. Como resultado, existe $C > 0$ tal que $\Phi_n \in N(\sigma; A, C)$ para todo $n \in \mathbb{N}$ [^1]. Pela Proposição 13.1, concluímos que $N(\sigma; A, C)$ é a imagem de um conjunto compacto sob um mapa contínuo e, portanto, é um conjunto compacto em $L^2([-1,1]^{d_0}, \mu)$ [^1]. Em particular, temos que $N(\sigma; A, C)$ é fechado [^1]. Portanto, (13.3.6) implica que $f \in N(\sigma; A, C)$, o que gera uma contradição. $\blacksquare$

Essa demonstração mostra que a convergência para uma função fora do espaço da rede neural implica necessariamente que os pesos da rede devem crescer ilimitadamente.

### Conclusão
O fenômeno dos pesos explosivos é uma consequência direta da falta de uma melhor aproximação dentro do espaço da rede neural [^1]. Quando a rede tenta aprender uma função que está fora de seu alcance, os pesos precisam crescer indefinidamente para minimizar o erro, o que pode levar a problemas de generalização e instabilidade [^1]. Este resultado destaca a importância de entender as limitações dos espaços de redes neurais e de escolher arquiteturas e funções de ativação adequadas para o problema em questão. Além disso, sugere a necessidade de técnicas de regularização que limitem o crescimento dos pesos, a fim de evitar o fenômeno dos pesos explosivos e melhorar a generalização do modelo.

### Referências
[^1]: Capítulo 13 do texto fornecido.
<!-- END -->