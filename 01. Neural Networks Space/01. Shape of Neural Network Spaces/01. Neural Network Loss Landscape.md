## A Não-Convexidade e a Paisagem de Perda das Redes Neurais

### Introdução
Como mencionado anteriormente, a paisagem de perda das redes neurais é intrincada e tipicamente não-convexa [^1]. Este capítulo explora a natureza não-convexa dos espaços de redes neurais e suas implicações. A não-convexidade surge da parametrização da rede, mas a análise do risco empírico revela convexidade quando considerado como um mapa de funções de redes neurais, em vez de parâmetros [^1]. Este capítulo investiga as razões pelas quais não otimizamos sobre conjuntos de redes neurais em vez de parâmetros, estudando o conjunto de redes neurais associadas a uma arquitetura fixa como um subconjunto de outros espaços de função [^1].

### Conceitos Fundamentais

A paisagem de perda de uma rede neural é tipicamente não-convexa devido à sua parametrização [^1]. Isso significa que existem múltiplos mínimos locais e pontos de sela, tornando o processo de otimização desafiador. No entanto, o risco empírico pode ser convexo quando considerado como um mapa de funções de rede neural, em vez de parâmetros [^1]. Essa convexidade evita mínimos espúrios e pontos de sela, simplificando o processo de otimização.

A não-convexidade surge da parametrização da rede, que é uma consideração fundamental na análise do comportamento e treinamento de redes neurais [^1]. A parametrização refere-se à maneira como os parâmetros da rede são organizados e interagem uns com os outros. Essa organização pode criar não-convexidades na paisagem de perda, tornando difícil encontrar o mínimo global.

A realização do mapa $R_\theta$ introduzida na Definição 12.1 é investigada [^1]. Especificamente, mostra-se que, se $\sigma$ é Lipschitz, então o conjunto de redes neurais é a imagem de $P_N(A, \infty)$ sob um mapa localmente Lipschitz [^1]. Este fato é usado para mostrar que conjuntos de redes neurais são tipicamente não-convexos e até mesmo têm buracos arbitrariamente grandes [^1]. Finalmente, investiga-se a extensão em que existem melhores aproximações para funções arbitrárias no conjunto de redes neurais. Demonstra-se que a falta de melhores aproximações faz com que os pesos das redes neurais cresçam infinitamente durante o treinamento [^1].

A convexidade dos espaços de redes neurais é investigada, observando que $N(\sigma; A, \infty)$ é star-shaped com poucos centros [^4]. Um ponto $x \in Z$ é chamado de *centro* de $Z$ se, para todo $y \in Z$, $\{tx + (1-t)y | t \in [0,1]\} \subseteq Z$ [^4]. Um conjunto é chamado *star-shaped* se tem pelo menos um centro [^4]. $N(\sigma; A, \infty)$ é scaling invariant, i.e., para todo $\lambda \in \mathbb{R}$, $\lambda f \in N(\sigma; A, \infty)$ se $f \in N(\sigma; A, \infty)$, e, portanto, $0 \in N(\sigma; A, \infty)$ é um centro de $N(\sigma; A, \infty)$ [^4].

O Teorema 13.6 estabelece que, se $\sigma: \mathbb{R} \to \mathbb{R}$ é Lipschitz contínuo, então $N(\sigma; A, \infty)$ contém no máximo $n_A = \sum_{l=0}^{L} d_l d_{l+1}$ centros linearmente independentes [^4]. Assumindo, por contradição, que existem funções $(g_i)_{i=1}^{n_A+1} \subseteq N(\sigma; A, \infty) \subseteq L^{\infty}([-1, 1]^{d_0})$ que são linearmente independentes e centros de $N(\sigma; A, \infty)$, e usando o Teorema de Hahn-Banach, chega-se a uma contradição, provando o teorema [^4].

O Corolário 13.7 afirma que, se $N(\sigma; A, \infty)$ contém mais do que $n_A = \sum_{l=0}^{L} d_l d_{l+1}$ funções linearmente independentes, então $N(\sigma; A, \infty)$ não é convexo [^5].

É mostrado no Corolário 13.7 que não se pode esperar conjuntos convexos de redes neurais, se o conjunto de redes neurais tem muitos elementos linearmente independentes [^5]. Conjuntos de redes neurais contêm para cada $f \in N(\sigma; A, \infty)$ também todos os shifts desta função, i.e., $f(\cdot + b)$ para um $b \in \mathbb{R}^d$ são elementos de $f \in N(\sigma; A, \infty)$ [^5].

A não-convexidade do conjunto de redes neurais pode ter sérias consequências para a forma como pensamos no problema de aproximação ou aprendizado por redes neurais [^8]. Se $H$ é um espaço de funções normado em $[-1,1]^{d_0}$ tal que $N(\sigma; A, \infty) \subseteq H$, para $h \in H$ gostaríamos de encontrar uma rede neural que melhor aproxime $h$, i.e., encontrar $\Phi \in N(\sigma; A, \infty)$ tal que

$$
||\Phi - h||_H = \inf_{\Phi^* \in N(\sigma; A, \infty)} ||\Phi^* - h||_H.
$$

Dizemos que $N(\sigma; A, \infty) \subseteq H$ tem:

*   A **propriedade de melhor aproximação**, se para todo $h \in H$ existe pelo menos um $\Phi \in N(\sigma; A, \infty)$ tal que (13.3.1) é válida [^8].

*   A **propriedade de melhor aproximação única**, se para todo $h \in H$ existe exatamente um $\Phi \in N(\sigma; A, \infty)$ tal que (13.3.1) é válida [^8].

*   A **propriedade de seleção contínua**, se existe uma função contínua $\phi: H \to N(\sigma; A, \infty)$ tal que $\Phi = \phi(h)$ satisfaz (13.3.1) para todo $h \in H$ [^8].

É mostrado que as classes de redes neurais tipicamente não satisfazem nem a seleção contínua nem a propriedade de melhor aproximação [^8]. O Teorema 13.10 afirma que todo subconjunto de $L^p([-1,1]^{d_0})$ com a propriedade de melhor aproximação única é convexo [^8].

A Proposição 13.11 demonstra que, sob condições muito brandas, a propriedade de seleção contínua não pode ser válida [^9]. Além disso, o próximo resultado mostra que, em muitos casos, também a propriedade de melhor aproximação falha em ser satisfeita.

A Proposição 13.14 pode ser estendida para todas as $f$ para as quais não existe melhor aproximação em $N(\sigma; A, \infty)$ [^11]. Os resultados implicam que, para as funções que desejamos aprender que carecem de uma melhor aproximação dentro de um conjunto de redes neurais, devemos esperar que os pesos das redes neurais de aproximação cresçam para o infinito [^11]. Isso pode ser indesejável porque, como veremos nas seções seguintes sobre generalização, um espaço de parâmetros limitado facilita muitos limites de generalização [^11].

### Conclusão

Este capítulo explorou a complexa paisagem de perda das redes neurais, focando na não-convexidade e suas implicações para a otimização e aproximação. A não-convexidade, resultante da parametrização da rede, apresenta desafios significativos, mas a análise do risco empírico revela convexidade quando considerado como um mapa de funções de redes neurais [^1]. A investigação da realização do mapa e das propriedades de star-shaped dos espaços de redes neurais fornece insights sobre a estrutura desses espaços [^1, 4]. Os resultados sobre a inexistência da propriedade de seleção contínua e da propriedade de melhor aproximação destacam as limitações das redes neurais e a necessidade de considerar o crescimento dos pesos durante o treinamento [^8, 9].

### Referências
[^1]: Capítulo 13, p. 175
[^4]: Capítulo 13, p. 178
[^5]: Capítulo 13, p. 179
[^8]: Capítulo 13, p. 182
[^9]: Capítulo 13, p. 183
[^11]: Capítulo 13, p. 186
<!-- END -->