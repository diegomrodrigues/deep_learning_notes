## Convexidade e Invariância de Escala em Espaços de Redes Neurais

### Introdução
Como mencionado no capítulo anterior, a superfície de perda (loss landscape) das redes neurais pode ser complexa e tipicamente não convexa. Este capítulo explora a forma dos espaços de redes neurais, focando na convexidade e em outras propriedades geométricas. Em particular, analisaremos a invariância de escala, a propriedade de serem star-shaped e as implicações da não-convexidade para a aproximação de funções [^1].

### Conceitos Fundamentais

#### Invariância de Escala e Propriedade Star-Shaped
Os espaços de redes neurais, denotados como $N(\sigma; A, \infty)$, exibem invariância de escala. Isso significa que se uma função $f$ pertence a $N(\sigma; A, \infty)$, então $\lambda f$ também pertence a $N(\sigma; A, \infty)$ para qualquer $\lambda \in \mathbb{R}$. Essa propriedade implica que o ponto 0 é um centro de $N(\sigma; A, \infty)$ [^1].

Além disso, esses espaços são *star-shaped* com poucos centros. Um conjunto é star-shaped se possui pelo menos um centro $x$ tal que o segmento de reta entre $x$ e qualquer outro ponto $y$ no conjunto está contido no conjunto. Uma cota superior para o número de centros linearmente independentes em $N(\sigma; A, \infty)$ é dada por $n_A = \sum_{l=0}^{L} d_l d_{l+1}$, onde $d_l$ são as dimensões das camadas [^1].

**Definição 13.4.** Seja $Z$ um subconjunto de um espaço linear. Um ponto $x \in Z$ é chamado de centro de $Z$ se, para todo $y \in Z$, vale que $\\{tx + (1-t)y \mid t \in [0,1]\\} \subseteq Z$ [^4]. Um conjunto é chamado *star-shaped* se tem pelo menos um centro [^4].

**Proposição 13.5.** Seja $L \in \mathbb{N}$ e $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. Então, $N(\sigma; A, \infty)$ é scaling invariant, i.e., para todo $\lambda \in \mathbb{R}$ vale que $\lambda f \in N(\sigma; A, \infty)$ se $f \in N(\sigma; A, \infty)$, e portanto $0 \in N(\sigma; A, \infty)$ é um centro de $N(\sigma; A, \infty)$ [^4].

#### Convexidade e Centros Linearmente Independentes
Um resultado importante é que se $N(\sigma; A, \infty)$ contém mais do que $n_A$ funções linearmente independentes, então $N(\sigma; A, \infty)$ não é convexo [^1].  A convexidade de um conjunto implica que qualquer combinação convexa de pontos no conjunto também pertence ao conjunto.  A não-convexidade, portanto, introduz complexidades na otimização e na análise desses espaços.

**Corolário 13.7.** Seja $A = (d_0, d_1, ..., d_{L+1})$, e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua. Se $N(\sigma; A, \infty)$ contém mais do que $n_A = \sum_{l=0}^{L} d_l d_{l+1}$ funções linearmente independentes, então $N(\sigma; A, \infty)$ não é convexo [^5].

#### Implicações da Não-Convexidade
A não-convexidade dos espaços de redes neurais tem implicações significativas para o aprendizado e a aproximação. Por exemplo, a existência de múltiplos mínimos locais e pontos de sela pode dificultar a otimização, exigindo técnicas sofisticadas para escapar desses pontos [^1]. Além disso, a não-convexidade pode levar a comportamentos inesperados durante o treinamento, como o crescimento descontrolado dos pesos [^11].

#### Relação com a Propriedade de Melhor Aproximação

A não-convexidade dos espaços de redes neurais está intimamente ligada à falta da propriedade de melhor aproximação. A propriedade de melhor aproximação garante que, para qualquer função $h$ em um espaço funcional $H$, existe uma rede neural $\Phi$ em $N(\sigma; A, \infty)$ que minimiza a distância entre $\Phi$ e $h$ [^8].

**Proposição 13.12.** Seja $A = (1, 2, 1)$ e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ Lipschitz contínua. Adicionalmente, assuma que existe $r > 0$ e $\alpha\' \neq \alpha$ tal que $\sigma$ é diferenciável para todo $|x| > r$ e $\sigma\'(x) \rightarrow \alpha$ para $x \rightarrow \infty$, $\sigma\'(x) \rightarrow \alpha\'$ para $x \rightarrow -\infty$. Então, existe uma sequência em $N(\sigma; A, \infty)$ que converge em $L^p([-1,1]^{d_0})$, para todo $p \in (1, \infty)$, e o limite desta sequência é descontínuo. Em particular, o limite da sequência não está em $N(\sigma; A\', \infty)$ para nenhum $A\'$ [^10].

Quando a propriedade de melhor aproximação não é satisfeita, pode ser necessário que os pesos da rede neural tendam ao infinito para alcançar uma boa aproximação, levando ao fenômeno de "pesos explodindo" [^11].

### Conclusão

A análise da forma dos espaços de redes neurais revela propriedades importantes como a invariância de escala e a natureza star-shaped. No entanto, a não-convexidade desses espaços impõe desafios significativos para a otimização e a aproximação, exigindo abordagens cuidadosas para garantir um treinamento eficaz e resultados robustos [^1]. A falta da propriedade de melhor aproximação, juntamente com a não-convexidade, pode levar a comportamentos indesejáveis, como o crescimento descontrolado dos pesos, que devem ser mitigados por meio de técnicas de regularização e outras estratégias de otimização [^11].

### Referências

[^1]: Chapter 13, Shape of neural network spaces, p. 175
[^4]: Chapter 13, Shape of neural network spaces, p. 178
[^5]: Chapter 13, Shape of neural network spaces, p. 179
[^8]: Chapter 13, Shape of neural network spaces, p. 182
[^10]: Chapter 13, Shape of neural network spaces, p. 184
[^11]: Chapter 13, Shape of neural network spaces, p. 185
<!-- END -->