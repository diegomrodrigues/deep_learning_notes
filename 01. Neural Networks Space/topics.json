{
  "topics": [
    {
      "topic": "Shape of Neural Network Spaces",
      "sub_topics": [
        "The loss landscape of neural networks is typically non-convex due to parameterization, complicating optimization. However, empirical risk can be convex when considered as a map of neural network functions rather than parameters, avoiding spurious minima or saddle points. The non-convexity arises from the parameterization of the network, which is a key consideration when analyzing their behavior and training.",
        "Neural network sets are generally non-convex and can have arbitrarily large 'holes', affecting the ability to approximate arbitrary functions and potentially leading to unbounded weight growth during training due to the lack of best approximations. This non-convexity has implications for approximation and learning problems, leading to investigations into best approximation, unique best approximation, and continuous selection properties to understand how well neural networks can approximate functions in a given normed function space.",
        "Lipschitz parameterizations, studied through the realization map \\(R_\\theta\\), provide insights into how changes in network parameters affect the resulting function, which is crucial for understanding the sensitivity and stability of neural networks. If the activation function \\(\\sigma\\) is Lipschitz, the set of neural networks is the image of \\(P_N(A, \\infty)\\) under a locally Lipschitz map. Under certain assumptions, the Lipschitz continuity of neural networks can be bounded by \\(||\\Phi(x) - \\Phi(x')||_\\infty < C_\\sigma (Bd_{max})^{L+1} ||x - x'||_\\infty\\), where \\(C_\\sigma\\) is the Lipschitz constant, \\(B\\) is a bound on the weights, \\(d_{max}\\) is the maximum dimension of layers, and \\(L\\) is the number of layers.",
        "Neural network spaces \\(N(\\sigma; A, \\infty)\\) are scaling invariant, meaning that if a function \\(f\\) is in \\(N(\\sigma; A, \\infty)\\), then \\(\\lambda f\\) is also in \\(N(\\sigma; A, \\infty)\\) for any \\(\\lambda \\in \\mathbb{R}\\), implying that 0 is a center of \\(N(\\sigma; A, \\infty)\\). They are also star-shaped with few centers, where a set is star-shaped if it has at least one center \\(x\\) such that the line segment between \\(x\\) and any other point \\(y\\) in the set is contained in the set. An upper bound on the number of linearly independent centers in \\(N(\\sigma; A, \\infty)\\) is given by \\(n_A = \\sum_{l=0}^{L} d_l d_{l+1}\\), where \\(d_l\\) are the dimensions of the layers. If \\(N(\\sigma; A, \\infty)\\) contains more than \\(n_A\\) linearly independent functions, then \\(N(\\sigma; A, \\infty)\\) is not convex.",
        "Neural network spaces typically do not satisfy the continuous selection property, meaning there does not exist a continuous function \\(\\varphi: H \\rightarrow N(\\sigma; A, \\infty)\\) such that \\(\\Phi = \\varphi(h)\\) satisfies \\(||\\Phi - h||_H = \\inf_{\\Phi^* \\in N(\\sigma; A, \\infty)} ||\\Phi^* - h||_H\\) for all \\(h \\in H\\), where \\(H\\) is a normed function space. Under certain conditions, neural network spaces also do not have the best approximation property, and there exists a sequence in \\(N(\\sigma; A, \\infty)\\) that converges in \\(L^p([-1,1]^{d_0})\\) for every \\(p \\in (1, \\infty)\\), but the limit of this sequence is discontinuous, implying the limit does not lie in \\(N(\\sigma; A', \\infty)\\) for any \\(A'\\).",
        "The non-existence of best approximations can lead to the exploding weights phenomenon, where the weights of neural networks tend to infinity, particularly when learning functions that lack a best approximation within the neural network set. This can be undesirable for generalization bounds. In regression problems using neural networks with a fixed architecture, the goal is to produce a sequence of neural networks (\\(\\Phi_n\\)) that minimizes the risk, defined as the expected squared difference between the network's output and the target function. If the sequence of neural networks (\\(\\Phi_n\\)) converges in \\(L^2\\) to a function \\(f\\), but \\(f\\) does not belong to the neural network space \\(N(\\sigma; A, \\infty)\\), the weights of \\(\\Phi_n\\) will diverge."
      ]
    },
    {
      "topic": "Convexity and Related Properties of Neural Network Spaces",
      "sub_topics": [
        "Neural network spaces are often star-shaped, meaning they have at least one center point such that the line segment connecting the center to any point in the set remains within the set. The origin (zero function) is a center of a neural network space, as is every constant function. Scaling invariance in neural networks means that if a function \\(f\\) belongs to the network space \\(N(\\sigma; A, \\infty)\\), then any scaled version of that function (\\(\\lambda f\\)) also belongs to the same space. This implies that the zero function (0) is a center of the neural network space \\(N(\\sigma; A, \\infty)\\), which has implications for understanding the geometry and properties of the space.",
        "The number of linearly independent centers in a neural network space \\(N(\\sigma; A, \\infty)\\) is bounded by \\(n_A = \\sum_{l=0}^{L} d_l d_{l+1}\\), where the sum is taken over the layers of the network. If a neural network space \\(N(\\sigma; A, \\infty)\\) contains more than \\(n_A\\) linearly independent functions, then the space is not convex, which has implications for optimization and learning. This indicates that we cannot expect convex sets of neural networks if the set has many linearly independent elements.",
        "The set of neural networks is generally non-convex but star-shaped, with 0 and constant functions being centers. The non-convexity implies potential failures for optimization algorithms. The notion of \\(\\epsilon\\)-convexity is introduced, where a set is \\(\\epsilon\\)-convex if its convex hull is contained within an \\(\\epsilon\\)-neighborhood of the set. However, there is no \\(\\epsilon > 0\\) such that \\(N(\\sigma; A, \\infty)\\) is \\(\\epsilon\\)-convex. Furthermore, if \\(N(\\sigma; A, \\infty)\\) is \\(\\epsilon\\)-convex, then \\(N(\\sigma; A, \\infty)\\) is dense in \\(C(K)\\), where \\(K\\) is a compact set and \\(\\sigma\\) satisfies certain conditions, implying that sets of neural networks of fixed architectures have arbitrarily large holes."
      ]
    },
    {
      "topic": "Approximation Properties and Closedness",
      "sub_topics": [
        "A neural network space \\(N(\\sigma; A, \\infty)\\) may possess properties like best approximation, unique best approximation, or continuous selection, which determine how well functions can be approximated by neural networks within that space. The best approximation property ensures that for any function \\(h\\) in a normed function space \\(H\\), there exists a neural network \\(\\Phi\\) in \\(N(\\sigma; A, \\infty)\\) that minimizes the distance to \\(h\\). The unique best approximation property strengthens this by requiring that there exists exactly one such \\(\\Phi\\). The continuous selection property requires a continuous function \\(\\varphi\\) that maps functions \\(h\\) from \\(H\\) to neural networks \\(\\Phi\\) in \\(N(\\sigma; A, \\infty)\\) such that \\(\\Phi = \\varphi(h)\\) satisfies the best approximation condition for all \\(h\\) in \\(H\\).",
        "Neural network classes typically do not satisfy the continuous selection property or the best approximation property in \\(L^p\\) spaces, which affects the stability and convergence of learning algorithms. The absence of the best approximation property implies that the learning problem may require the weights of the neural networks to tend to infinity, which can be undesirable in applications. Every subset of \\(L^p([-1,1]^{d_0})\\) with the unique best approximation property is convex, and \\(N(\\sigma; A, \\infty)\\) is non-closed and does not have the continuous selection property.",
        "The exploding weights phenomenon discusses one of the consequences of the non-existence of best approximations. If one aims to learn a function \\(f\\) using neural networks with a fixed architecture \\(N(A; \\sigma, \\infty)\\) and the loss \\(L\\) is the squared loss, then under certain conditions, the weights of the neural networks must diverge. This implies that for functions lacking a best approximation within a neural network set, the weights of approximating neural networks must grow to infinity, which can be undesirable for generalization and parameter space bounds."
      ]
    }
  ]
}