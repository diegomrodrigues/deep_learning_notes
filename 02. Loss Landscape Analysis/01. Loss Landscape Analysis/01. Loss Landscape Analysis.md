## A Análise Detalhada da Superfície de Perda em Redes Neurais

### Introdução
No Capítulo 10, foi discutido como os pesos das redes neurais se adaptam durante o treinamento, utilizando variantes do gradiente descendente. Em alguns casos, incluindo as redes mais amplas consideradas no Capítulo 11, o esquema iterativo correspondente converge para um minimizador global. No entanto, isso nem sempre é garantido, e o gradiente descendente pode ficar preso em mínimos não globais ou pontos de sela. Para obter uma melhor compreensão dessas situações, este capítulo aborda a superfície de perda, que representa o gráfico do risco empírico em função dos pesos [^1].

### Conceitos Fundamentais

A **superfície de perda** é uma ferramenta crucial para entender os ajustes de peso durante o treinamento e os desafios de otimização em redes neurais [^1]. Matematicamente, a superfície de perda é representada pela função $\Lambda_{A,\sigma,S,L}$, que mapeia os parâmetros da rede neural para um número real, quantificando a perda associada a esses parâmetros. Essa função depende da arquitetura da rede ($A$), da função de ativação ($\sigma$), da amostra de treinamento ($S$) e da função de perda ($L$) [^2].

Formalmente, seguindo a Definição 12.2, dada uma arquitetura $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, uma função de ativação $\sigma: \mathbb{R} \rightarrow \mathbb{R}$, um conjunto de amostras $S = \\{(x_i, y_i)\\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R}^{d_{L+1}})^m$ e uma função de perda $L$, a superfície de perda é o gráfico da função $\Lambda_{A,\sigma,S,L}$ definida como [^2]:

$$\
\Lambda_{A,\sigma,S,L}: PN(A; \infty) \rightarrow \mathbb{R}
$$
$$\
\theta \mapsto L(R_S(R_O(\theta)))
$$

onde $R_S$ está definido em (1.2.3) e $R_O$ em (12.0.1) [^2].  Aqui, $PN(A, B)$ denota o conjunto de redes neurais com $L$ camadas, larguras de camada $d_0, d_1, ..., d_{L+1}$, todos os pesos limitados em módulo por $B$, e usando a função de ativação $\sigma$ [^2].

#### Visualização da Superfície de Perda

A visualização da superfície de perda oferece *insights* valiosos sobre os efeitos da profundidade, largura e funções de ativação da rede neural. No entanto, a visualização direta é limitada a superfícies bidimensionais, enquanto a superfície de perda é um objeto de alta dimensão [^3]. Para tornar a superfície de perda acessível, é necessário reduzir sua dimensionalidade, avaliando a função $\Lambda_{A,\sigma,S,L}$ em um subespaço bidimensional de $PN(A, \infty)$ [^3]. Isso é feito escolhendo três parâmetros $\mu, \theta_1, \theta_2$ e examinando a função [^3]:

$$\
\mathbb{R}^2 \ni (\alpha_1, \alpha_2) \mapsto \Lambda_{A,\sigma,S,L}(\mu + \alpha_1\theta_1 + \alpha_2\theta_2) \qquad (12.1.1)
$$

Existem várias escolhas naturais para $\mu, \theta_1, \theta_2$ [^3]:

*   **Direções Aleatórias:** $\theta_1, \theta_2$ são escolhidos aleatoriamente, enquanto $\mu$ é um mínimo de $\Lambda_{A,\sigma,S,L}$ ou escolhido aleatoriamente. Essa abordagem oferece uma visão rápida da rugosidade da superfície [^3].
*   **Componentes Principais da Trajetória de Aprendizagem:** $\mu, \theta_1, \theta_2$ são determinados para capturar a trajetória de aprendizagem. Se $\Theta^{(1)}, \Theta^{(2)}, ..., \Theta^{(N)}$ são os parâmetros resultantes do treinamento por SGD, $\mu, \theta_1, \theta_2$ minimizam a distância média quadrática para os $\Theta^{(j)}$.
*   **Baseado em Pontos Críticos:** $\mu, \theta_1, \theta_2$ são escolhidos para garantir a observação de múltiplos pontos críticos. Isso pode ser alcançado executando o procedimento de otimização três vezes com parâmetros finais $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$ e definindo $\mu = \Theta^{(1)}, \theta_1 = \Theta^{(2)} - \mu, \theta_2 = \Theta^{(3)} - \mu$ [^3].

#### Mínimos Espúrios

Do ponto de vista da otimização, a superfície de perda ideal tem um mínimo global no centro de um grande vale, de modo que o gradiente descendente converge para o mínimo, independentemente da inicialização [^4]. No entanto, essa situação não é realista para redes neurais profundas [^4]. Para uma rede neural rasa simples [^4]:

$$\
\mathbb{R}^d \ni x \mapsto \Phi(x) = W^{(1)}\sigma(W^{(0)}x + b^{(0)}) + b^{(1)}
$$

é claro que para cada matriz de permutação $P$ [^4]:

$$\
\Phi(x) = W^{(1)}P^T(PW^{(0)}x + Pb^{(0)}) + b^{(1)} \quad \text{para todo } x \in \mathbb{R}^d
$$

Em geral, existem múltiplas parametrizações que realizam a mesma função de saída. Além disso, se existir pelo menos um mínimo global com pesos não invariantes à permutação, então existem mais de um mínimo global na superfície de perda [^4].

Um conceito importante relacionado aos mínimos não globais são os **vales espúrios**. De acordo com a Definição 12.3, para $c \in \mathbb{R}$, o conjunto de subnível de $\Lambda_{A,\sigma,S,L}$ é definido como [^4]:

$$\
\Omega_\Lambda(c) := \\{\theta \in PN(A, \infty) \mid \Lambda_{A,\sigma,S,L}(\theta) \leq c\\}
$$

Um componente conectado por caminho de $\Omega_\Lambda(c)$ que não contém um mínimo global de $\Lambda_{A,\sigma,S,L}$ é chamado de vale espúrio [^4].

#### Pontos de Sela

Pontos de sela são pontos críticos da superfície de perda nos quais a perda diminui em uma direção. Em certo sentido, pontos de sela não são tão problemáticos quanto mínimos locais ou vales espúrios, se as atualizações na iteração de aprendizado tiverem alguma estocasticidade. Eventualmente, uma etapa aleatória na direção correta pode ser tomada e o ponto de sela pode ser evitado [^5].

O estudo dos pontos de sela na superfície de perda revelou que, sob certas suposições, os pontos críticos associados a uma grande perda são tipicamente pontos de sela, enquanto aqueles associados a uma pequena perda correspondem a mínimos [^5].

### Conclusão
A análise da superfície de perda é fundamental para compreender o comportamento das redes neurais durante o treinamento. A visualização da superfície de perda, embora desafiadora devido à sua alta dimensionalidade, fornece *insights* valiosos sobre a influência da arquitetura da rede e das funções de ativação. A existência de mínimos espúrios e pontos de sela apresenta desafios de otimização, mas a compreensão de suas características pode levar a estratégias de treinamento mais eficazes. Técnicas como visualização em subespaços bidimensionais, análise de componentes principais e estudo de pontos críticos contribuem para uma compreensão mais profunda da superfície de perda e, consequentemente, para o aprimoramento das técnicas de otimização em aprendizado profundo.

### Referências
[^1]: Chapter 12, Loss landscape analysis, page 165
[^2]: Definition 12.2, page 165
[^3]: Section 12.1, Visualization of loss landscapes, page 167
[^4]: Section 12.2, Spurious minima, page 167-168
[^5]: Section 12.3, Saddle points, page 170
<!-- END -->