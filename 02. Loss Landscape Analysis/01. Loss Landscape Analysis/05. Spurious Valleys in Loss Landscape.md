## Spurious Valleys na Análise do Loss Landscape

### Introdução
Em continuidade ao Capítulo 10 e à discussão sobre como os pesos das redes neurais são ajustados durante o treinamento [^1], e em contraste com os casos em que esquemas iterativos convergem para um minimizador global [^1], este capítulo explora a complexidade dos *loss landscapes* através da análise de *spurious valleys*. Como visto na Figura 12.1 [^2], o *loss landscape* pode conter regiões complexas, incluindo mínimos locais e *saddle points*. Este capítulo visa aprofundar a compreensão dessas estruturas, com foco específico nas *spurious valleys*, que representam um desafio significativo na otimização de redes neurais. A existência e as características dessas regiões dependem de fatores como a arquitetura da rede e a função de perda [^1].

### Conceitos Fundamentais
**Definição de Spurious Valleys:** Uma *spurious valley* é definida como um componente conexo por caminhos do *sub-level set* de um *loss landscape* que não contém um mínimo global [^4]. Formalmente, dado um conjunto de dados $S$ e uma função de perda $L$, o *sub-level set* $\Omega_A(c)$ é definido como:

$$\
\Omega_A(c) := \\{ \theta \in PN(A, \infty) \mid \Lambda_{A,\sigma,S,L}(\theta) \leq c \\}\
$$

onde $A$ representa a arquitetura da rede, $\sigma$ a função de ativação, e $\Lambda_{A,\sigma,S,L}$ o *loss landscape* [^4]. Uma *spurious valley* é então um componente conexo por caminhos de $\Omega_A(c)$ que não contém um mínimo global de $\Lambda_{A,\sigma,S,L}$ [^4].

**Implicações para a Otimização:** A presença de *spurious valleys* no *loss landscape* representa um problema para algoritmos de otimização, como o *gradient descent*, pois o processo de treinamento pode ficar preso nessas regiões subótimas [^1]. Ao contrário dos mínimos locais isolados, as *spurious valleys* são regiões extensas e conectadas, tornando mais difícil escapar delas.

**Inexistência em Redes Overparameterizadas:** A Proposição 12.4 [^4] estabelece que *spurious valleys* não existem em redes neurais rasas e *overparameterizadas*, ou seja, redes com um número de parâmetros na camada oculta maior ou igual ao número de amostras de treinamento. Formalmente, se $A = (d_0, d_1, 1) \in \mathbb{N}^3$ e $S = \\{(x_i, y_i)\\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R})^m$ com $m \leq d_1$, e $\sigma$ não é um polinômio e $L$ é uma função de perda convexa, então $\Lambda_{A,\sigma,S,L}$ não possui *spurious valleys* [^4].

**Prova da Inexistência:** A prova da Proposição 12.4 [^4] demonstra que, dados dois parâmetros $\theta_a$ e $\theta_b$ em $PN(A, \infty)$ com $\Lambda_{A,\sigma,S,L}(\theta_a) > \Lambda_{A,\sigma,S,L}(\theta_b)$, existe um parâmetro $\theta_c$ tal que $\Lambda_{A,\sigma,S,L}(\theta_b) = \Lambda_{A,\sigma,S,L}(\theta_c)$ e existe um caminho contínuo $\alpha: [0,1] \rightarrow PN(A, \infty)$ com $\alpha(0) = \theta_a$, $\alpha(1) = \theta_c$, e $\Lambda_{A,\sigma,S,L}(\alpha(t))$ monotonicamente decrescente [^4]. Escolhendo $\theta_a$ como um elemento de uma *spurious valley* e $\theta_b$ como um mínimo global, essa construção elimina a existência de *spurious valleys* [^4]. A construção do caminho $\alpha$ é feita em dois casos, dependendo do *rank* de uma matriz $V_a$ definida em termos das ativações da rede [^5].

**Exercício 12.7:** O Exercício 12.7 [^9] reforça essa ideia, pedindo para demonstrar que se existe um caminho contínuo $\alpha$ entre um parâmetro $\theta_1$ e um mínimo global $\theta_2$ tal que $\Lambda_{A,\sigma,S,L}(\alpha(t))$ é monotonicamente decrescente, então $\theta_1$ não pode ser um elemento de uma *spurious valley* [^9].

**Exercício 12.8:** O Exercício 12.8 [^9] sugere encontrar um exemplo de *spurious valley* para uma arquitetura simples, como uma rede neural ReLU de um único neurônio [^9]. A sugestão é observar que, para duas redes com inclinação positiva e negativa, qualquer caminho contínuo no espaço de parâmetros que conecta as duas deve passar por um parâmetro correspondente a uma função constante [^9].

### Conclusão
A análise de *spurious valleys* é crucial para entender os desafios na otimização de redes neurais. Embora a Proposição 12.4 [^4] mostre que elas não existem em redes rasas e *overparameterizadas* sob certas condições, a sua presença em outros cenários pode dificultar a convergência para um mínimo global. A visualização e a caracterização dessas regiões, juntamente com o desenvolvimento de algoritmos de otimização robustos, são áreas importantes de pesquisa para melhorar o treinamento de redes neurais.

### Referências
[^1]: Capítulo 12, página 165
[^2]: Capítulo 12, página 166
[^3]: Capítulo 12, página 167
[^4]: Capítulo 12, página 168
[^5]: Capítulo 12, página 169
[^6]: Capítulo 12, página 170
[^7]: Capítulo 12, página 171
[^8]: Capítulo 12, página 172
[^9]: Capítulo 12, página 173
[^10]: Capítulo 12, página 174
<!-- END -->