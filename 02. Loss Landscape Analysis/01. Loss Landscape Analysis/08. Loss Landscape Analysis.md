## Análise de Minima Locais, Globais e Pontos de Sela na Loss Landscape

### Introdução
A análise da *loss landscape* é crucial para entender o comportamento dos algoritmos de otimização no treinamento de redes neurais [^1]. O presente capítulo visa explorar as características da *loss landscape*, incluindo a probabilidade de encontrar mínimos locais versus globais, a nitidez e o volume desses mínimos, a dificuldade de escapar de mínimos locais, a profundidade dos mínimos locais em relação ao mínimo global e a rugosidade da superfície [^2]. Tais características dependem da arquitetura da rede e influenciam o processo de otimização. Como vimos anteriormente no Capítulo 10 [^1], os pesos das redes neurais são ajustados durante o treinamento usando variantes do gradiente descendente. Em alguns casos, como nas redes amplas consideradas no Capítulo 11 [^1], o esquema iterativo correspondente converge para um minimizador global. No entanto, isso nem sempre é garantido, e o gradiente descendente pode ficar preso em mínimos não globais ou pontos de sela [^1].

### Conceitos Fundamentais
Um dos principais desafios na otimização de redes neurais é a presença de **múltiplos mínimos locais** [^2]. A questão central é: qual a probabilidade de encontrar um mínimo local em vez de um mínimo global? [^2]. Essa probabilidade, juntamente com as propriedades dos mínimos locais, impacta diretamente a eficácia do treinamento.

**Minima Espúrios:**
Para formalizar a discussão sobre mínimos não globais, introduzimos o conceito de *spurious valleys*. Dada uma arquitetura de rede $A$, uma função de ativação $\sigma$, um conjunto de amostras $S$ e uma função de perda $L$, definimos o conjunto de subníveis de $\Lambda_{A,\sigma,S,L}$ como [^3]:
$$\
\Omega_c := \\{\theta \in PN(A, \infty) \mid \Lambda_{A,\sigma,S,L}(\theta) \leq c\\}\
$$
onde $PN(A, \infty)$ representa o espaço de parâmetros da rede [^3]. Uma componente conexa por caminho de $\Omega_c$ que não contém um mínimo global de $\Lambda_{A,\sigma,S,L}$ é chamada de *spurious valley* [^3].

**Volume e Nitidez dos Mínimos:**
Outra questão relevante é se os mínimos locais são tipicamente *sharp*, com pequeno volume, ou se fazem parte de *large flat valleys* difíceis de escapar [^2]. A nitidez de um mínimo está relacionada à curvatura da *loss landscape* em torno desse ponto. Mínimos *sharp* implicam em alta curvatura, enquanto *flat valleys* indicam baixa curvatura. O volume, por outro lado, refere-se ao tamanho da região em torno do mínimo onde a perda permanece baixa.

**Profundidade dos Mínimos Locais:**
A profundidade dos mínimos locais em relação ao mínimo global é crucial [^2]. Se os mínimos locais forem significativamente mais altos que o mínimo global, o algoritmo de otimização pode ficar preso em soluções subótimas [^2].

**Rugosidade da Superfície:**
A *loss landscape* pode ser *rough* ou *smooth*. Uma superfície *rough* possui muitas oscilações e irregularidades, o que dificulta a convergência para um mínimo global [^2]. A rugosidade da superfície depende da arquitetura da rede e da função de perda.

**Visualização da Loss Landscape:**
Visualizar a *loss landscape* pode fornecer *insights* valiosos sobre os efeitos da profundidade, largura e funções de ativação da rede neural [^3]. No entanto, a *loss landscape* é um objeto de alta dimensão, tornando a visualização direta desafiadora [^3]. Uma abordagem comum é reduzir a dimensionalidade, avaliando a função de perda em um subespaço bidimensional do espaço de parâmetros [^3]. Escolhemos três parâmetros $\mu, \theta_1, \theta_2$ e examinamos a função:
$$\
\mathbb{R}^2 \ni (\alpha_1, \alpha_2) \mapsto \Lambda_{A,\sigma,S,L}(\mu + \alpha_1\theta_1 + \alpha_2\theta_2) \qquad (12.1.1)\
$$
Existem várias escolhas naturais para $\mu, \theta_1, \theta_2$ [^3]:
*   **Direções aleatórias**: $\theta_1, \theta_2$ são escolhidos aleatoriamente, enquanto $\mu$ é um mínimo de $\Lambda_{A,\sigma,S,L}$ ou também escolhido aleatoriamente [^3].
*   **Componentes principais da trajetória de aprendizado**: $\mu, \theta_1, \theta_2$ são determinados para melhor capturar uma dada trajetória de aprendizado [^3].
*   **Baseado em pontos críticos**: $\mu, \theta_1, \theta_2$ são escolhidos para garantir a observação de múltiplos pontos críticos [^3].

**Spurious Minima:**
Do ponto de vista da otimização, a *loss landscape* ideal tem um mínimo global no centro de um grande vale, de modo que o gradiente descendente convirja para o mínimo, independentemente da inicialização escolhida [^3]. No entanto, esta situação não é realista para redes neurais profundas [^3].

**Proposição 12.4:** Seja $A = (d_0, d_1, 1) \in \mathbb{N}^3$ e seja $S = \\{(x_i, y_i)\\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R})^m$ uma amostra tal que $m \leq d_1$. Além disso, seja $\sigma \in \mathcal{M}$ não um polinômio, e seja $L$ uma função de perda convexa. Suponha ainda que $\Lambda_{A, \sigma, S, L}$ tenha pelo menos um mínimo global. Então, $\Lambda_{A, \sigma, S, L}$ não tem *spurious valleys* [^4].

**Prova:** Seja $\theta_a, \theta_b \in PN(A, \infty)$ com $\Lambda_{A, \sigma, S, L}(\theta_a) > \Lambda_{A, \sigma, S, L}(\theta_b)$. Então, mostraremos abaixo que existe outro parâmetro $\theta_c$ tal que [^4]:
* $\Lambda_{A, \sigma, S, L}(\theta_b) = \Lambda_{A, \sigma, S, L}(\theta_c)$
* Existe um caminho contínuo $\alpha : [0, 1] \rightarrow PN(A, \infty)$ tal que $\alpha(0) = \theta_a$, $\alpha(1) = \theta_c$, e $\Lambda_{A, \sigma, S, L}(\alpha(t))$ é monotonicamente decrescente $\blacksquare$ [^4].

**Pontos de Sela:**
Pontos de sela são pontos críticos da *loss landscape* nos quais a perda diminui em uma direção [^6]. Nesse sentido, os pontos de sela não são tão problemáticos quanto os mínimos locais ou os *spurious valleys* se as atualizações na iteração de aprendizado tiverem alguma estocasticidade [^6]. Eventualmente, um passo aleatório na direção certa pode ser dado e o ponto de sela pode ser escapado [^6].

**Proposição 12.5:** Seja $A = (d_0, d_1, 1)$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. Então, para todo $\theta \in PN(A, \infty)$ onde $R_S(\Phi_\theta)$ em (12.3.1) é duas vezes continuamente diferenciável com respeito aos pesos, vale que [^6]:
$$\
H(\theta) = H_0(\theta) + H_1(\theta)\
$$
onde $H(\theta)$ é o Hessiano de $R_S(\Phi_\theta)$ em $\theta$, $H_0(\theta)$ é uma matriz semidefinida positiva que é independente de $(y_i)_{i=1}^m$, e $H_1(\theta)$ é uma matriz simétrica que, para fixo $\theta$ e $(x_i)_{i=1}^m$, depende linearmente de $(e_i)_{i=1}^m$ [^7].

### Conclusão
A análise da *loss landscape* é um campo complexo e multifacetado. A probabilidade de encontrar mínimos locais versus globais, a nitidez e o volume desses mínimos, a dificuldade de escapar de mínimos locais, a profundidade dos mínimos locais em relação ao mínimo global e a rugosidade da superfície são todos fatores críticos que influenciam o processo de otimização [^2]. Embora fornecer respostas completas a essas questões seja difícil em geral, este capítulo forneceu algumas *insights* e resultados matemáticos para casos específicos [^2]. A compreensão das propriedades da *loss landscape* é essencial para o desenvolvimento de algoritmos de otimização mais eficazes e para a melhoria do desempenho das redes neurais [^2].

### Referências
[^1]: Capítulo 10 e 11 mencionados no texto.
[^2]: Pergunta de interesse sobre a *loss landscape* mencionada no texto.
[^3]: Definições e visualização da *loss landscape* mencionadas no texto.
[^4]: Proposição 12.4 e sua prova.
[^5]: Construção do caminho contínuo $\alpha$.
[^6]: Discussão sobre pontos de sela.
[^7]: Proposição 12.5 e sua descrição.
<!-- END -->