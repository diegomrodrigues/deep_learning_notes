## Métodos para Seleção de Parâmetros na Visualização de Loss Landscapes

### Introdução
A visualização de *loss landscapes* é crucial para entender o comportamento do treinamento de redes neurais [^1]. No entanto, a alta dimensionalidade desses *landscapes* exige técnicas de redução de dimensionalidade para torná-los visualizáveis [^1]. Especificamente, busca-se examinar a função $\Lambda_{A,\sigma,S,L}$ em um subespaço bidimensional de $PN(A, \infty)$ através da escolha de três parâmetros $\mu, \theta_1, \theta_2$ [^1]:

$$\
\mathbb{R}^2 \ni (\alpha_1, \alpha_2) \mapsto \Lambda_{A,\sigma,S,L}(\mu + \alpha_1\theta_1 + \alpha_2\theta_2)
$$\

Este capítulo explora diferentes métodos para selecionar esses parâmetros, com foco em suas vantagens e desvantagens [^1].

### Conceitos Fundamentais

#### Direções Aleatórias
Um método simples consiste em escolher $\theta_1$ e $\theta_2$ aleatoriamente, enquanto $\mu$ pode ser um mínimo de $\Lambda_{A,\sigma,S,L}$ ou também escolhido aleatoriamente [^1]. Este método oferece uma visão rápida da rugosidade da superfície [^1]. No entanto, conforme apontado em [132], direções aleatórias tendem a ser ortogonais à trajetória de otimização, o que significa que podem perder características relevantes do *loss landscape* [^1].

#### Componentes Principais da Trajetória de Aprendizagem
Para superar as limitações das direções aleatórias, pode-se determinar $\mu, \theta_1, \theta_2$ que melhor capturem uma dada trajetória de aprendizagem [^1]. Por exemplo, se $\Theta^{(1)}, \Theta^{(2)}, ..., \Theta^{(N)}$ são os parâmetros resultantes do treinamento por SGD, podemos determinar $\mu, \theta_1, \theta_2$ de forma que o hiperplano $\\{\mu + \alpha_1\theta_1 + \alpha_2\theta_2 | \alpha_1, \alpha_2 \in \mathbb{R}\\}$ minimize a distância quadrática média aos $\Theta^{(j)}$ para $j \in \\{1, ..., N\\}$ [^1]. Este é o método utilizado em [132] e pode ser alcançado por meio de uma Análise de Componentes Principais (PCA) [^1]. A PCA alinha a visualização com o processo de otimização, destacando as características relevantes do *loss landscape* [^1].

#### Baseado em Pontos Críticos
Para uma perspectiva mais global, $\mu, \theta_1, \theta_2$ podem ser escolhidos para garantir a observação de múltiplos pontos críticos [^1]. Uma forma de alcançar isso é executar o procedimento de otimização três vezes, obtendo os parâmetros finais $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$ [^1]. Se os procedimentos convergiram, então cada um desses parâmetros estará próximo de um ponto crítico de $\Lambda_{A,\sigma,S,L}$ [^1]. Podemos então definir $\mu = \Theta^{(1)}, \theta_1 = \Theta^{(2)} - \mu, \theta_2 = \Theta^{(3)} - \mu$ [^1]. Isso garante que a função em (12.1.1) passe por, ou pelo menos chegue perto de, três pontos críticos (em $(\alpha_1, \alpha_2) = (0,0), (1,0), (0,1)$) [^1].

### Conclusão
A escolha do método de seleção de parâmetros para visualização de *loss landscapes* depende do objetivo da análise [^1]. Direções aleatórias oferecem uma visão geral rápida, enquanto a PCA se concentra nas características relevantes para a otimização [^1]. A abordagem baseada em pontos críticos permite a visualização de múltiplos pontos críticos, fornecendo uma perspectiva mais global do *loss landscape* [^1]. Em [132], o uso de PCA é apresentado como uma forma de mitigar as limitações das direções aleatórias, garantindo que a visualização esteja alinhada com a trajetória de otimização.

### Referências
[^1]: Capítulo 12 do texto fornecido.
<!-- END -->