## Decomposição do Hessiano e a Prevalência de Pontos de Sela

### Introdução
Em capítulos anteriores, foi discutido como os pesos das redes neurais são ajustados durante o treinamento, geralmente usando variantes do método do gradiente descendente. Contudo, o gradiente descendente pode ficar preso em mínimos locais não globais ou pontos de sela, especialmente em redes complexas. Para aprofundar a compreensão dessas situações, este capítulo explora a análise da *loss landscape*, que representa o gráfico do risco empírico em função dos pesos [^1].

### Conceitos Fundamentais
A análise do *loss landscape* é crucial para entender o comportamento dos algoritmos de otimização em redes neurais. Um aspecto fundamental dessa análise é a decomposição do Hessiano da função de perda, que fornece informações valiosas sobre a curvatura do espaço de perda e a presença de pontos de sela.

**Decomposição do Hessiano**

O Hessiano da função de perda, denotado por \\( H(\theta) \\), pode ser decomposto em duas matrizes: uma matriz semidefinida positiva \\( H_0(\theta) \\) e uma matriz simétrica \\( H_1(\theta) \\) [^1]. Matematicamente, temos:

$$ H(\theta) = H_0(\theta) + H_1(\theta) $$

Onde:
*   \\( H(\theta) \\) é o Hessiano da função de perda \\( R_S(\Phi_\theta) \\) em \\( \theta \\).
*   \\( H_0(\theta) \\) é uma matriz semidefinida positiva independente dos erros \\( (y_i)_1^m \\).
*   \\( H_1(\theta) \\) é uma matriz simétrica que depende linearmente dos erros \\( (e_i)_1^m \\), onde \\( e_i = \Phi_\theta(x_i) - y_i \\) [^6].

**Justificativa da Decomposição**

A decomposição do Hessiano permite analisar a contribuição de diferentes fatores para a curvatura do *loss landscape*. A matriz \\( H_0(\theta) \\) representa a curvatura intrínseca da rede neural, independente dos dados de treinamento. Já a matriz \\( H_1(\theta) \\) captura a influência dos erros nos dados de treinamento sobre a curvatura.

**Prova da Decomposição**

Para provar a decomposição, considere \\( \theta \\) como um vetor em \\( \mathbb{R}^{n_A} \\), conforme a identificação introduzida após a Definição 12.2 [^6, 1]. Para \\( k = 1, ..., n_A \\), temos:

$$ \frac{\partial R_S(\Phi_\theta)}{\partial \theta_k} = \frac{2}{m} \sum_{i=1}^m e_i \frac{\partial \Phi_\theta(x_i)}{\partial \theta_k} $$

Portanto, para \\( j = 1, ..., n_A \\), pela regra de Leibniz, temos:

$$ \frac{\partial^2 R_S(\Phi_\theta)}{\partial \theta_j \partial \theta_k} = \frac{2}{m} \sum_{i=1}^m \frac{\partial \Phi_\theta(x_i)}{\partial \theta_j} \frac{\partial \Phi_\theta(x_i)}{\partial \theta_k} + \frac{2}{m} \sum_{i=1}^m e_i \frac{\partial^2 \Phi_\theta(x_i)}{\partial \theta_j \partial \theta_k} $$

Definindo:

$$ H_0(\theta) = \frac{2}{m} \sum_{i=1}^m \frac{\partial \Phi_\theta(x_i)}{\partial \theta_j} \frac{\partial \Phi_\theta(x_i)}{\partial \theta_k} $$

$$ H_1(\theta) = \frac{2}{m} \sum_{i=1}^m e_i \frac{\partial^2 \Phi_\theta(x_i)}{\partial \theta_j \partial \theta_k} $$

Temos \\( H(\theta) = H_0(\theta) + H_1(\theta) \\) [^7].

Para mostrar que \\( H_0(\theta) \\) é semidefinida positiva, defina:

$$ J_{i,\theta} = \begin{bmatrix} \frac{\partial \Phi_\theta(x_i)}{\partial \theta_1} \\\\ \vdots \\\\ \frac{\partial \Phi_\theta(x_i)}{\partial \theta_{n_A}} \end{bmatrix} \in \mathbb{R}^{n_A} $$

Assim, \\( H_0(\theta) = \frac{2}{m} \sum_{i=1}^m J_{i,\theta} J_{i,\theta}^T \\), que é uma soma de matrizes semidefinidas positivas. A simetria de \\( H_1(\theta) \\) segue da simetria das segundas derivadas, e a linearidade de \\( H_1(\theta) \\) em \\( (e_i)_1^m \\) é clara [^7]. $\blacksquare$

**Implicações para a Prevalência de Pontos de Sela**

A decomposição do Hessiano oferece *insights* sobre a prevalência de pontos de sela em relação ao tamanho da perda. Se \\( \theta \\) corresponde a um ponto crítico, e \\( H(\theta) \\) tem pelo menos um autovalor negativo, então \\( \theta \\) não pode ser um mínimo, mas sim um ponto de sela ou um máximo [^7].

Em modelos onde o número de parâmetros \\( n_A \\) é grande, é razoável assumir que \\( H_1(\theta) \\) tenha um autovalor negativo. Considere um modelo simplificado onde fixamos um parâmetro \\( \theta \\) e temos um conjunto de dados \\( S_0 = (x_i, y_i)_1^m \\) com erros associados \\( (e_i)_1^m \\). Seja \\( H_0^0(\theta) \\), \\( H_1^0(\theta) \\) as matrizes correspondentes de acordo com a Proposição 12.5 [^7, 6].

Agora, considere um novo conjunto de dados \\( S_\lambda = (x_i, y_i)_1^m \\) tal que os erros associados são \\( (e_i)_\lambda = \lambda (e_i)_1^m \\), onde \\( \lambda > 0 \\). O Hessiano de \\( R_S(\Phi_\theta) \\) em \\( \theta \\) é então \\( H^\lambda(\theta) = H_0^0(\theta) + \lambda H_1^0(\theta) \\) [^7].

Se \\( \lambda \\) é grande, então \\( H^\lambda(\theta) \\) é uma perturbação de uma versão amplificada de \\( H_1^0(\theta) \\). Se \\( v \\) é um autovetor de \\( H_1^0(\theta) \\) com autovalor negativo \\( -\mu \\), então:

$$ v^T H^\lambda(\theta) v \leq (||H_0^0(\theta)|| - \lambda \mu) ||v||^2 $$

Para \\( \lambda \\) suficientemente grande, o lado direito da desigualdade é negativo, indicando que \\( H^\lambda(\theta) \\) tem um autovalor negativo [^7].

### Conclusão
A decomposição do Hessiano em uma componente independente dos erros e outra linearmente dependente dos erros fornece uma ferramenta poderosa para analisar a geometria do *loss landscape*. Essa decomposição permite relacionar a magnitude da perda com a prevalência de pontos de sela, sugerindo que pontos críticos associados a perdas maiores têm maior probabilidade de serem pontos de sela. Em contrapartida, pontos críticos com perdas pequenas tendem a ser mínimos locais [^7]. Essa análise, embora baseada em algumas simplificações, oferece *insights* valiosos sobre os desafios da otimização em redes neurais profundas.

### Referências
[^1]: Chapter 12, Loss landscape analysis, page 165.
[^6]: Chapter 12, Saddle points, page 170.
[^7]: Chapter 12, Saddle points, page 171.
<!-- END -->