## Saddle Points in Loss Landscapes

### Introdução
Como discutido anteriormente, o treinamento de redes neurais envolve a adaptação dos pesos por meio de variantes do gradiente descendente [^1]. Embora em certos casos, como em redes amplas, a convergência para um minimizador global seja garantida, em geral, o gradiente descendente pode ficar preso em mínimos não globais ou *saddle points* [^1]. Para compreender melhor essas situações, este capítulo foca na análise da *loss landscape*, que representa o gráfico do risco empírico em função dos pesos [^1]. Esta seção aprofunda a discussão sobre *saddle points*, explorando suas características e o impacto na otimização.

### Conceitos Fundamentais

**Saddle points** são pontos críticos na *loss landscape* onde a perda diminui em uma direção e aumenta em outra [^6]. Eles representam um desafio para os algoritmos de otimização, pois podem retardar ou interromper o processo de treinamento [^6]. Ao contrário dos mínimos locais, onde o gradiente aponta para cima em todas as direções, nos *saddle points* existe pelo menos uma direção na qual o gradiente aponta para baixo, permitindo, em teoria, a fuga do ponto.

Apesar do potencial de escape, a presença de *saddle points* pode dificultar significativamente a otimização. O gradiente descendente, por exemplo, pode se aproximar de um *saddle point* e oscilar em torno dele, sem conseguir encontrar a direção de descida mais íngreme [^1]. Isso pode levar a um treinamento lento ou, em casos extremos, à estagnação do processo.

No entanto, é importante notar que os *saddle points* não são tão problemáticos quanto os mínimos locais ou *spurious valleys*, especialmente se as atualizações no processo de treinamento tiverem alguma estocasticidade [^6]. Eventualmente, um passo aleatório na direção certa pode permitir escapar do *saddle point* [^6].

Em um *loss landscape* onde a maioria dos pontos críticos são *saddle points*, ainda existe uma boa chance de eventualmente alcançar o mínimo global [^6]. Isso ocorre porque, embora a otimização seja desafiadora, a presença de *saddle points* indica que existem direções nas quais a perda pode ser reduzida.

Um estudo sobre *saddle points* na *loss landscape* foi realizado em [44, 170] [^6]. A principal observação em [170] é que, sob algumas suposições fortes, os pontos críticos na *loss landscape* associados a uma grande perda são tipicamente *saddle points*, enquanto aqueles associados a uma pequena perda correspondem a mínimos [^6]. Essa situação é encorajadora para as perspectivas de otimização em *deep learning*, pois, mesmo que se fique preso em um mínimo local, é muito provável que a perda esteja próxima do ótimo [^6].

Os resultados de [170] utilizam teoria de matrizes aleatórias [^6]. Embora não seja possível garantir que as suposições feitas sejam satisfeitas para um problema específico, a ideia principal fornece alguma intuição para apoiar a afirmação acima [^6].

### Conclusão

A análise da *loss landscape* é crucial para entender os desafios e oportunidades no treinamento de redes neurais. Os *saddle points*, embora representem um obstáculo, não são inerentemente tão problemáticos quanto os mínimos locais. A estocasticidade nos algoritmos de otimização e a tendência de *saddle points* estarem associados a perdas maiores sugerem que é possível escapar desses pontos e eventualmente alcançar o mínimo global.

### Referências
[^1]: Capítulo 12, p. 165
[^6]: Capítulo 12, p. 170

<!-- END -->