## Ausência de Mínimos Espúrios em Redes Neurais Rasas e Sobreparametrizadas

### Introdução
O Capítulo 12 discute a análise da *loss landscape* e, em particular, a ocorrência de mínimos locais e globais [^1]. Como vimos anteriormente, a convergência para um minimizador global não é garantida em geral, e o gradiente descendente pode ficar preso em mínimos não-globais ou pontos de sela [^1]. Este capítulo explora o conceito de *spurious minima* e as condições sob as quais eles podem ser evitados. Em particular, este capítulo investiga a ausência de *spurious minima* em redes neurais rasas e sobreparametrizadas [^3].

### Conceitos Fundamentais
**Definição de Spurious Valley**
Um *spurious valley* é definido como um componente conexo por caminho do subconjunto de nível Ωc(c) da *loss landscape* que não contém um mínimo global [^3]. Formalmente, seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$ a arquitetura da rede, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ a função de ativação, $m \in \mathbb{N}$ o número de amostras, $S = \\{(x_i, y_i)\\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R}^{d_{L+1}})^m$ um conjunto de amostras, e $L$ uma função de perda. Para $c \in \mathbb{R}$, o subconjunto de nível de $\Lambda_{A, \sigma, S, L}$ é definido como:
$$\
\Omega_L(c) := \\{\theta \in PN(A, \infty) \mid \Lambda_{A, \sigma, S, L}(\theta) \leq c\\}\
$$
Um componente conexo por caminho de $\Omega_L(c)$ que não contém um mínimo global de $\Lambda_{A, \sigma, S, L}$ é chamado de *spurious valley* [^3].

**Condições para a Ausência de Spurious Minima**
A Proposição 12.4 estabelece que *spurious local minima* não existem para redes neurais rasas e sobreparametrizadas [^3]. Uma rede neural é considerada sobreparametrizada quando o número de parâmetros na camada escondida é pelo menos tão grande quanto o número de amostras de treinamento [^3].

**Proposição 12.4**
Seja $A = (d_0, d_1, 1) \in \mathbb{N}^3$ e $S = \\{(x_i, y_i)\\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R})^m$ um conjunto de amostras tal que $m \leq d_1$. Além disso, seja $\sigma \in M$ (onde $M$ é um conjunto de funções de ativação) não polinomial e $L$ uma função de perda convexa [^3]. Se $\Lambda_{A, \sigma, S, L}$ possui pelo menos um mínimo global, então $\Lambda_{A, \sigma, S, L}$ não possui *spurious valleys* [^3].

**Prova da Proposição 12.4**
A prova da Proposição 12.4 envolve demonstrar que, dados dois parâmetros $\theta_a, \theta_b \in PN(A, \infty)$ com $\Lambda_{A, \sigma, S, L}(\theta_a) > \Lambda_{A, \sigma, S, L}(\theta_b)$, existe outro parâmetro $\theta_c$ tal que:
1.  $\Lambda_{A, \sigma, S, L}(\theta_b) = \Lambda_{A, \sigma, S, L}(\theta_c)$
2.  Existe um caminho contínuo $\alpha: [0, 1] \rightarrow PN(A, \infty)$ tal que $\alpha(0) = \theta_a$, $\alpha(1) = \theta_c$, e $\Lambda_{A, \sigma, S, L}(\alpha(t))$ é monotonicamente decrescente [^3].

A construção desse caminho contínuo exclui a existência de *spurious valleys*, escolhendo $\theta_a$ como um elemento de um *spurious valley* e $\theta_b$ como um mínimo global [^3]. A prova é dividida em dois casos, dependendo do posto de uma matriz $V_a$ construída a partir das ativações da rede [^3].

**Caso 1:** $V_a$ tem posto $m$. Neste caso, existe uma matriz $W$ tal que $WV_a = (\Lambda_{R_\theta(b)}(\theta_b)(x_i) - b_i)_{i=1}^m$ [^5]. Um caminho $\alpha(t)$ é definido como uma combinação linear entre os pesos da primeira camada, garantindo que $\Lambda_{A, \sigma, S, L}(\alpha(t))$ seja monotonicamente decrescente devido à convexidade da função de perda [^5].

**Caso 2:** $V_a$ tem posto menor que $m$. Neste caso, é mostrado que existe um caminho contínuo de $\theta_a$ para outro parâmetro de rede neural com posto maior. Isso é feito expressando um dos vetores de ativação como uma combinação linear dos outros e construindo um novo caminho que aumenta o posto da matriz $V_a$ [^5].

Por meio da iteração dessa construção, é possível encontrar um caminho para um parâmetro de rede neural onde a matriz associada tem posto completo, reduzindo o problema ao Caso 1 [^5]. $\blacksquare$

### Conclusão
A Proposição 12.4 e sua prova demonstram que, sob certas condições (principalmente sobreparametrização e convexidade da função de perda), redes neurais rasas não sofrem do problema de *spurious minima* [^3]. Este resultado sugere que aumentar a capacidade do modelo pode mitigar o problema de convergência subótima [^3]. A análise da *loss landscape* e a identificação de condições que garantem a ausência de *spurious minima* são cruciais para o desenvolvimento de algoritmos de treinamento mais eficientes e robustos [^1]. Os resultados apresentados neste capítulo fornecem *insights* valiosos sobre o comportamento de redes neurais em diferentes regimes de parametrização e podem guiar o projeto de arquiteturas de redes neurais mais adequadas para problemas específicos.

### Referências
[^1]: Capítulo 12, p. 165.
[^3]: Capítulo 12, p. 168.
[^5]: Capítulo 12, p. 169.
<!-- END -->