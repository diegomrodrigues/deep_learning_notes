## O Cenário de Perda Ideal e a Realidade das Redes Neurais Profundas

### Introdução
Como vimos anteriormente, o treinamento de redes neurais envolve a adaptação dos pesos através de algoritmos como o gradiente descendente. O sucesso desse processo depende crucialmente da **estrutura do cenário de perda**, que representa o risco empírico em função dos pesos da rede [^1]. Este capítulo explora a idealização de um cenário de perda e como ele se distancia da realidade em redes neurais profundas, focando especificamente no fenômeno dos **mínimos espúrios**.

### Conceitos Fundamentais

Do ponto de vista da otimização, o **cenário de perda ideal** apresenta um único **mínimo global** situado no centro de um amplo vale [^3]. Essa configuração facilita a convergência através do gradiente descendente, independentemente da inicialização. Em outras palavras, qualquer que seja o ponto de partida na paisagem, o gradiente descendente levaria ao mínimo global.

No entanto, essa situação ideal raramente se manifesta em redes neurais profundas [^3]. A complexidade dessas redes introduz múltiplos **mínimos locais** e **pontos de sela**, dificultando a convergência para o mínimo global. Para ilustrar essa complexidade, a Definição 12.2 [^1] formaliza a noção de **cenário de perda** para uma arquitetura de rede neural específica, um conjunto de dados e uma função de perda. O cenário de perda é definido como o gráfico da função $\Lambda_{A,\sigma,S,L}$ que mapeia os pesos da rede para o valor da função de perda.

$$\
\Lambda_{A,\sigma,S,L} : PN(A; \infty) \rightarrow \mathbb{R} \\\\\
\theta \mapsto R_S(R_O(\theta))
$$

Onde $A$ representa a arquitetura da rede, $\sigma$ a função de ativação, $S$ o conjunto de dados e $L$ a função de perda. $PN(A; \infty)$ denota o espaço de todos os pesos possíveis para a arquitetura $A$, e $R_O$ é o mapa de realização que associa os pesos à rede neural correspondente.

A existência de múltiplos **mínimos locais** é um problema significativo, pois o gradiente descendente pode ficar preso em um desses mínimos, impedindo a rede de alcançar o desempenho ótimo. A seção 12.2 [^3] introduz o conceito de **mínimos espúrios**, que são mínimos locais que não correspondem a soluções globais. A Definição 12.3 [^4] formaliza essa noção:

**Definição 12.3.** Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. Seja $m \in \mathbb{N}$, e $S = \\{(x_i, y_i)\\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R}^{d_{L+1}})^m$ seja uma amostra e seja $L$ uma função de perda. Para $c \in \mathbb{R}$, definimos o subconjunto de nível de $\Lambda_{A, \sigma, S, L}$ como

$$\
\Omega_A(c) := \\{\theta \in PN(A; \infty) | \Lambda_{A, \sigma, S, L}(\theta) \leq c \\}
$$

Um componente conectado por caminho de $\Omega_A(c)$, que não contém um mínimo global de $\Lambda_{A, \sigma, S, L}$ é chamado de vale espúrio.

A **Proposição 12.4** [^4] fornece um resultado interessante: para redes neurais rasas e superparametrizadas (ou seja, com mais parâmetros na camada oculta do que amostras de treinamento), sob certas condições (função de ativação não polinomial e função de perda convexa), não existem mínimos espúrios. No entanto, essa garantia não se estende a redes profundas.

$$\
\Phi(x) = W^{(1)} P^T (P W^{(0)} x + P b^{(0)}) + b^{(1)} \text{ para todo } x \in \mathbb{R}^d.
$$

Em geral, existem múltiplas parametrizações que realizam a mesma função de saída [^4]. Além disso, se existir pelo menos um mínimo global com pesos não invariantes por permutação, então existem mais de um mínimo global no cenário de perda. A existência de múltiplos mínimos globais não é necessariamente um problema; o verdadeiro desafio reside na presença de mínimos não globais (espúrios).

### Conclusão

A análise do cenário de perda é fundamental para entender os desafios da otimização em redes neurais profundas. A disparidade entre o cenário idealizado e a realidade complexa das redes profundas destaca a necessidade de desenvolver técnicas de otimização mais robustas. A compreensão dos mínimos espúrios, como formalizado pelas definições e proposições apresentadas, é um passo crucial nessa direção. A visualização e a caracterização do cenário de perda, conforme abordado nas seções anteriores do capítulo, fornecem insights valiosos para o desenvolvimento de algoritmos de treinamento mais eficazes e para a concepção de arquiteturas de redes neurais mais fáceis de otimizar.

### Referências
[^1]: Capítulo 12, Loss landscape analysis, p. 165
[^2]: Capítulo 12, Loss landscape analysis, p. 166
[^3]: Capítulo 12, Spurious minima, p. 167
[^4]: Capítulo 12, Spurious minima, p. 168
[^5]: Capítulo 12, Spurious minima, p. 169
[^6]: Capítulo 12, Saddle points, p. 170
[^7]: Capítulo 12, Saddle points, p. 171
[^8]: Capítulo 12, p. 172
[^9]: Capítulo 12, p. 173
[^10]: Capítulo 12, p. 174
<!-- END -->