## 12.2 Spurious Minima: Parameterization and Optimization Challenges

### Introdução
No contexto da otimização de redes neurais, o cenário ideal seria um *loss landscape* com um único mínimo global situado no centro de um vale amplo, permitindo que o *gradient descent* converja para esse mínimo independentemente da inicialização [^1]. No entanto, essa situação é irrealista para redes neurais profundas [^3]. Um dos desafios reside na existência de múltiplos mínimos, tanto globais quanto locais, que podem dificultar o processo de otimização. Este capítulo se aprofunda no conceito de *spurious minima*, analisando como a existência de múltiplas parametrizações e a presença de mínimos não globais afetam a otimização.

### Conceitos Fundamentais
A complexidade do *loss landscape* surge, em parte, da existência de múltiplas parametrizações que realizam a mesma função de saída [^4]. Considerando uma rede neural simples e rasa definida por:

$$\
\Phi(x) = W^{(1)} \sigma(W^{(0)}x + b^{(0)}) + b^{(1)}
$$

É evidente que, para cada matriz de permutação $P$, a função de saída permanece inalterada:

$$\
\Phi(x) = W^{(1)} P^T(PW^{(0)}x + Pb^{(0)}) + b^{(1)} \quad \forall x \in \mathbb{R}^d
$$

A existência de múltiplos mínimos globais, embora não seja inerentemente problemática, torna-se relevante quando coexiste com mínimos não globais [^4]. O verdadeiro desafio reside na presença desses *non-global minima*, ou *spurious valleys*, que podem aprisionar o algoritmo de otimização, impedindo-o de alcançar o mínimo global [^1].

Para formalizar essa noção, generalizamos o conceito de mínimos não globais para *spurious valleys* [^4]:

**Definição 12.3:** Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. Seja $m \in \mathbb{N}$, e $S = \{(x_i, y_i)\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R}^{d_{L+1}})^m$ uma amostra e $L$ uma função de perda. Para $c \in \mathbb{R}$, definimos o subconjunto de nível de $\Lambda_{A, \sigma, S, L}$ como:

$$\
\Omega_A(c) := \{\theta \in PN(A, \infty) \mid \Lambda_{A, \sigma, S, L}(\theta) \leq c\}
$$

Um componente conectado por caminho de $\Omega_A(c)$, que não contém um mínimo global de $\Lambda_{A, \sigma, S, L}$, é chamado de *spurious valley* [^4].

A Proposição 12.4, a seguir, demonstra que *spurious local minima* não existem para redes neurais rasas superparametrizadas, ou seja, para redes neurais que têm pelo menos tantos parâmetros na camada oculta quanto amostras de treinamento [^4].

**Proposição 12.4:** Seja $A = (d_0, d_1, 1) \in \mathbb{N}^3$ e seja $S = \{(x_i, y_i)\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R})^m$ uma amostra tal que $m \leq d_1$. Além disso, seja $\sigma \in \mathcal{M}$ não um polinômio, e seja $L$ uma função de perda convexa. Suponha ainda que $\Lambda_{A, \sigma, S, L}$ tenha pelo menos um mínimo global. Então, $\Lambda_{A, \sigma, S, L}$ não tem *spurious valleys* [^4].

*Prova:* Sejam $\theta_a, \theta_b \in PN(A, \infty)$ com $\Lambda_{A, \sigma, S, L}(\theta_a) > \Lambda_{A, \sigma, S, L}(\theta_b)$. Mostraremos que existe outro parâmetro $\theta_c$ tal que:

*   $\Lambda_{A, \sigma, S, L}(\theta_b) = \Lambda_{A, \sigma, S, L}(\theta_c)$
*   Existe um caminho contínuo $\alpha: [0, 1] \rightarrow PN(A, \infty)$ tal que $\alpha(0) = \theta_a$, $\alpha(1) = \theta_c$, e $\Lambda_{A, \sigma, S, L}(\alpha(t))$ é monotonicamente decrescente [^4].

Pelo Exercício 12.7, a construção acima exclui a existência de *spurious valleys* escolhendo $\theta_a$ como um elemento de um *spurious valley* e $\theta_b$ um mínimo global [^4].

A construção é a seguinte: Denotemos $\theta = \{(W^{(l)}, b^{(l)})\}_{l=0}^{1}$ para $\theta \in \{a, b, c\}$ [^4].

Para $j = 1, ..., d_1$, introduzimos $v_a^j \in \mathbb{R}^m$ definido como:

$$\
(v_a^j)_i = \sigma((W_a^{(0)})^j x_i + b_a^{(0)}), \quad \text{para } i = 1, ..., m
$$

Se definirmos $V_a = \{(v_a^j)\}_{j=1}^{d_1}$, então:

$$\
W_a^{(1)} V_a = (R_{\theta_a}(x_i) - b_a^{(1)})_{i=1}^m
$$

Distinguiremos agora entre dois casos. Para o primeiro, o resultado é trivial e o segundo pode ser transformado no primeiro [^5].

*   **Caso 1:** Assumimos que $V_a$ tem rank $m$. Nesse caso, é óbvio a partir de (12.2.1), que existe $W$ tal que:

    $$\
    W V_a = (R_{\theta_b}(x_i) - b_b^{(1)})_{i=1}^m
    $$

    Podemos então definir $\alpha(t) = \{(W_a^{(0)}, b_a^{(0)}), ((1-t)W_a^{(1)} + tW, b_b^{(1)})\}$ [^5]. Note que, por construção, $\alpha(0) = \theta_a$ e $\Lambda_{A, \sigma, S, L}(\alpha(1)) = \Lambda_{A, \sigma, S, L}(\theta_b)$. Além disso, $t \rightarrow (R_{\alpha(t)}(x_i))_1$ descreve um caminho reto em $\mathbb{R}^m$ e, portanto, pela convexidade de $L$, fica claro que $t \rightarrow \Lambda_{A, \sigma, S, L}(\alpha(t))$ é monotonicamente decrescente [^5].
*   **Caso 2:** Assumimos que $V_a$ tem rank menor que $m$. Nesse caso, mostramos que encontramos um caminho contínuo de $\theta_a$ para outro parâmetro de rede neural com rank mais alto. O caminho será tal que $\Lambda_{A, \sigma, S, L}$ é monotonicamente decrescente [^5].

    Sob as suposições, temos que um $v_a^j$ pode ser escrito como uma combinação linear dos $v_a^i$ restantes, $i \neq j$. Sem perda de generalidade, assumimos $j = 1$. Então, existem $(a_i)_{i=2}^m$ tais que:

    $$\
    v_a^1 = \sum_{i=2}^m a_i v_a^i
    $$

    Observamos que aqui existe $v^* \in \mathbb{R}^m$ que é linearmente independente de todos os $(v_a^i)$, e pode ser escrito como $(v^*)_i = \sigma((w^*) x_i + b^*)$ para algum $w^* \in \mathbb{R}^{d_0}, b^* \in \mathbb{R}$ [^5]. De fato, se assumirmos que tal $v^*$ não existe, então segue que $\text{span}\{(\sigma(w x_i + b))_i \mid w \in \mathbb{R}^{d_0}, b \in \mathbb{R}\}$ é um subespaço de dimensão $m - 1$ de $\mathbb{R}^m$, o que leva a uma contradição com o Teorema 9.3 [^5].

    Agora, definimos dois caminhos: Primeiro,

    $$\
    \alpha_1(t) = \{(W_a^{(0)}, b_a^{(0)}), (W^{(1)}(t), b_a^{(1)})\}, \quad \text{para } t \in [0, 1/2]
    $$

    onde

    $$\
    (W^{(1)}(t))_1 = (1 - 2t)(W_a^{(1)})_1 \quad \text{e} \quad (W^{(1)}(t))_i = (W_a^{(1)})_i + 2t a_i (W_a^{(1)})_1
    $$

    para $i = 2, ..., d_1$, para $t \in [0, 1/2]$. Segundo,

    $$\
    \alpha_2(t) = \{(W^{(0)}(t), b_a^{(0)}), (W^{(1)}(1/2), b_a^{(1)})\}, \quad \text{para } t \in (1/2, 1]
    $$

    onde

    $$\
    (W^{(0)}(t))_1 = 2(t - 1/2)(W_a^{(0)})_1 + (2t - 1)w^* \quad \text{e} \quad (W^{(0)}(t))_i = (W_a^{(0)})_i
    $$

    para $i = 2, ..., d_1$, $(b^{(0)}(t))_1 = 2(t - 1/2)(b_a^{(0)})_1 + (2t - 1)b^*$, e $(b^{(0)}(t))_i = (b_a^{(0)})_i$ para $i = 2, ..., d_1$ [^6]. Fica claro por (12.2.2) que $(R_{\alpha_1}(x_i))_{i=1}^m$ é constante. Além disso, $R_{\alpha_2}(x)$ é constante para todo $x \in \mathbb{R}^{d_0}$ [^6]. Além disso, por construção para

    $$\
    V := \{(\sigma((W_a^{(1)})^j x_i + b_a^{(1)}))_i\}_{j=1}^m
    $$

    vale que $(\{(V)_i\}_{i=1}^m)$ tem rank maior que o de $V_a$ [^6]. Concatenando $\alpha_1$ e $\alpha_2$ agora produz um caminho contínuo de $\theta_a$ para outro parâmetro de rede neural com rank associado maior, de tal forma que $\Lambda_{A, \sigma, S, L}$ é monotonicamente decrescente ao longo do caminho [^6]. Iterando esta construção, podemos encontrar um caminho para um parâmetro de rede neural onde a matriz associada tem rank total [^6]. Isso reduz o problema ao Caso 1 [^6]. $\blacksquare$

### Conclusão
A presença de *spurious minima* representa um desafio significativo na otimização de redes neurais. A Proposição 12.4 demonstra que, sob certas condições, redes neurais rasas superparametrizadas não possuem *spurious valleys*. No entanto, essa garantia não se estende a redes profundas, onde a complexidade do *loss landscape* aumenta consideravelmente. A visualização e a caracterização desses *spurious minima* são cruciais para desenvolver estratégias de otimização mais robustas e eficientes. As seções subsequentes deste capítulo exploram outras características do *loss landscape*, como a presença de *saddle points*, e como essas características afetam a capacidade de generalização das redes neurais.

### Referências
[^1]: Capítulo 12, p. 165
[^3]: Capítulo 12, p. 167
[^4]: Capítulo 12, p. 168
[^5]: Capítulo 12, p. 169
[^6]: Capítulo 12, p. 170
<!-- END -->