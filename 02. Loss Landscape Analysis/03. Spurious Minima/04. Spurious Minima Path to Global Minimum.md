## A Monotonic Path Condition for Excluding Spurious Valleys

### Introdução
No Capítulo 12, foi introduzido o conceito de *loss landscape* como uma representação gráfica do risco empírico em função dos pesos de uma rede neural [^1]. Um dos desafios na otimização de redes neurais é a presença de mínimos espúrios, que podem impedir que algoritmos de otimização convirjam para um mínimo global [^1]. Para mitigar esse problema, é crucial entender as características do *loss landscape* que favorecem a existência de soluções subótimas. Este capítulo explora a conexão entre a existência de um caminho contínuo com perda monotonicamente decrescente e a ausência de *spurious valleys*. Em continuidade à definição de *spurious valleys* apresentada na Definição 12.3 [^4], exploraremos uma condição suficiente para garantir que o *loss landscape* não contenha tais regiões indesejáveis.

### Conceitos Fundamentais

A Definição 12.3 introduz formalmente o conceito de *spurious valley* como um componente conexo por caminhos do conjunto de subnível da função de perda, que não contém um mínimo global [^4]. Formalmente, dado um conjunto de dados $S = \{(x_i, y_i)\}_{i=1}^m$, uma função de perda $L$, e um valor $c \in \mathbb{R}$, o conjunto de subnível é definido como:

$$
\Omega_A(c) := \{\theta \in PN(A, \infty) \mid \Lambda_{A, \sigma, S, L}(\theta) \leq c\}
$$

onde $PN(A, \infty)$ representa o conjunto de redes neurais com arquitetura $A$ e pesos não limitados [^4]. Um componente conexo por caminhos de $\Omega_A(c)$ que não contém um mínimo global de $\Lambda_{A, \sigma, S, L}$ é denominado *spurious valley* [^4].

A Proposição 12.4 apresenta uma condição para a não existência de *spurious valleys* em redes neurais *shallow* e *overparameterized* [^4]. A proposição estabelece que, sob certas condições, a existência de um caminho contínuo entre um ponto em um *spurious valley* e um mínimo global, ao longo do qual a função de perda diminui monotonicamente, garante a ausência de *spurious valleys* [^4].

**Proposição 12.4:** Seja $A = (d_0, d_1, 1) \in \mathbb{N}^3$ e seja $S = \{(x_i, y_i)\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R})^m$ uma amostra tal que $m \leq d_1$. Além disso, seja $\sigma \in \mathcal{M}$ não um polinômio, e seja $L$ uma função de perda convexa. Suponha ainda que $\Lambda_{A, \sigma, S, L}$ tenha pelo menos um mínimo global. Então, $\Lambda_{A, \sigma, S, L}$ não tem *spurious valleys* [^4].

A prova da Proposição 12.4 demonstra que, dados dois parâmetros $\theta_a, \theta_b \in PN(A, \infty)$ com $\Lambda_{A, \sigma, S, L}(\theta_a) > \Lambda_{A, \sigma, S, L}(\theta_b)$, existe um parâmetro $\theta_c$ tal que:

1.  $\Lambda_{A, \sigma, S, L}(\theta_b) = \Lambda_{A, \sigma, S, L}(\theta_c)$
2.  Existe um caminho contínuo $\alpha: [0, 1] \rightarrow PN(A, \infty)$ tal que $\alpha(0) = \theta_a$, $\alpha(1) = \theta_c$, e $\Lambda_{A, \sigma, S, L}(\alpha(t))$ é monotonicamente decrescente [^4].

A existência desse caminho contínuo, com perda monotonicamente decrescente, implica que $\theta_a$ e $\theta_b$ pertencem ao mesmo vale, descartando a existência de *spurious valleys* [^4]. O Exercício 12.7 reforça essa ideia, mostrando que se tal caminho existe entre um parâmetro $\theta_1$ e um mínimo global $\theta_2$, então $\theta_1$ não pode ser um elemento de um *spurious valley* [^9].

A construção desse caminho envolve a manipulação dos pesos da rede neural de forma a reduzir monotonicamente a função de perda. A prova considera dois casos:

*   **Caso 1:** A matriz $V_a$, construída a partir das ativações da camada oculta, tem rank completo ($m$) [^5]. Nesse caso, é possível encontrar uma matriz $W$ que conecta $\theta_a$ a $\theta_b$ através de um caminho linear, garantindo a diminuição monotônica da perda [^5].

*   **Caso 2:** A matriz $V_a$ tem rank menor que $m$ [^5]. Nesse caso, a prova demonstra que é possível encontrar um caminho contínuo para outro parâmetro de rede neural com rank maior. Esse processo é iterado até que a matriz tenha rank completo, reduzindo o problema ao Caso 1 [^5].

### Conclusão

A Proposição 12.4 e sua prova fornecem um insight valioso sobre as condições que garantem a ausência de *spurious valleys* no *loss landscape* de redes neurais [^4]. A existência de um caminho contínuo com perda monotonicamente decrescente entre qualquer ponto e um mínimo global é uma condição suficiente para excluir a presença de *spurious valleys* [^4]. Esse resultado tem implicações importantes para o design de algoritmos de otimização, pois sugere que a busca por tais caminhos pode ser uma estratégia eficaz para escapar de regiões subótimas e convergir para um mínimo global. Em particular, o Exercício 12.7 formaliza essa conexão, demonstrando que a existência do caminho monotonicamente decrescente impede que o ponto inicial pertença a um *spurious valley* [^9].

### Referências
[^1]: Capítulo 12, p. 165
[^2]: Capítulo 12, p. 166
[^3]: Capítulo 12, p. 167
[^4]: Capítulo 12, p. 168
[^5]: Capítulo 12, p. 169
[^6]: Capítulo 12, p. 170
[^7]: Capítulo 12, p. 171
[^8]: Capítulo 12, p. 172
[^9]: Capítulo 12, p. 173
<!-- END -->