## 12.2 Spurious Minima in Loss Landscapes

### Introdução
O Capítulo 12 foca na análise da *loss landscape* [^1], que representa o gráfico do risco empírico em função dos pesos de uma rede neural. Entender a topografia dessa *landscape* é crucial para compreender o comportamento dos algoritmos de otimização, como o gradiente descendente, durante o treinamento de redes neurais. Este capítulo se aprofunda no conceito de **spurious minima**, que representam um desafio significativo na otimização de redes neurais profundas. Ao contrário dos mínimos globais, que correspondem às soluções ótimas, os **spurious minima** são mínimos locais que podem aprisionar os algoritmos de otimização e impedir que alcancem a solução ideal [^1].

### Conceitos Fundamentais
Do ponto de vista da otimização, o cenário ideal da função de perda (loss landscape) possui um único mínimo global no centro de um grande vale, de forma que o método do gradiente descendente convirja para o mínimo independentemente da inicialização escolhida [^3]. Contudo, essa situação ideal não é realista para redes neurais profundas [^3].

Para uma rede neural rasa simples definida como:

$$\
\Phi(x) = W^{(1)} \sigma(W^{(0)}x + b^{(0)}) + b^{(1)}
$$

onde $x \in \mathbb{R}^d$, fica claro que para cada matriz de permutação $P$:

$$\
\Phi(x) = W^{(1)} P^T (PW^{(0)}x + Pb^{(0)}) + b^{(1)} \quad \text{para todo } x \in \mathbb{R}^d
$$

Em geral, existem múltiplas parametrizações que realizam a mesma função de saída. Além disso, se existir pelo menos um mínimo global com pesos não invariantes à permutação, então existem mais de um mínimo global na *loss landscape* [^4].

A existência de múltiplos mínimos globais não é necessariamente um problema; na verdade, pode ser benéfico. A questão mais crítica é a existência de **non-global minima** [^4]. Generalizando a noção de **non-global minima** para **spurious valleys**, a Definição 12.3 [^4] formaliza esse conceito:

**Definição 12.3:** Seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. Seja $m \in \mathbb{N}$, e $S = \{(x_i, y_i)\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R}^{d_{L+1}})^m$ uma amostra e $\mathcal{L}$ uma função de perda. Para $c \in \mathbb{R}$, definimos o subconjunto de nível de $\Lambda_{A,\sigma,S,\mathcal{L}}$ como:

$$\
\Omega_A(c) := \{\theta \in \mathcal{P}N(A, \infty) \mid \Lambda_{A,\sigma,S,\mathcal{L}}(\theta) \leq c\}
$$

Uma componente conexa por caminhos de $\Omega_A(c)$, que não contém um mínimo global de $\Lambda_{A,\sigma,S,\mathcal{L}}$, é chamada de **spurious valley** [^4].

A Proposição 12.4 [^4] demonstra que os mínimos locais espúrios não existem para redes neurais rasas superparametrizadas, ou seja, para redes neurais que possuem pelo menos tantos parâmetros na camada oculta quanto amostras de treinamento.

**Proposição 12.4:** Seja $A = (d_0, d_1, 1) \in \mathbb{N}^3$ e seja $S = \{(x_i, y_i)\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R})^m$ uma amostra tal que $m \leq d_1$. Além disso, seja $\sigma \in \mathcal{M}$ não polinomial, e seja $\mathcal{L}$ uma função de perda convexa. Assuma ainda que $\Lambda_{A,\sigma,S,\mathcal{L}}$ tenha pelo menos um mínimo global. Então, $\Lambda_{A,\sigma,S,\mathcal{L}}$ não possui **spurious valleys** [^4].

*Prova*: A prova demonstra que, dados $\theta_a, \theta_b \in \mathcal{P}N(A, \infty)$ com $\Lambda_{A,\sigma,S,\mathcal{L}}(\theta_a) > \Lambda_{A,\sigma,S,\mathcal{L}}(\theta_b)$, existe um parâmetro $\theta_c$ tal que:

*   $\Lambda_{A,\sigma,S,\mathcal{L}}(\theta_b) = \Lambda_{A,\sigma,S,\mathcal{L}}(\theta_c)$
*   Existe um caminho contínuo $\alpha: [0, 1] \rightarrow \mathcal{P}N(A, \infty)$ tal que $\alpha(0) = \theta_a$, $\alpha(1) = \theta_c$, e $\Lambda_{A,\sigma,S,\mathcal{L}}(\alpha(t))$ é monotonicamente decrescente.

Pelo Exercício 12.7 [^4], a construção acima elimina a existência de **spurious valleys**, escolhendo $\theta_a$ como um elemento de uma **spurious valley** e $\theta_b$ como um mínimo global. $\blacksquare$

### Conclusão
A existência de **spurious minima** e **spurious valleys** na *loss landscape* é um desafio significativo na otimização de redes neurais profundas. Enquanto a Proposição 12.4 [^4] demonstra que **spurious valleys** não existem em redes rasas superparametrizadas sob certas condições, a realidade das redes profundas é mais complexa. A visualização da *loss landscape* [^3] e o desenvolvimento de algoritmos de otimização robustos são cruciais para mitigar os efeitos negativos dos **spurious minima** e garantir o treinamento eficaz de redes neurais profundas.

### Referências
[^1]: Capítulo 12, Loss landscape analysis
[^3]: Seção 12.2, Spurious minima
[^4]: Seção 12.2, Spurious minima, Definição 12.3 e Proposição 12.4
<!-- END -->