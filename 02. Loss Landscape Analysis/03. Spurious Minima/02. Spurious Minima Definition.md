## Spurious Valleys no Loss Landscape

### Introdução
No Capítulo 12, foi introduzido o conceito de *loss landscape* como a representação gráfica do risco empírico em função dos pesos de uma rede neural [^1]. A análise da *loss landscape* é crucial para entender o comportamento dos algoritmos de otimização, como o gradiente descendente, durante o treinamento de redes neurais [^1]. Um dos desafios na otimização é a presença de mínimos locais e pontos de sela, que podem impedir a convergência para um mínimo global [^1]. Expandindo essa discussão, este capítulo se aprofunda no conceito de *spurious valleys*, que representam regiões do espaço de parâmetros onde o algoritmo de otimização pode ficar preso sem atingir o ótimo global.

### Conceitos Fundamentais

**Definição de Spurious Valley:** Uma *spurious valley* é definida como um componente conexo por caminhos do subconjunto de nível da função de perda que não contém um mínimo global [^4]. Em termos mais intuitivos, é uma região no *loss landscape* que se assemelha a um vale, mas que leva a um ponto sub-ótimo [^2].

Formalmente, seja $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$ a arquitetura da rede neural, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ a função de ativação, $m \in \mathbb{N}$ o número de amostras, e $S = \{(x_i, y_i)\}_{i=1}^m \in (\mathbb{R}^{d_0} \times \mathbb{R}^{d_{L+1}})^m$ o conjunto de treinamento [^4]. Seja $L$ a função de perda. Para $c \in \mathbb{R}$, definimos o subconjunto de nível de $\Lambda_{A,\sigma,S,L}$ como:

$$\Omega_A(c) := \{\theta \in PN(A, \infty) \mid \Lambda_{A,\sigma,S,L}(\theta) \leq c\}$$

onde $PN(A, \infty)$ representa o espaço de parâmetros da rede neural [^4]. Uma *spurious valley* é, então, um componente conexo por caminhos de $\Omega_A(c)$ que não contém um mínimo global de $\Lambda_{A,\sigma,S,L}$ [^4].

**Importância das Spurious Valleys:** A existência de *spurious valleys* é um problema significativo na otimização de redes neurais, pois pode levar a um desempenho inferior ao ideal. Um algoritmo de otimização preso em uma *spurious valley* pode convergir para um ponto que não generaliza bem para dados não vistos, resultando em baixa acurácia e desempenho ruim [^2].

**Relação com Mínimos Locais:** Embora *spurious valleys* estejam relacionadas a mínimos locais, elas representam um conceito mais geral. Um mínimo local pode estar contido em uma *spurious valley*, mas nem toda *spurious valley* contém um mínimo local. A *spurious valley* representa uma região mais ampla do espaço de parâmetros onde a otimização pode ser dificultada [^2].

**Proposição sobre a Não Existência em Redes Overparameterizadas:** A Proposição 12.4 [^4] aborda a não existência de *spurious valleys* em redes neurais *overparameterizadas*, ou seja, redes neurais que possuem mais parâmetros na camada oculta do que amostras no conjunto de treinamento. Essa proposição estabelece que, sob certas condições (função de perda convexa, função de ativação não polinomial), uma rede neural *overparameterizada* não possui *spurious valleys*, desde que $\Lambda_{A,\sigma,S,L}$ possua pelo menos um mínimo global [^4]. A demonstração envolve a construção de um caminho contínuo entre um ponto em uma *spurious valley* e um mínimo global, ao longo do qual a função de perda diminui monotonicamente [^4]. O exercício 12.7 [^9] reforça esse conceito.

**Demonstração da Proposição 12.4 (Resumo):**
A prova da Proposição 12.4 [^4] envolve a construção de um caminho contínuo entre um ponto $\theta_a$ em uma potencial *spurious valley* e um mínimo global $\theta_b$, de forma que a função de perda $\Lambda_{A,\sigma,S,L}$ diminua monotonicamente ao longo desse caminho. Se tal caminho pode ser construído, então $\theta_a$ não pode estar em uma *spurious valley*, pois estaria conectado a um mínimo global por um caminho de diminuição de perda.

A prova considera dois casos [^5]:
1.  Onde uma matriz $V_a$ associada aos parâmetros da rede tem rank $m$ (o número de amostras) [^5].
2.  Onde a matriz $V_a$ tem rank menor que $m$ [^5].

No segundo caso, um caminho é construído para outro parâmetro de rede neural com um rank maior até que o primeiro caso se aplique [^5]. A monotonicidade decrescente da função de perda ao longo do caminho é garantida pela convexidade da função de perda $L$ [^5].

### Conclusão
As *spurious valleys* representam um desafio significativo na otimização de redes neurais, pois podem levar à convergência para soluções sub-ótimas [^2]. A compreensão das condições sob as quais as *spurious valleys* surgem, como em redes não *overparameterizadas* ou com funções de perda não convexas, é crucial para o desenvolvimento de algoritmos de otimização mais eficazes [^4]. A Proposição 12.4 [^4] fornece um *insight* importante sobre a relação entre *overparameterization* e a ausência de *spurious valleys*, sugerindo que redes maiores podem ser mais fáceis de otimizar nesse aspecto. A visualização da *loss landscape*, conforme discutido na Seção 12.1 [^3], também pode ajudar a identificar a presença de *spurious valleys* e outros obstáculos à otimização.

### Referências
[^1]: Capítulo 12, p. 165
[^2]: Figura 12.1, p. 166
[^3]: Seção 12.1, p. 167
[^4]: Definição 12.3 e Proposição 12.4, p. 168
[^5]: Demonstração da Proposição 12.4, p. 169
[^9]: Exercício 12.7, p. 173

<!-- END -->