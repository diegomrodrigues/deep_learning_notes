## Funções de Ativação: Universalidade e Restrições Polinomiais

### Introdução
Este capítulo explora a capacidade de aproximação universal de redes neurais, com foco na influência das funções de ativação nessa capacidade. Em particular, investigaremos como a escolha da função de ativação afeta a capacidade de uma rede neural de aproximar funções contínuas em conjuntos compactos [^21]. Como vimos anteriormente, redes neurais são ferramentas poderosas, mas é crucial entender suas limitações e requisitos para garantir que possam modelar relações complexas de entrada-saída. Este capítulo se baseia nos conceitos introduzidos no Capítulo 2 e formaliza a noção de aproximação universal, culminando na demonstração de que redes neurais com funções de ativação adequadas podem aproximar quase todas as relações de entrada-saída razoáveis [^21].

### Conceitos Fundamentais
**Funções de Ativação e Aproximação Universal**

As **funções de ativação** desempenham um papel fundamental nas redes neurais. Para que uma rede neural seja um **aproximador universal**, ou seja, capaz de aproximar qualquer função contínua, as funções de ativação devem pertencer ao conjunto *M* de funções contínuas por partes e localmente limitadas [^23]. Exemplos comuns incluem ReLU, SiLU e Sigmoid. No entanto, uma condição crucial é que a função de ativação *não seja um polinômio* [^23].

A razão para essa restrição reside no fato de que redes neurais com funções de ativação polinomiais são limitadas em sua capacidade de aproximar funções complexas. Redes com funções de ativação polinomiais são equivalentes a polinômios de ordem superior e, portanto, não conseguem capturar a complexidade de funções não polinomiais [^23].

Formalmente, o conjunto *M* é definido como:
$$
M := \{\sigma \in L_{loc}^{\infty}(\mathbb{R}) \mid \exists \text{ intervalos } I_1, \dots, I_M \text{ particionando } \mathbb{R}, \text{ s.t. } \sigma \in C^0(I_j) \text{ para todo } j = 1, \dots, M\}
$$
onde $M \in \mathbb{N}$ é finito e os intervalos $I_j$ possuem medida de Lebesgue positiva [^23].

**Definição de Aproximador Universal**

Um conjunto de funções *H* de $\mathbb{R}^d$ em $\mathbb{R}$ é um **aproximador universal** (de $C^0(\mathbb{R}^d)$) se, para todo $\epsilon > 0$, todo conjunto compacto $K \subseteq \mathbb{R}^d$ e toda função $f \in C^0(\mathbb{R}^d)$, existe uma função $g \in H$ tal que:
$$
\sup_{x \in K} |f(x) - g(x)| < \epsilon
$$
Em outras palavras, *H* é um aproximador universal se pode aproximar uniformemente qualquer função contínua em qualquer conjunto compacto [^22].

**Teorema da Aproximação Universal para Redes Neurais Rasas**

O Teorema 3.8 [^23] afirma que, dado $d \in \mathbb{N}$ e $\sigma \in M$, então $N_1(\sigma;1)$ é um aproximador universal de $C^0(\mathbb{R}^d)$ se, e somente se, $\sigma$ não for um polinômio. Aqui, $N_1(\sigma;1)$ representa o conjunto de todas as funções realizadas por redes neurais rasas (shallow neural networks) com entrada *d*-dimensional, profundidade no máximo 1, largura arbitrária e função de ativação $\sigma$.

A prova deste teorema envolve verificar três afirmações cruciais [^24]:
1.  Se $C^0(\mathbb{R}^1) \subseteq \overline{N_1(\sigma;1)}$, então $C^0(\mathbb{R}^d) \subseteq \overline{N_1(\sigma;1)}$.
2.  Se $\sigma \in C^\infty(\mathbb{R})$ não é um polinômio, então $C^0(\mathbb{R}^1) \subseteq \overline{N_1(\sigma;1)}$.
3.  Se $\sigma \in M$ não é um polinômio, então existe $\tilde{\sigma} \in C^\infty(\mathbb{R}) \cap \overline{N_1(\sigma;1)}$ que não é um polinômio.

Essas afirmações, juntamente com a Proposição 3.3 [^22], estabelecem a implicação "$\Leftarrow$" do Teorema 3.8. A direção reversa é direta de verificar.

**Teorema de Stone-Weierstrass**
O Teorema de Stone-Weierstrass (Teorema 3.4) fornece uma ferramenta fundamental para demonstrar que um conjunto de funções é um aproximador universal [^22]. O teorema afirma que, dado um conjunto compacto $K \subseteq \mathbb{R}^d$ e uma álgebra de funções $H \subseteq C^0(K, \mathbb{R})$ que satisfaz:
(a) Para todo $x \in K$, existe $f \in H$ tal que $f(x) \neq 0$.
(b) Para todo $x \neq y \in K$, existe $f \in H$ tal que $f(x) \neq f(y)$.
Então, $H$ é denso em $C^0(K)$.

Em outras palavras, qualquer função contínua em *K* pode ser aproximada arbitrariamente bem por uma função em *H* [^22].

**Exemplo: Polinômios como Aproximadores Universais**

Um exemplo importante é o conjunto de polinômios. O Exemplo 3.5 [^22] demonstra que os polinômios são densos em $C^0(\mathbb{R}^d)$. Isso significa que qualquer função contínua em $\mathbb{R}^d$ pode ser aproximada por um polinômio.

### Conclusão

Em resumo, a escolha da função de ativação é crucial para a capacidade de aproximação universal de redes neurais. A condição de que a função de ativação não seja um polinômio garante que a rede neural possa modelar relações complexas e não lineares. O Teorema da Aproximação Universal formaliza essa ideia, demonstrando que redes neurais com funções de ativação adequadas podem aproximar qualquer função contínua em um conjunto compacto [^23]. Os teoremas de Stone-Weierstrass e Kolmogorov fornecem ferramentas adicionais para analisar e entender a capacidade de aproximação de redes neurais [^22, 31].

### Referências
[^21]: Capítulo 3, página 21
[^22]: Capítulo 3, página 22
[^23]: Capítulo 3, página 23
[^24]: Capítulo 3, página 24
[^31]: Capítulo 3, página 31
<!-- END -->