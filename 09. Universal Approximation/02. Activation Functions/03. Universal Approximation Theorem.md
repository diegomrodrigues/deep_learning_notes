## Teorema da Aproximação Universal com Funções de Ativação Descontínuas

### Introdução
No capítulo anterior, foi introduzida a vasta classe de funções de ativação que permitem a aproximação universal. O **Teorema da Aproximação Universal** [^1] estabelece que redes neurais com uma única camada oculta podem aproximar qualquer função contínua em um conjunto compacto com precisão arbitrária, desde que a camada oculta contenha um número suficientemente grande de neurônios. Este capítulo aprofunda-se nas condições sob as quais esse teorema se mantém, especialmente no contexto de funções de ativação que podem apresentar descontinuidades. Expandindo sobre o conceito de **aproximadores universais** [^2], investigaremos como a escolha da função de ativação influencia a capacidade da rede neural de aproximar funções.

### Conceitos Fundamentais

O **Teorema da Aproximação Universal** é formulado para um conjunto maior $M$ que permite funções de ativação com descontinuidades em um conjunto (possivelmente não finito) de medida de Lebesgue zero [^4]. No entanto, um caso mais simples é frequentemente usado para evitar tecnicidades.

Definimos $M$ como o conjunto de funções $\sigma \in L_{loc}^1(\mathbb{R})$ para as quais existem intervalos $I_1, ..., I_M$ particionando $\mathbb{R}$, tal que $\sigma \in C^0(I_j)$ para todo $j = 1, ..., M$ [^4]. Em outras palavras, $\sigma$ é uma função contínua por partes, com um número finito de descontinuidades.

**Exemplos de funções de ativação pertencentes a $M$ incluem** [^4]:
*   Todas as funções contínuas não polinomiais (e.g., ReLU, SiLU, Sigmoid)
*   Funções descontínuas como a função de Heaviside ($x \mapsto \mathbb{1}_{x>0}$) e $x \mapsto \mathbb{1}_{x>0} \sin(1/x)$.

**Funções excluídas de $M$ incluem** [^4]:
*   $x \mapsto 1/x$ (não localmente limitada).

O teorema fundamental que exploramos é o seguinte [^4]:

**Teorema 3.8.** Seja $d \in \mathbb{N}$ e $\sigma \in M$. Então $N_1(\sigma; 1)$ é um aproximador universal de $C^0(\mathbb{R}^d)$ se e somente se $\sigma$ não é um polinômio.

*Prova (Esboço):* A prova envolve demonstrar as seguintes afirmações [^4]:

*   (i) Se $C^0(\mathbb{R}^1) \subseteq \overline{N_1(\sigma; 1)}$, então $C^0(\mathbb{R}^d) \subseteq \overline{N_1(\sigma; 1)}$.
*   (ii) Se $\sigma \in C^\infty(\mathbb{R})$ não é um polinômio, então $C^0(\mathbb{R}^1) \subseteq \overline{N_1(\sigma; 1)}$.
*   (iii) Se $\sigma \in M$ não é um polinômio, então existe $\tilde{\sigma} \in C^\infty(\mathbb{R}) \cap \overline{N_1(\sigma; 1)}$ que não é um polinômio.

A combinação dessas afirmações, juntamente com a Proposição 3.3 [^2], estabelece a implicação "$\Leftarrow$" do Teorema 3.8. A direção reversa é direta [^4]. $\blacksquare$

**Observação 3.9.** Redes neurais podem aproximar bem funções não contínuas em relação a normas adequadas [^5].

O Teorema da Aproximação Universal de Leshno, Lin, Pinkus e Schocken [130] é formulado para um conjunto $M$ ainda maior, permitindo funções de ativação que têm descontinuidades em um conjunto de medida de Lebesgue zero (possivelmente não finito) [^5]. Em vez de provar o teorema nesta generalidade, recorremos ao caso mais simples mencionado acima. Isso permite evitar algumas tecnicidades, mas as principais ideias permanecem as mesmas [^5].

### Conclusão

Este capítulo apresentou uma visão detalhada do **Teorema da Aproximação Universal** no contexto de funções de ativação descontínuas. Ao restringir as funções de ativação ao conjunto $M$ de funções contínuas por partes, conseguimos evitar tecnicidades desnecessárias e fornecer uma prova concisa do teorema. Os conceitos e resultados apresentados aqui servem como base para explorar as capacidades e limitações das redes neurais na aproximação de funções complexas.

### Referências
[^1]: Chapter 3: Universal approximation [^2]: Definition 3.2. Let d∈ N. A set of functions H from Rd to R is a universal approximator (of CO(Rd)), if for every ɛ > 0, every compact KC Rd, and every f ∈ Cº (Rd), there exists g ∈ H such that supx∈K |f(x) – g(x)| < ε. [^4]: The universal approximation theorem by Leshno, Lin, Pinkus and Schocken [130]—of which Theorem 3.8 is a special case is even formulated for a much larger set M, which allows for activation functions that have discontinuities at a (possibly non-finite) set of Lebesgue measure zero. Instead of proving the theorem in this generality, we resort to the simpler case stated above. This allows to avoid some technicalities, but the main ideas remain the same. [^5]: Remark 3.9. We will see in Exercise 3.26 and Corollary 3.18 that neural networks can also arbitrarily well approximate non-continuous functions with respect to suitable norms.
<!-- END -->