## A Função de Ativação Sigmoidal e sua Aplicação em Redes Neurais

### Introdução
As funções de ativação desempenham um papel crucial na capacidade das redes neurais de modelar relações complexas e não lineares. Entre as diversas opções disponíveis, a função sigmoidal se destaca por suas propriedades únicas e sua relevância histórica no campo do aprendizado profundo. Este capítulo se dedica a explorar a função sigmoidal em detalhes, analisando suas características matemáticas e sua aplicação em redes neurais. Este capítulo se baseia nas informações apresentadas nas páginas anteriores, em particular a discussão sobre funções de ativação e a necessidade de não-linearidade [^3].

### Conceitos Fundamentais
A **função sigmoidal** é uma função contínua $\sigma$ que se aproxima de 1 quando $x$ tende ao infinito e de 0 quando $x$ tende ao infinito negativo [^1]. Matematicamente, ela é definida como:

$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$

Essa função é comumente utilizada em redes neurais devido à sua capacidade de introduzir **não-linearidade** [^1]. Como vimos anteriormente [^3], a não-linearidade é essencial para que as redes neurais possam aproximar funções complexas e modelar relações que não podem ser representadas por modelos lineares.

**Características da Função Sigmoidal:**

*   **Continuidade:** A função sigmoidal é contínua e diferenciável em todo o seu domínio. Essa propriedade é importante para o treinamento de redes neurais utilizando algoritmos de otimização baseados em gradientes.
*   **Limites Assintóticos:** Como mencionado anteriormente [^1], a função sigmoidal possui limites assintóticos bem definidos:
    *   $\lim_{x \to \infty} \sigma(x) = 1$
    *   $\lim_{x \to -\infty} \sigma(x) = 0$
*   **Saída Limitada:** A saída da função sigmoidal está sempre no intervalo (0, 1). Essa característica pode ser útil em problemas de classificação binária, onde a saída da rede neural pode ser interpretada como uma probabilidade.
*   **Derivada:** A derivada da função sigmoidal possui uma forma simples e pode ser expressa em termos da própria função sigmoidal:

    $$ \sigma'(x) = \sigma(x)(1 - \sigma(x)) $$

    Essa propriedade facilita o cálculo do gradiente durante o treinamento da rede neural.

**Vanishing Gradients:**

Apesar de suas vantagens, a função sigmoidal apresenta uma desvantagem significativa conhecida como o problema dos **vanishing gradients**. Esse problema ocorre quando o valor absoluto de $x$ é muito grande (positivo ou negativo), a derivada da função sigmoidal se aproxima de zero. Consequentemente, durante o treinamento da rede neural, o gradiente se torna muito pequeno, dificultando a atualização dos pesos das camadas anteriores. Esse problema pode levar a um treinamento lento ou até mesmo à estagnação do aprendizado.

**Alternativas à Função Sigmoidal:**

Devido ao problema dos vanishing gradients, outras funções de ativação, como a ReLU (Rectified Linear Unit), têm ganhado popularidade nos últimos anos [^3]. A ReLU possui uma derivada constante para valores positivos de $x$, o que ajuda a mitigar o problema dos vanishing gradients. No entanto, a função sigmoidal ainda pode ser útil em algumas situações, especialmente em camadas de saída de redes neurais para problemas de classificação binária.

### Conclusão
A função sigmoidal é uma função de ativação clássica com propriedades interessantes, como continuidade, limites assintóticos bem definidos e saída limitada. No entanto, seu uso em redes neurais profundas pode ser limitado pelo problema dos vanishing gradients. Apesar disso, a função sigmoidal ainda pode ser uma escolha adequada em algumas situações, especialmente em problemas de classificação binária ou em redes neurais com poucas camadas. A escolha da função de ativação ideal depende das características específicas do problema e da arquitetura da rede neural. As discussões sobre shallow networks [^3] dão contexto a essa decisão.

### Referências
[^1]: "A sigmoidal activation function is a continuous function \u03c3 that approaches 1 as x goes to infinity and 0 as x goes to negative infinity, commonly used in neural networks for its ability to introduce non-linearity."
[^3]: Example 3.7. Activation functions belonging to M include, in particular, all continuous non-polynomial functions, which in turn includes all practically relevant activation functions such as the ReLU, the SiLU, and the Sigmoid discussed in Section 2.3.

<!-- END -->