## Corolário da Aproximação Universal para Redes Neurais Profundas

### Introdução
O Teorema da Aproximação Universal estabelece a capacidade das redes neurais de aproximar funções contínuas. O Capítulo 3 expande essa ideia, explorando as condições sob as quais redes neurais *profundas* podem ser consideradas aproximadores universais. O Corolário 3.17 [^9] fornece uma extensão crucial deste teorema, focando em redes neurais profundas com funções de ativação não polinomiais.

### Conceitos Fundamentais
O Corolário 3.17 afirma que uma rede neural profunda, denotada por $N_d(\sigma; L)$, é um aproximador universal de $C^0(\mathbb{R}^d)$ se e somente se a função de ativação $\sigma$ não for um polinômio [^9]. Aqui, $N_d(\sigma; L)$ representa o conjunto de todas as funções realizadas por redes neurais com entrada de dimensão $d$, profundidade no máximo $L$ e função de ativação $\sigma$.

**Demonstração (Esboço):**

A demonstração do Corolário 3.17 [^10] segue uma abordagem construtiva, dividindo o problema em passos lógicos:

1.  **Implicação "←":** Assume-se que $\sigma \in M$ (o conjunto de funções contínuas por partes e localmente limitadas) não é um polinômio. O objetivo é mostrar que $N_d(\sigma; L)$ é um aproximador universal de $C^0(\mathbb{R}^d)$.

2.  **Caso L = 1:** O caso base, onde a rede tem apenas uma camada oculta, é estabelecido pelo Teorema 3.8 [^3].

3.  **Caso L > 1:** Para redes mais profundas, a demonstração utiliza o Teorema 3.8 para aproximar a função alvo $f$ com uma rede neural rasa $\Phi_{shallow}$ [^10]:
    $$
    \sup_{x \in K} |f(x) - \Phi_{shallow}(x)| < \frac{\epsilon}{2}
    $$
    onde $K \subseteq \mathbb{R}^d$ é um conjunto compacto e $\epsilon > 0$ é a precisão desejada.

4.  **Aproximação da Identidade:** A chave para estender o resultado para redes profundas é aproximar a função identidade. A Proposição 3.16 [^9] (ou sua extensão) garante a existência de uma rede neural $ \Phi_{id} \in N_1(\sigma; L-1)$ que aproxima a função identidade em um intervalo $[-n,n]$:
    $$
    \sup_{x \in [-n, n]} |x - \Phi_{id}(x)| < \frac{\epsilon}{2}
    $$
    onde $n$ é escolhido de forma que $\{\Phi_{shallow}(x) | x \in K \} \subseteq [-n, n]$ [^10].

5.  **Composição:** Define-se $\Phi = \Phi_{id} \circ \Phi_{shallow}$. Pela Proposição 2.3 (iv) [^10], $\Phi \in N_d(\sigma; L)$.

6.  **Erro Total:** O erro total é então limitado da seguinte forma [^10]:
    $$
    \sup_{x \in K} |f(x) - \Phi(x)| = \sup_{x \in K} |f(x) - \Phi_{id}(\Phi_{shallow}(x))| \leq \sup_{x \in K} (|f(x) - \Phi_{shallow}(x)| + |\Phi_{shallow}(x) - \Phi_{id}(\Phi_{shallow}(x))|) < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
    $$

7.  **Implicação "→":** Esta parte é deixada como exercício (Exercício 3.23) [^14].

### Conclusão
O Corolário 3.17 [^10] expande significativamente o Teorema da Aproximação Universal para redes neurais profundas. Ele demonstra que, contanto que a função de ativação não seja um polinômio, uma rede neural profunda com largura suficiente pode aproximar qualquer função contínua em um conjunto compacto com precisão arbitrária. Este resultado sublinha a capacidade inerente das redes neurais profundas como ferramentas de aproximação universal.

### Referências
[^3]: Theorem 3.8.
[^9]: Corollary 3.17, Proposition 3.16.
[^10]: Proof of Corollary 3.17.
[^14]: Exercise 3.23.

<!-- END -->