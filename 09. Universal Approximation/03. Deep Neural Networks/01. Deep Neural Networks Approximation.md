## Aproximação Universal com Redes Neurais Profundas

### Introdução
Em continuidade ao Capítulo 2, que introduziu as redes neurais, este capítulo explora suas capacidades de aproximação [^1]. Especificamente, investiga-se se existem limitações inerentes ao tipo de funções que uma rede neural pode representar [^1]. O objetivo é demonstrar que as redes neurais são ferramentas universais, capazes de aproximar quase qualquer relação de entrada-saída sensível, desde que possuam arquiteturas suficientemente grandes e complexas [^1]. Este capítulo se baseia fortemente nos conceitos apresentados no Capítulo 2 e formaliza e prova a alegação de aproximação universal [^1].

### Conceitos Fundamentais

**Teorema da Aproximação Universal:**
Para analisar os tipos de funções que podem ser aproximadas por redes neurais, o foco inicial é a aproximação uniforme de funções contínuas $f: \mathbb{R}^d \rightarrow \mathbb{R}$ em conjuntos compactos [^1]. Para isso, é introduzida a noção de *convergência compacta* [^1].

**Definição 3.1 (Convergência Compacta):**
Uma sequência de funções $f_n: \mathbb{R}^d \rightarrow \mathbb{R}$, $n \in \mathbb{N}$, converge compactamente para uma função $f: \mathbb{R}^d \rightarrow \mathbb{R}$ se, para todo compacto $K \subseteq \mathbb{R}^d$, tem-se que $\lim_{n \to \infty} \sup_{x \in K} |f_n(x) - f(x)| = 0$. Neste caso, escrevemos $f_n \xrightarrow{CC} f$ [^1].

Ao longo deste capítulo, considera-se sempre o espaço $C^0(\mathbb{R}^d)$ equipado com a topologia da Definição 3.1 [^1].

**Definição 3.2 (Aproximadores Universais):**
Um conjunto de funções $\mathcal{H}$ de $\mathbb{R}^d$ para $\mathbb{R}$ é um *aproximador universal* (de $C^0(\mathbb{R}^d)$) se, para todo $\epsilon > 0$, todo compacto $K \subseteq \mathbb{R}^d$ e toda $f \in C^0(\mathbb{R}^d)$, existe $g \in \mathcal{H}$ tal que $\sup_{x \in K} |f(x) - g(x)| < \epsilon$ [^2].

Para um conjunto $\mathcal{H}$ de funções (não necessariamente contínuas) mapeando entre $\mathbb{R}^d$ e $\mathbb{R}$, denota-se por $\overline{\mathcal{H}}$ o seu fecho com respeito à convergência compacta [^2].

**Proposição 3.3:**
Seja $\mathcal{H}$ um conjunto de funções de $\mathbb{R}^d$ para $\mathbb{R}$. Então, $\mathcal{H}$ é um aproximador universal de $C^0(\mathbb{R}^d)$ se e somente se $C^0(\mathbb{R}^d) \subseteq \overline{\mathcal{H}}$ [^2].

*Prova:*
Suponha que $\mathcal{H}$ é um aproximador universal e fixe $f \in C^0(\mathbb{R}^d)$. Para $n \in \mathbb{N}$, defina $K_n := [-n, n]^d \subseteq \mathbb{R}^d$. Então, para todo $n \in \mathbb{N}$, existe $f_n \in \mathcal{H}$ tal que $\sup_{x \in K_n} |f_n(x) - f(x)| < 1/n$. Como para todo compacto $K \subseteq \mathbb{R}^d$ existe $n_0$ tal que $K \subseteq K_n$ para todo $n \geq n_0$, tem-se que $f_n \xrightarrow{CC} f$. A parte "somente se" da afirmação é trivial. $\blacksquare$

**Teorema de Stone-Weierstrass (Teorema 3.4):**
Seja $K \subseteq \mathbb{R}^d$ compacto e seja $\mathcal{H} \subseteq C^0(K, \mathbb{R})$ tal que:
(a) para todo $x \in K$ existe $f \in \mathcal{H}$ tal que $f(x) \neq 0$;
(b) para todos $x \neq y \in K$ existe $f \in \mathcal{H}$ tal que $f(x) \neq f(y)$;
(c) $\mathcal{H}$ é uma álgebra de funções, i.e., $\mathcal{H}$ é fechado sob adição, multiplicação e multiplicação escalar.
Então, $\mathcal{H}$ é denso em $C^0(K)$ [^2].

**Exemplo 3.5:**
Os polinômios são densos em $C^0(\mathbb{R}^d)$ [^2].

**Redes Neurais *Shallow* (com uma camada oculta):**
Com o formalismo estabelecido, é possível demonstrar que redes neurais *shallow* de largura arbitrária formam um aproximador universal sob certas condições sobre a função de ativação [^3].

**Definição 3.6:**
Sejam $d, m, L, n \in \mathbb{N}$ e $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. O conjunto de todas as funções realizadas por redes neurais com entrada $d$-dimensional, saída $m$-dimensional, profundidade no máximo $L$, largura no máximo $n$ e função de ativação $\sigma$ é denotado por:

$$
\mathcal{N}(σ; L, n) := \{\Phi : \mathbb{R}^d \rightarrow \mathbb{R}^m \mid \Phi \text{ como na Def. 2.1, depth}(\Phi) \leq L, \text{ width}(\Phi) \leq n\}
$$

Além disso,

$$
\mathcal{N}(σ; L) := \bigcup_{n \in \mathbb{N}} \mathcal{N}(σ; L, n).
$$

A função de ativação $\sigma$ deve pertencer ao conjunto de funções contínuas por partes e localmente limitadas:

$$
\mathcal{M} := \{σ \in L_{loc}^{\infty}(\mathbb{R}) \mid \text{existem intervalos } I_1, ..., I_M \text{ particionando } \mathbb{R}, \text{ s.t. } σ \in C^0(I_j) \text{ para todo } j = 1, ..., M\}.
$$

**Exemplo 3.7:**
Funções de ativação pertencentes a $\mathcal{M}$ incluem, em particular, todas as funções contínuas não-polinomiais, como ReLU, SiLU e Sigmoid [^3].

**Teorema 3.8:**
Sejam $d \in \mathbb{N}$ e $σ \in \mathcal{M}$. Então $\mathcal{N}(\sigma; 1)$ é um aproximador universal de $C^0(\mathbb{R}^d)$ se e somente se $\sigma$ não é um polinômio [^3].

**Redes Neurais Profundas:**
O Teorema 3.8 demonstra a capacidade de aproximação universal de redes neurais com uma única camada oculta, desde que a função de ativação não seja um polinômio [^9]. Este resultado se estende diretamente para redes neurais de qualquer profundidade fixa $L > 1$ [^9]. A ideia é utilizar o fato de que a função identidade pode ser aproximada por uma rede neural *shallow* [^9]. Ao compor uma aproximação de rede neural *shallow* da função alvo $f$ com (múltiplas) redes neurais *shallow* que aproximam a função identidade, obtém-se uma aproximação de rede neural profunda de $f$ [^9].

**Proposição 3.16:**
Sejam $d, L \in \mathbb{N}$, seja $K \subseteq \mathbb{R}^d$ compacto e seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ tal que existe um conjunto aberto no qual $\sigma$ é diferenciável e não constante [^9]. Então, para todo $\epsilon > 0$, existe uma rede neural $\Phi \in \mathcal{N}(\sigma; L, d)$ tal que

$$
||\Phi(x) - x||_{\infty} < \epsilon \text{ para todo } x \in K.
$$

**Corolário 3.17:**
Sejam $d \in \mathbb{N}$, $L \in \mathbb{N}$ e $\sigma \in \mathcal{M}$. Então $\mathcal{N}(\sigma; L)$ é um aproximador universal de $C^0(\mathbb{R}^d)$ se e somente se $\sigma$ não é um polinômio [^10].

### Conclusão
Este capítulo explorou a capacidade de aproximação universal das redes neurais, demonstrando que tanto redes *shallow* quanto profundas podem aproximar qualquer função contínua em conjuntos compactos, desde que a função de ativação não seja um polinômio [^3, 9, 10]. A extensão do teorema da aproximação universal para redes profundas destaca a importância da profundidade na representação de funções complexas [^9]. Os resultados apresentados aqui estabelecem uma base teórica sólida para a aplicação de redes neurais em uma ampla gama de problemas [^1].

### Referências
[^1]: Chapter 3: Universal approximation
[^2]: Definition 3.2, Proposition 3.3, Theorem 3.4, Example 3.5
[^3]: Definition 3.6, Theorem 3.8, Example 3.7
[^9]: Section 3.1.3: Deep neural networks, Proposition 3.16
[^10]: Corollary 3.17
<!-- END -->