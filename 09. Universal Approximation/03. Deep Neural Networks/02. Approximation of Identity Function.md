## Aproximação da Função Identidade e Redes Neurais Profundas

### Introdução
Como vimos anteriormente, as redes neurais rasas (shallow neural networks) com funções de ativação apropriadas são aproximadores universais [^23]. Este capítulo explora como essa propriedade se estende a redes neurais profundas (deep neural networks) [^1]. A aproximação da função identidade desempenha um papel crucial nessa demonstração. Ao compor aproximações de redes neurais rasas da função alvo com aproximações da função identidade, podemos construir aproximações de redes neurais profundas. [^29]

### Conceitos Fundamentais

A ideia central é que, se podemos aproximar a função identidade arbitrariamente bem com uma rede neural rasa, podemos "inserir" essa aproximação em uma rede neural profunda existente sem alterar significativamente a saída. Isso nos permite aumentar a profundidade da rede sem sacrificar a precisão.

**Proposição 3.16** [^29] estabelece formalmente essa capacidade de aproximar a função identidade. Ela afirma que, dado um conjunto compacto $K \subseteq \mathbb{R}^d$ e uma função de ativação $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ que seja diferenciável e não constante em um conjunto aberto, então para cada $\epsilon > 0$, existe uma rede neural $\Phi \in N^d(\sigma; L, d)$ tal que $||\Phi(x) - x|| < \epsilon$ para todo $x \in K$.

A prova desta proposição [^29] utiliza uma ideia semelhante à utilizada no Lema 3.13 [^25], aproximando a derivada da função de ativação por uma rede neural simples. Inicialmente, assume-se que $d \in \mathbb{N}$ e $L = 1$. Seja $x^* \in \mathbb{R}$ tal que $\sigma$ seja diferenciável em uma vizinhança de $x^*$ e $\sigma'(x^*) \neq 0$. Além disso, seja $x^* = (x^*, ..., x^*) \in \mathbb{R}^d$. Então, para $\lambda > 0$, define-se:
$$
\Phi_\lambda(x) := \frac{\sigma(x_i/\lambda + x^*) - \sigma(x^*)}{\frac{\sigma'(x^*)}{\lambda}}
$$
Com isso, é possível demonstrar que, para todo $x \in K$:
$$
\Phi_\lambda(x) - x = \frac{\sigma(x_i/\lambda + x^*) - \sigma(x^*)}{\frac{\sigma'(x^*)}{\lambda}} - x
$$
Se $x_i = 0$ para $i \in \{1,...,d\}$, então $(\Phi_\lambda(x) - x)_i = 0$. Caso contrário:
$$
|(\Phi_\lambda(x) - x)_i| = \frac{|x_i||\sigma(x_i/\lambda + x^*) - \sigma(x^*)|}{|\sigma'(x^*)|x_i/\lambda}
$$
Pela definição da derivada, $|(\Phi_\lambda(x) - x)_i| \rightarrow 0$ para $\lambda \rightarrow \infty$ uniformemente para todo $x \in K$ e $i \in \{1,...,d\}$. Portanto, $|\Phi_\lambda(x) - x| \rightarrow 0$ para $\lambda \rightarrow \infty$ uniformemente para todo $x \in K$. A extensão para $L > 1$ é direta e é o conteúdo do Exercício 3.27 [^34].

**Corolário 3.17** [^30] formaliza a extensão do Teorema 3.8 [^23] para redes neurais profundas. Ele afirma que, se $\sigma \in M$ (o conjunto de funções piecewise contínuas e localmente limitadas) e $\sigma$ não é um polinômio, então $N^*(\sigma; L)$ é um aproximador universal de $C^0(\mathbb{R}^d)$, onde $L$ é a profundidade da rede.

A prova do Corolário 3.17 [^30] mostra que é possível aproximar qualquer função contínua em um conjunto compacto arbitrário, com precisão $\epsilon > 0$, utilizando uma rede neural profunda com profundidade $L > 1$. Isso é feito compondo uma aproximação rasa da função alvo (obtida pelo Teorema 3.8 [^23]) com uma aproximação da função identidade (obtida pela Proposição 3.16 [^29]).

### Conclusão
A capacidade de aproximar a função identidade com redes neurais rasas é um componente chave para demonstrar a propriedade de aproximação universal das redes neurais profundas [^1]. Essa técnica permite aumentar a profundidade da rede sem comprometer a precisão da aproximação, possibilitando a modelagem de funções complexas com maior eficiência.

### Referências
[^1]: Capítulo 3, "Universal approximation".
[^23]: Teorema 3.8
[^25]: Lema 3.13
[^29]: Proposição 3.16
[^30]: Corolário 3.17
[^34]: Exercício 3.27
<!-- END -->