## Relaxando as Condições para Aproximação Universal em Redes Neurais Profundas

### Introdução
Como vimos anteriormente no Capítulo 2, as redes neurais profundas são ferramentas poderosas para aproximar funções complexas. O Teorema da Aproximação Universal (TAU) estabelece que, sob certas condições, uma rede neural pode aproximar qualquer função contínua em um conjunto compacto [^3]. Este capítulo explora uma versão relaxada dessas condições, especificamente no contexto de redes neurais *profundas* [^9]. Em vez de exigir que a função de ativação seja não-polinomial, relaxamos essa condição para requerer apenas que a função de ativação seja diferenciável e não constante em um conjunto aberto. Essa abordagem permite provas construtivas que fornecem limites explícitos no tamanho da rede neural [^9].

### Conceitos Fundamentais
O Teorema da Aproximação Universal clássico (Theorem 3.8) demonstra a capacidade de aproximação de redes neurais com uma única camada oculta, utilizando funções de ativação não polinomiais [^3]. No entanto, o Proposition 3.16 introduz uma perspectiva diferente, focando na aproximação da função identidade, relaxando as condições sobre a função de ativação [^9].

**Proposition 3.16:** Seja $d, L \in \mathbb{N}$, seja $K \subseteq \mathbb{R}^d$ compacto, e seja $\sigma: \mathbb{R} \to \mathbb{R}$ tal que exista um conjunto aberto no qual $\sigma$ seja diferenciável e não constante. Então, para todo $\epsilon > 0$, existe uma rede neural $\Phi \in \mathcal{N}_d(\sigma; L, d)$ tal que
$$ ||\Phi(x) - x||_\infty < \epsilon \quad \text{para todo } x \in K. $$[^9]

Essa proposição é crucial porque demonstra que, mesmo com condições menos restritivas sobre a função de ativação, ainda é possível aproximar a função identidade arbitrariamente bem. A prova, como mencionado, usa a mesma ideia da Lemma 3.13, onde aproximamos a derivada da função de ativação por uma rede neural simples [^9].

A ideia central é utilizar essa capacidade de aproximar a identidade para construir redes neurais *profundas* que podem aproximar qualquer função contínua [^9]. A Corollary 3.17 formaliza essa ideia:

**Corollary 3.17:** Sejam $d \in \mathbb{N}, L \in \mathbb{N}$ e $\sigma \in \mathcal{M}$. Então $\mathcal{N}_d(\sigma; L)$ é um aproximador universal de $C^0(\mathbb{R}^d)$ se e somente se $\sigma$ não é um polinômio [^10].

A prova dessa Corollary utiliza a Proposition 3.16 para aproximar a função identidade e, em seguida, compõe essa aproximação com uma rede neural *rasa* (shallow), obtida pelo Teorema 3.8, para aproximar a função alvo [^10].

**Prova Construtiva e Limites Explícitos:**
A grande vantagem dessa abordagem é que ela permite provas construtivas. Ao invés de apenas demonstrar a existência de uma rede neural que aproxima a função desejada, a prova fornece um método explícito para construir essa rede [^9]. Isso, por sua vez, permite derivar limites explícitos sobre o tamanho da rede neural necessária para atingir uma dada precisão [^9].

### Conclusão
Ao relaxar as condições sobre a função de ativação e focar na aproximação da função identidade, este capítulo apresentou uma abordagem alternativa para demonstrar o Teorema da Aproximação Universal para redes neurais *profundas* [^9]. Essa abordagem não apenas fornece uma prova mais flexível, mas também abre caminho para provas construtivas que permitem derivar limites explícitos sobre o tamanho da rede neural necessária para atingir uma dada precisão [^9]. Essa é uma contribuição significativa para a teoria das redes neurais, pois fornece *insights* práticos sobre como projetar e treinar redes neurais eficientes.

### Referências
[^3]: Capítulo 3: Universal approximation.
[^9]: Seção 3.1.3: Deep neural networks.
[^10]: Corollary 3.17.

<!-- END -->