## Redes Neurais Rasas e Aproximação Universal

### Introdução
Em continuidade ao Capítulo 2, que introduziu redes neurais, e ao início deste capítulo que estabeleceu o teorema da aproximação universal, esta seção aprofunda a capacidade de aproximação de **redes neurais rasas** (shallow neural networks). Anteriormente, vimos que redes neurais são ferramentas poderosas para modelar relações complexas. Agora, exploraremos em detalhes como redes neurais rasas, com uma única camada oculta e largura arbitrária, podem atuar como **aproximadores universais** sob certas condições.

### Conceitos Fundamentais

#### Teorema da Aproximação Universal para Redes Rasas
O **teorema da aproximação universal** para redes rasas afirma que, sob condições brandas na função de ativação, uma rede neural rasa pode aproximar qualquer função contínua com um grau de precisão desejado. Mais precisamente:

> Redes neurais rasas, caracterizadas por uma única camada oculta com largura arbitrária e sob certas condições brandas na função de ativação, podem servir como aproximadores universais.

Isso significa que, dada uma função contínua $f: \mathbb{R}^d \rightarrow \mathbb{R}$, um conjunto compacto $K \subseteq \mathbb{R}^d$, e um $\epsilon > 0$, existe uma rede neural rasa $g$ tal que:

$$\
\sup_{x \in K} |f(x) - g(x)| < \epsilon
$$

A chave para este teorema reside na escolha da **função de ativação**.

#### Conjunto de Funções de Ativação Adequadas (M)
Para que uma rede neural rasa seja um aproximador universal, sua função de ativação deve pertencer a um conjunto específico de funções, denotado por $M$. Este conjunto é definido como:

> O conjunto M abrange funções *piecewise continuous* e *locally bounded*, que servem como funções de ativação para redes neurais rasas, garantindo a propriedade de aproximação universal.

Formalmente:
$$\
M := \{\sigma \in L_{loc}(\mathbb{R}) \mid \exists \text{ intervalos } I_1, ..., I_M \text{ particionando } \mathbb{R}, \text{ s.t. } \sigma \in C^0(I_j) \text{ para todo } j = 1, ..., M \}
$$

Onde:
*   $L_{loc}(\mathbb{R})$ representa o espaço das funções localmente integráveis de Lebesgue em $\mathbb{R}$.
*   $C^0(I_j)$ representa o espaço das funções contínuas no intervalo $I_j$.
*   $M \in \mathbb{N}$ é finito, significando que a função de ativação pode ter um número finito de descontinuidades.
*   Os intervalos $I_j$ têm medida de Lebesgue positiva, excluindo casos triviais como pontos isolados.

#### Exemplos de Funções de Ativação Adequadas
Funções de ativação que pertencem ao conjunto $M$ e, portanto, são adequadas para aproximação universal em redes rasas incluem:

*   **ReLU (Rectified Linear Unit)**: $\sigma(x) = \max(0, x)$
*   **SiLU (Sigmoid Linear Unit)**
*   **Sigmoid**: $\sigma(x) = \frac{1}{1 + e^{-x}}$
*   **Heaviside**: $\sigma(x) = 1_{x>0}$

Essas funções são contínuas e não polinomiais ou piecewise continuous. Funções como $1/x$, que não são localmente limitadas, são excluídas.

#### Importância da Não-Polinomialidade
Um requisito crucial para a função de ativação é que ela não seja um polinômio. Se a função de ativação fosse um polinômio, a rede neural rasa seria limitada em sua capacidade de aproximar funções complexas, pois essencialmente estaria combinando polinômios, resultando em outro polinômio. A não-polinomialidade permite que a rede capture relações não lineares e, portanto, atinja a aproximação universal.

#### Theorem 3.8
O texto referencia o Teorema 3.8 que formaliza a condição para aproximação universal em redes rasas:

> Seja $d \in \mathbb{N}$ e $\sigma \in M$. Então $N_1(\sigma;1)$ é um aproximador universal de $C^0(\mathbb{R}^d)$ se e somente se $\sigma$ não é um polinômio.

Onde $N_1(\sigma;1)$ representa o conjunto de funções realizadas por redes neurais com entrada $d$-dimensional, uma camada oculta, e função de ativação $\sigma$.

#### Theorem 3.4 (Stone-Weierstrass)
O texto também cita o Teorema de Stone-Weierstrass, uma ferramenta chave para demonstrar a propriedade de aproximação universal. Este teorema estabelece condições sob as quais uma álgebra de funções contínuas em um conjunto compacto é densa no espaço de todas as funções contínuas nesse conjunto.

### Conclusão
Em resumo, redes neurais rasas, equipadas com funções de ativação adequadas (piecewise continuous, locally bounded e não polinomiais), possuem a capacidade notável de aproximar qualquer função contínua em conjuntos compactos. Este resultado fundamental, conhecido como o teorema da aproximação universal, sublinha o poder inerente das redes neurais como ferramentas de modelagem versáteis.

### Referências
[^1]: Capítulo 3, página 21
[^2]: Capítulo 3, página 22
[^3]: Capítulo 3, página 23
<!-- END -->