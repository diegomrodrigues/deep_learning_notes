{
  "topics": [
    {
      "topic": "Adversarial Examples: Definition and Significance",
      "sub_topics": [
        "Adversarial examples are inputs to a neural network intentionally modified to cause incorrect outputs, revealing the sensitivity of these networks to small perturbations and raising concerns about their reliability and security. These examples are created by adding often imperceptible perturbations to original inputs, leading trained neural networks to misclassify them with high confidence, even if the original input was correctly classified. The existence of adversarial examples indicates potential unreliability and security vulnerabilities exploitable by malicious actors, posing a significant risk in applications with critical consequences.",
        "The formal definition of an adversarial example involves assigning a label y to a vector x, where the relation between x and y is described by a distribution D. A ground-truth classifier g is introduced, representing how a human would classify data, including a 'nonrelevant' class for nonsensical inputs, differing from a minimizer of the Bayes risk. An adversarial example x' for an input x with perturbation \u03b4 must satisfy three conditions: the distance between x and x' is at most \u03b4, both x and x' belong to the same class according to the ground truth classifier, and the classifier h correctly classifies x but misclassifies x'.",
        "A Bayes classifier minimizes the Bayes risk and coincides with the ground truth g on the feature support Dx. If a classifier h is a Bayes classifier and the data distribution exhausts the domain, then no adversarial examples exist, as every point is either in the nonrelevant class or classified correctly by both h and g. However, even if h is a Bayes classifier, adversarial examples can exist if the distribution does not exhaust the domain, underscoring the vulnerability of classifiers to out-of-distribution inputs. If h is not a Bayes classifier and the distribution is exhaustive, adversarial examples in the feature support can exist if the distance between certain subsets of Dx is smaller than \u03b4, where \u03b4 is the perturbation size. If h is not a Bayes classifier and the distribution is non-exhaustive, anything is possible, meaning that data points and their associated adversarial examples can appear in the feature support, or adversarial examples can be created by leaving the feature support.",
        "For linear classifiers, high input dimensionality can cause adversarial examples because small perturbations can significantly alter the classifier's output. A sufficient condition for the existence of adversarial examples in linear classifiers is that the decision margin of x for the ground truth g is larger than that for the classifier h, indicating a misalignment between the two classifiers. The term (w transpose w)/(||w|| ||W||) describes the alignment of the two classifiers; if the classifiers are not aligned, i.e., w and w have a large angle between them, then adversarial examples exist even if the margin of the classifier is larger than that of the ground-truth classifier. Adversarial examples with small perturbation are possible if |w transpose x| < ||w||, meaning that x lies close to the decision boundary of h.",
        "ReLU neural networks are also vulnerable to adversarial examples, and the arguments for affine classifiers can be applied to the affine pieces of the ReLU network to demonstrate the existence of such examples. The geometric margin of the ground-truth classifier \u03bcg(x) is the distance from x to the closest element that is classified differently from x; a larger geometric margin makes it easier to construct adversarial examples. The distance to the next affine piece \u03bd\u03a6(x) is the distance from x to the closest adjacent affine piece in the ReLU network; a larger distance allows for more perturbation within that piece. The perturbation \u03b4 for ReLU networks depends on the classification margin |\u03a6(x)| and the sensitivity to inputs ||\u2207\u03a6(x)||; smaller margins and higher sensitivity facilitate adversarial examples."
      ]
    },
    {
      "topic": "Robustness Against Adversarial Examples",
      "sub_topics": [
        "Global Lipschitz regularity, which bounds the rate of change of a function, can be used to prevent adversarial examples; classifiers with smaller Lipschitz constants are less sensitive to small perturbations in the input. Proposition 16.10 states that if a classifier \u03a6: Rd \u2192 R is CL-Lipschitz with CL > 0 and satisfies \u03a6(x)g(x) \u2265 s for some s > 0, then there does not exist an adversarial example to x of perturbation \u03b4 < s/CL. Bounding the Lipschitz constant of the inner functions of a classifier can be an effective measure against adversarial examples; however, this may also restrict the capabilities of the neural network.",
        "Local Lipschitz bounds can also be used to prevent adversarial examples; these bounds are often tighter than global Lipschitz bounds and may be more practical for deep neural networks. An algorithm can be used to compute bounds on the local Lipschitz constant, allowing for a more practical assessment of robustness against adversarial examples. Theorem 16.12 presents a result that shows, under certain assumptions on the training set, there exists a ReLU neural network that classifies the training set correctly but does not allow for adversarial examples within the training set."
      ]
    }
  ]
}