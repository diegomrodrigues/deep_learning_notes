## Generalização no Regime de Sobreparametrização e o Fenômeno da Dupla Descida

### Introdução
No capítulo anterior, a teoria da generalização para redes neurais profundas treinadas minimizando o risco empírico foi discutida, destacando a importância de escolher uma arquitetura com um número moderado de parâmetros em relação ao número de amostras de treinamento [^p1]. Observou-se também que o melhor desempenho pode ser alcançado ao equilibrar os erros de generalização e aproximação, minimizando sua soma [^p1]. No entanto, a aplicação prática do aprendizado profundo frequentemente opera em um regime significativamente diferente do analisado anteriormente, levantando a questão de por que esses métodos ainda funcionam efetivamente [^p2]. Este capítulo explora o fenômeno da dupla descida, que desafia as noções tradicionais de que mais parâmetros sempre levam a um *overfitting*, e investiga como a sobreparametrização pode, na verdade, melhorar o desempenho da generalização [^p2].

### Conceitos Fundamentais
#### 15.1 O Fenômeno da Dupla Descida
O sucesso do aprendizado profundo em um regime não coberto pela teoria estatística tradicional intrigou pesquisadores por algum tempo [^p2]. Experimentos indicam que, enquanto o risco segue o limite superior da Seção 14.6 para arquiteturas que não interpolam os dados, a curva não se expande para o infinito como sugerido na Figura 14.4 [^p2]. Em vez disso, após ultrapassar o chamado *limiar de interpolação*, o risco começa a diminuir novamente [^p2]. Esse comportamento, conhecido como **dupla descida**, é ilustrado na Figura 15.2 [^p2].

A Figura 15.2 divide o comportamento do modelo em três regimes distintos [^p2]:
*   **Regime Clássico (Underparameterized):** O modelo é subparametrizado, levando a *underfitting* e alto viés. O modelo não consegue capturar os padrões subjacentes nos dados, resultando em um risco empírico mínimo não nulo.
*   **Limiar de Interpolação:** À medida que a complexidade do modelo aumenta, ele se aproxima do limiar de interpolação, onde pode ajustar perfeitamente os dados de treinamento (risco empírico zero). No entanto, isso frequentemente leva ao *overfitting*, onde o modelo tem um desempenho ruim em dados não vistos devido à alta variância.
*   **Regime Moderno (Overparameterized):** O modelo é sobreparametrizado, e o erro de teste começa a diminuir novamente, indicando que a sobreparametrização pode levar a um melhor desempenho de generalização.

#### 15.1.1 Regressão por Mínimos Quadrados Revisitada
Para obter mais *insights*, considere a regressão por mínimos quadrados (kernel) introduzida na Seção 11.2 [^p2]. Considere uma amostra de dados $(x_j, y_j)_{j=1}^{m} \subset \mathbb{R}^d \times \mathbb{R}$ gerada por alguma função *ground-truth* $f$, isto é, $y_j = f(x_j)$ para $j = 1, ..., m$ [^p2]. Seja $\phi_j: \mathbb{R}^d \rightarrow \mathbb{R}, j \in \mathbb{N}$, uma sequência de funções *ansatz*. Para $n \in \mathbb{N}$, deseja-se ajustar uma função $\sum_{i=1}^{n} w_i \phi_i(x)$ aos dados usando mínimos quadrados lineares [^p2]. Para tanto, introduz-se o *feature map* [^p2]:

$$\
\mathbb{R}^d \ni x \mapsto \Phi(x) := (\phi_1(x), ..., \phi_n(x))^T \in \mathbb{R}^n.
$$

O objetivo é determinar os coeficientes $w \in \mathbb{R}^n$ minimizando o risco empírico [^p3]:

$$\
R_S(w) = \frac{1}{m} \sum_{j=1}^{m} \left( \sum_{i=1}^{n} w_i \phi_i(x_j) - y_j \right)^2 = \frac{1}{m} \sum_{j=1}^{m} \left( \langle \Phi(x_j), w \rangle - y_j \right)^2.
$$

Com [^p3]

$$\
A_n :=
\begin{pmatrix}
\phi_1(x_1) & \cdots & \phi_n(x_1) \\\\
\vdots & & \vdots \\\\
\phi_1(x_m) & \cdots & \phi_n(x_m)
\end{pmatrix}
=
\begin{pmatrix}
\Phi(x_1)^T \\\\
\vdots \\\\
\Phi(x_m)^T
\end{pmatrix}
\in \mathbb{R}^{m \times n}
$$

e $y = (y_1, ..., y_m)$, tem-se [^p3]

$$\
R_S(w) = \frac{1}{m} ||A_n w - y||^2.
$$

Conforme discutido nas Seções 11.1-11.2, um minimizador único de $R_S(w)$ existe apenas se $A_n$ tiver *rank* $n$ [^p3]. Para um minimizador $w_n$, a função ajustada é dada por [^p3]:

$$\
f_n(x) := \sum_{j=1}^{n} w_{n,j} \phi_j(x).
$$

O interesse reside no comportamento de $f_n$ em função de $n$ (o número de funções *ansatz*/parâmetros do modelo), distinguindo entre dois casos [^p3]:

*   **Subparametrizado (Underparameterized):** Se $n < m$, há menos parâmetros $n$ do que pontos de treinamento $m$ [^p3]. Para o problema de mínimos quadrados de minimizar $R_S$, isso significa que há mais condições $m$ do que parâmetros livres $n$ [^p3]. Assim, em geral, não é possível interpolar os dados, e tem-se $\min_{w \in \mathbb{R}^n} R_S(w) > 0$ [^p3].

*   **Sobreparametrizado (Overparameterized):** Se $n \geq m$, então há pelo menos tantos parâmetros $n$ quanto pontos de treinamento $m$ [^p3]. Se os $x_j$ e os $\phi_j$ são tais que $A_n \in \mathbb{R}^{m \times n}$ tem *rank* completo $m$, então existe $w$ tal que $R_S(w) = 0$ [^p3]. Se $n > m$, então $A_n$ necessariamente tem um *kernel* não trivial, e existem infinitas escolhas de parâmetros $w$ que resultam em risco empírico zero $R_S$ [^p3]. Alguns deles levam a funções de predição melhores, e outros levam a piores [^p3].

No caso sobreparametrizado, existem muitos minimizadores de $R_S$ [^p3]. O algoritmo de treinamento usado para computar um minimizador determina o tipo de função de predição $f_n$ obtida [^p3]. Para observar a dupla descida, ou seja, para alcançar uma boa generalização para $n$ grande, é necessário escolher o minimizador cuidadosamente [^p3]. A seguir, considera-se o minimizador único de norma-2 mínima, definido como [^p3]:

$$\
w_{n,*} = \left( \arg \min_{\\{ w \in \mathbb{R}^n \mid R_S(w) \leq R_S(v) \\ \forall v \in \mathbb{R}^n \\}} ||w|| \right) \in \mathbb{R}^n.
$$

#### 15.1.2 Um Exemplo
Considere um exemplo concreto [^p3]. A Figura 15.3 mostra um conjunto de 40 funções *ansatz* $\phi_1, ..., \phi_{40}$, que são extraídas de um processo Gaussiano [^p3]. Adicionalmente, a figura mostra um gráfico da função de Runge $f$, e $m = 18$ pontos equidistantes que são usados como pontos de dados de treinamento [^p3]. Em seguida, ajusta-se uma função no espaço gerado por $\\{\phi_1, ..., \phi_n\\}$ via (15.1.5) e (15.1.4) [^p3]. O resultado é exibido na Figura 15.4 [^p3].

A análise do exemplo revela [^p4]:
*   **n = 2:** O modelo só pode representar funções em span{$\phi_1, \phi_2$}. Ainda não é expressivo o suficiente para fornecer uma aproximação significativa de *f* [^p4].
*   **n = 15:** O modelo tem expressividade suficiente para capturar as principais características de *f*. Como *n* = 15 < 18 = *m*, ainda não é capaz de interpolar os dados. Assim, permite alcançar um bom equilíbrio entre o erro de aproximação e o erro de generalização, o que corresponde ao cenário discutido no Capítulo 14 [^p4].
*   **n = 18:** Está-se no limiar de interpolação. O modelo é capaz de interpolar os dados, e existe um *w* único tal que $R_S(w)$ = 0. No entanto, entre os pontos de dados, o comportamento do preditor $f_{18}$ parece errático e exibe fortes oscilações. Isso é conhecido como *overfitting* e é esperado devido à análise no Capítulo 14; embora o erro de aproximação nos pontos de dados tenha melhorado em comparação com o caso *n* = 15, o erro de generalização piorou [^p4].
*   **n = 40:** Este é o regime sobreparametrizado, onde há significativamente mais parâmetros do que pontos de dados. A predição $f_{40}$ interpola os dados e parece ser a melhor aproximação geral para *f* até agora, devido a uma "boa" escolha do minimizador de $R_S$, nomeadamente (15.1.5). Também é notado que, embora muito bom, o ajuste não é perfeito. Não se pode esperar uma melhoria significativa no desempenho aumentando ainda mais *n*, uma vez que neste ponto o principal fator limitante é a quantidade de dados disponíveis. Veja também a Figura 15.5 (a) [^p4].

A Figura 15.5 (a) exibe o erro $||f - f_n||_{L^2([-1,1])}$ sobre *n* [^p4]. Observa-se a característica curva de dupla descida, onde o erro inicialmente diminui, após atingir o pico no limiar de interpolação, que é marcado pela linha vermelha tracejada [^p4]. Posteriormente, no regime sobreparametrizado, ele começa a diminuir novamente [^p4]. A Figura 15.5 (b) exibe $||w_{n,*}||$ [^p4]. Note como a norma Euclidiana do vetor de coeficiente também atinge o pico no limiar de interpolação [^p4].

É enfatizado que a natureza precisa das curvas de convergência depende fortemente de vários fatores, como a distribuição e o número de pontos de treinamento *m*, a *ground truth* *f* e a escolha das funções *ansatz* $\phi_j$ (e.g., o kernel específico usado para gerar os $\phi_j$ na Figura 15.3 (a)) [^p4]. Na presente configuração, alcança-se uma boa aproximação de *f* para *n* = 15 < 18 = *m* correspondendo ao regime onde os erros de aproximação e interpolação são equilibrados [^p4]. No entanto, como a Figura 15.5 (a) mostra, pode ser difícil determinar um valor adequado de *n* < *m* *a priori*, e a faixa aceitável de valores de *n* pode ser bastante estreita [^p6]. Para sobreparametrização (*n* ≫ *m*), a escolha precisa de *n* é menos crítica, potencialmente tornando o algoritmo mais estável neste regime [^p6]. O leitor é encorajado a conduzir experimentos semelhantes e explorar diferentes configurações para obter uma melhor sensação do fenômeno da dupla descida [^p6].

#### 15.2 Tamanho dos Pesos
Na Figura 15.5, observou-se que a norma dos coeficientes $||w_{n,*}||$ exibe um comportamento semelhante ao erro $L^2$, atingindo o pico no limiar de interpolação *n* = 18 [^p6]. No aprendizado de máquina, grandes pesos são geralmente indesejáveis, pois estão associados a grandes derivadas ou comportamento oscilatório [^p6]. Isso é evidente no exemplo mostrado na Figura 15.4 para *n* = 18 [^p6]. Assumindo que os dados em (15.1.1) foram gerados por uma função "suave" *f*, e.g., uma função com constante de Lipschitz moderada, essas grandes derivadas da função de predição podem levar a uma generalização pobre [^p6]. É importante notar que tal suposição de suavidade sobre *f* pode ou não ser satisfeita [^p6]. No entanto, se *f* não é suave, há pouca esperança de recuperar *f* com precisão a partir de dados limitados (veja a discussão na Seção 9.2) [^p6]. O próximo resultado fornece uma explicação para o comportamento observado de $||w_{n,*}||$ [^p6].

**Proposição 15.1.** Assuma que $x_1, ..., x_m$ e os $(\phi_j)_{j \in \mathbb{N}}$ são tais que $A_n$ em (15.1.2) tem *rank* completo *n* para todo *n* ≤ *m* [^p6]. Dado $y \in \mathbb{R}^m$, denote por $w_{n,*}(y)$ o vetor em (15.1.5) [^p6]. Então [^p6]:

$$\
\sup_{||y||=1} ||w_{n,*}(y)|| \text{ é monotonicamente }
\begin{cases}
\text{crescente para } n < m, \\\\
\text{decrescente para } n \geq m.
\end{cases}
$$

*Prova.* Começa-se com o caso *n* ≥ *m* [^p6]. Por suposição, $A_m$ tem *rank* completo *m*, e assim $A_n$ tem *rank* *m* para todo *n* ≥ *m*, veja (15.1.2) [^p6]. Em particular, existe $w_n \in \mathbb{R}^n$ tal que $A_n w_n = y$ [^p6]. Agora fixe $y \in \mathbb{R}^m$ e seja $w_n$ qualquer vetor desses [^p7]. Então $w_{n+1} := (w_n, 0) \in \mathbb{R}^{n+1}$ satisfaz $A_{n+1} w_{n+1} = y$ e $||w_{n+1}|| = ||w_n||$ [^p7]. Assim, necessariamente $||w_{n+1,*}|| \leq ||w_{n,*}||$ para as soluções de norma mínima definidas em (15.1.5) [^p7]. Como isso vale para todo *y*, obtém-se a afirmação para *n* ≥ *m* [^p7].

Agora seja *n* < *m* [^p7]. Recorde que a solução de norma mínima pode ser escrita através da pseudo inversa [^p7]:

$$\
w_{n,*}(y) = A_n^+ y,
$$

veja, por exemplo, o Exercício 11.32 [^p7]. Aqui [^p7],

$$\
A_n^+ = V_n
\begin{pmatrix}
\sigma_{n,1}^{-1} & & 0 \\\\
& \ddots & \\\\
0 & & \sigma_{n,n}^{-1} \\\\
0 & \cdots & 0
\end{pmatrix}
U_n^T \in \mathbb{R}^{n \times m},
$$

onde $A_n = U_n \Sigma_n V_n^T$ é a decomposição em valores singulares de $A_n$, e [^p7]

$$\
\Sigma_n =
\begin{pmatrix}
\sigma_{n,1} & & & & 0 \\\\
& \ddots & & & \\\\
& & \sigma_{n,n} & & \\\\
& & & 0 & \\\\
0 & & & & 0
\end{pmatrix}
\in \mathbb{R}^{m \times n}
$$

contém os valores singulares $\sigma_{n,1} \geq ... \geq \sigma_{n,n} > 0$ de $A_n \in \mathbb{R}^{m \times n}$ ordenados por tamanho decrescente [^p7]. Como $V_n \in \mathbb{R}^{n \times n}$ e $U_n \in \mathbb{R}^{m \times m}$ são matrizes ortogonais, tem-se [^p7]:

$$\
\sup_{||y||=1} ||w_{n,*}(y)|| = \sup_{||y||=1} ||A_n^+ y|| = \sigma_{n,n}^{-1}.
$$

Finalmente, como o menor valor singular $\sigma_{n,n}$ de $A_n$ pode ser escrito como [^p7]:

$$\
\sigma_{n,n} = \inf_{\substack{x \in \mathbb{R}^n \\\\ ||x||=1}} ||A_n x|| > \inf_{\substack{x \in \mathbb{R}^{n+1} \\\\ ||x||=1}} ||A_{n+1} x|| = \sigma_{n+1,n+1},
$$

observa-se que $n \rightarrow \sigma_{n,n}$ é monotonicamente decrescente para *n* ≤ *m* [^p7]. Isso conclui a prova [^p7]. $\blacksquare$

#### 15.3 Justificativa Teórica
Agora, examine uma possível explicação para o fenômeno da dupla descida para redes neurais [^p7]. Embora existam muitos argumentos alternativos disponíveis na literatura (veja a seção de bibliografia), a explicação apresentada aqui é baseada em uma simplificação das ideias em [12] [^p7].

A principal suposição subjacente à análise é que grandes redes neurais sobreparametrizadas tendem a ser contínuas de Lipschitz com uma constante de Lipschitz independente do tamanho [^p7]. Isso é uma consequência de redes neurais tipicamente terem pesos relativamente pequenos [^p7]. Para motivar isso, considere a classe de redes neurais $\mathcal{N}(\sigma; A, B)$ para uma arquitetura $A$ de profundidade $d \in \mathbb{N}$ e largura $L \in \mathbb{N}$ [^p7]. Se $\sigma$ é $C_0$-Lipschitz contínua tal que $B \leq c_B \cdot (dC_0)^{-1}$ para algum $c_B > 0$, então pelo Lema 13.2 [^p7]:

$$\
\mathcal{N}(\sigma; A, B) \subseteq \text{Lip}(c_B).
$$

Uma suposição do tipo $B \leq c_B \cdot (dC_0)^{-1}$, i.e., um escalonamento dos pesos pelo recíproco 1/d da largura, não é desarrazoado na prática [^p8]: Esquemas de inicialização padrão, como LeCun [127] ou He [85] inicialização, usam pesos aleatórios com variância escalonada inversamente proporcional à dimensão de entrada de cada camada [^p8]. Além disso, como visto no Capítulo 11, para redes neurais muito amplas, os pesos não se movem significativamente de sua inicialização durante o treinamento [^p8]. Adicionalmente, muitas rotinas de treinamento usam termos de regularização nos pesos, incentivando assim a rotina de otimização a encontrar pesos pequenos [^p8].

O estudo da capacidade de generalização de funções de Lipschitz é feito através dos resultados de aprendizado baseados em número de cobertura do Capítulo 14 [^p8]. O conjunto de funções C-Lipschitz em um domínio Euclidiano compacto d-dimensional Lip(C) tem números de cobertura limitados de acordo com [^p8]:

$$\
\log(\mathcal{G}(\text{Lip}(C), \epsilon, L^{\infty})) \leq C_{cov} \cdot \left( \frac{1}{\epsilon} \right)^{\frac{d}{2}} \text{ para todo } \epsilon > 0
$$

para alguma constante $C_{cov}$ independente de $\epsilon > 0$ [^p8]. Uma prova pode ser encontrada em [74, Lema 7], veja também [229] [^p8]. Como resultado dessas considerações, dois regimes podem ser identificados [^p8]:

*   **Regime padrão:** Para tamanho de rede pequeno $n_A$, considera-se redes neurais como um conjunto parametrizado por $n_A$ parâmetros [^p8]. Como visto antes, isso produz um limite no erro de generalização que escala linearmente com $n_A$ [^p8]. Contanto que $n_A$ seja pequeno em comparação com o número de amostras, espera-se uma boa generalização pelo Teorema 14.15 [^p8].

*   **Regime sobreparametrizado:** Para tamanho de rede grande $n_A$ mas pesos pequenos como descrito antes, considera-se redes neurais como um subconjunto de Lip(C) para uma constante $C > 0$ [^p8]. Este conjunto tem um limite de número de cobertura que é independente do número de parâmetros $n_A$ [^p8].

Escolhendo o melhor dos dois limites de generalização para cada regime, obtém-se o seguinte resultado [^p8]. Recorde que $\mathcal{N}^*(\sigma; A, B)$ denota todas as redes em $\mathcal{N}(\sigma; A, B)$ com um alcance contido em [-1,1] (veja (14.5.1)) [^p8].

**Teorema 15.2.** Sejam $C, C_L > 0$ e seja $L: [-1,1] \times [-1,1] \rightarrow \mathbb{R}$ seja $C_L$-Lipschitz [^p8]. Além disso, sejam $A = (d_0, d_1, ..., d_{L+1}) \in \mathbb{N}^{L+2}$, seja $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ seja $C_0$-Lipschitz contínua com $C_0 > 1$, e $|\sigma(x)| \leq C_0 |x|$ para todo $x \in \mathbb{R}$, e seja $B > 0$ [^p8].

Então, existem $C_1, C_2 > 0$, tal que para todo $m \in \mathbb{N}$ e toda distribuição $D$ em $[-1,1]^{d_0} \times [-1,1]$ vale com probabilidade de pelo menos $1 - \delta$ sobre $S \sim D^m$ que para todo $\Phi \in \mathcal{N}^*(\sigma; A, B) \cap \text{Lip}(C)$ [^p8]

$$\
|\mathcal{R}(\Phi) - \hat{\mathcal{R}}_S(\Phi)| \leq g(A, C, B, m) + 4C_C \sqrt{\frac{\log(4/\delta)}{m}},
$$

onde [^p8]

$$\
g(A, C, B, m) = \min \\{ \kappa_{NN}(A, m; c_1), \kappa_{\text{Lip}}(A, m; c_2) \\},
$$

$$\
\kappa_{NN}(A, m; c_1) = c_1 \frac{n_A \log(n_A \sqrt{m}) + Ln_A \log(d_{\max})}{m},
$$

$$\
\kappa_{\text{Lip}}(A, m; c_2) = c_2 m^{-\frac{2}{2 + d_0}}.
$$

*Prova.* Aplicando o Teorema 14.11 com $\alpha = 1/(2 + d_0)$ e (15.3.2), obtém-se que com probabilidade de pelo menos $1 - \delta/2$ vale para todo $\Phi \in \text{Lip}(C)$ [^p9]

$$\
|\mathcal{R}(\Phi) - \hat{\mathcal{R}}_S(\Phi)| \leq 4C_C \sqrt{\frac{C_{cov} (m \epsilon^2)^{d_0} + \log(4/\delta)}{m}} \leq 4C_C \sqrt{\frac{C_{cov} C^{d_0} m^{\frac{d_0}{2+d_0}} + \log(4/\delta)}{m}}
$$

$$\
\leq 4C_C \sqrt{\frac{C_{cov} C^{d_0} (m^{d_0/(d_0+2)} - 1)}{m}} + 4C_C \sqrt{\frac{\log(4/\delta)}{m}} = 4C_C \sqrt{\frac{C_{cov} C^{d_0} m^{\frac{-2}{d_0+2}} + \log(4/\delta)}{m}}
$$

$$\
\leq \frac{4 C_C C_{cov} C^{d_0}}{m^{\frac{2}{d_0+2}}} + \frac{4 C_C \log(4/\delta)}{m}.
$$

Adicionalmente, o Teorema 14.15 produz que com probabilidade de pelo menos $1 - \delta/2$ vale para todo $\Phi \in \mathcal{N}^*(\sigma; A, B)$ [^p9]

$$\
|\mathcal{R}(\Phi) - \hat{\mathcal{R}}_S(\Phi)| \leq 4C_C \sqrt{\frac{n_A \log(n_A \sqrt{m}) + L n_A \log(2 C_0 B d_{\max}) + \log(4/\delta)}{m}}
$$

$$\
\leq \frac{2 C_C}{\sqrt{m}} \sqrt{n_A \log(n_A \sqrt{m}) + L n_A \log(2 C_0 B d_{\max})} + \frac{\log(4/\delta)}{m}
$$

Então, para $\Phi \in \mathcal{N}^*(\sigma; A, B) \cap \text{Lip}(C)$, o mínimo de ambos os limites superiores vale com probabilidade de pelo menos $1 - \delta$ [^p9]. $\blacksquare$

Os dois regimes no Teorema 15.2 correspondem aos dois termos que compreendem o mínimo na definição de $g(A, C_0, B, m)$ [^p9]. O primeiro termo aumenta com $n_A$ enquanto o segundo é constante [^p9]. No primeiro regime, onde o primeiro termo é menor, a lacuna de generalização $|\mathcal{R}(\Phi) - \hat{\mathcal{R}}_S(\Phi)|$ aumenta com $n_A$ [^p9].

No segundo regime, onde o segundo termo é menor, a lacuna de generalização é constante com $n_A$ [^p9]. Além disso, é razoável assumir que o risco empírico $\hat{\mathcal{R}}_S$ diminuirá com o aumento do número de parâmetros $n_A$ [^p9].

Por (15.3.3), o risco pode ser limitado por [^p9]

$$\
\mathcal{R}(\Phi) \leq \hat{\mathcal{R}}_S + g(A, C, B, m) + 4 C_C \sqrt{\frac{\log(4/\delta)}{m}}.
$$

No segundo regime, este limite superior é monotonicamente decrescente [^p9]. No primeiro regime, pode diminuir e aumentar [^p9]. Em alguns casos, este comportamento pode levar a um limite superior no risco que se assemelha à curva da Figura 15.2 [^p9]. A seção a seguir descreve um cenário específico onde este é o caso [^p9].

**Observação 15.3.** O Teorema 15.2 assume a continuidade $C$-Lipschitz das redes neurais [^p9]. Como visto nas Seções 15.1.2 e 15.2, esta suposição pode não valer perto do limiar de interpolação [^p9]. Portanto, o Teorema 15.2 provavelmente fornece um limite superior muito otimista perto do limiar de interpolação [^p9].

#### 15.4 Dupla Descida para Aprendizado de Redes Neurais
Agora, entenda o fenômeno da dupla descida no contexto do Teorema 15.2 [^p10]. Algumas suposições simplificadoras são feitas para obter uma fórmula para um limite superior no risco [^p10]. Primeiro, assume-se que os dados $S = (x_i, y_i)_{i=1}^{m} \in \mathbb{R}^{d_0} \times \mathbb{R}$ derivam de uma função contínua $C_M$-Lipschitz [^p10]. Além disso, fixa-se uma profundidade $L \in \mathbb{N}$ e considera-se, para $d \in \mathbb{N}$, arquiteturas da forma $(\sigma_{\text{ReLU}}; A_d)$, onde [^p10]

$$\
A_d = (d_0, d, ..., d, 1).
$$

Para esta arquitetura, o número de parâmetros é limitado por [^p10]

$$\
n_{A_d} = (d_0 + 1)d + (L - 1)(d + 1)d + d + 1.
$$

Para derivar um limite superior no risco, começa-se limitando superiormente o risco empírico e, em seguida, aplicando o Teorema 15.2 para estabelecer um limite superior na lacuna de generalização [^p10]. Em combinação, essas estimativas fornecem um limite superior no risco [^p10]. Em seguida, observa-se que este limite superior segue a curva de dupla descida na Figura 15.2 [^p10].

#### 15.4.1 Limite Superior no Risco Empírico
Estabelece-se um limite superior em $\hat{\mathcal{R}}_S(\Phi)$ para $\Phi \in \mathcal{N}^*(\sigma_{\text{ReLU}}; A_d, B) \cap \text{Lip}(C_M)$ [^p10]. Para $B > C_M$, pode-se aplicar o Teorema 9.6 e concluir que com uma rede neural de profundidade suficiente pode-se interpolar *m* pontos de uma função $C_M$-Lipschitz com uma rede neural em $\text{Lip}(C_M)$, se $n_A \geq C_{\text{int}} \log(m) d_{\text{om}}$ [^p10]. Para simplificar a exposição, assume-se $C_{\text{int}} = 1$ no seguinte [^p10].

Assim, $\hat{\mathcal{R}}_S(\Phi) = 0$ assim que $n_A \geq \log(m) d_{\text{om}}$ [^p10].

Além disso, dependendo das propriedades de suavidade dos dados, o erro de interpolação pode decair com alguma taxa, por um dos resultados nos Capítulos 5, 7 ou 8 [^p10]. Por simplicidade, escolhe-se que $\hat{\mathcal{R}}_S(\Phi) = \mathcal{O}(n_A^{-1})$ para $n_A$ significativamente menor do que $\log(m) d_{\text{om}}$ [^p10]. Se combinar estas duas suposições, pode-se fazer o seguinte Ansatz para o risco empírico de $\Phi_{A_d} \in \mathcal{N}^*(\sigma_{\text{ReLU}}; A_d, B) \cap \text{Lip}(C_M)$ [^p10]:

$$\
\hat{\mathcal{R}}_S(\Phi_{A_d}) \leq \breve{\mathcal{R}}_S(\Phi_{A_d}) := C_{\text{approx}} \max \\{0, n_A^{-1} - (\log(m) d_{\text{om}})^{-1} \\}
$$

para uma constante $C_{\text{approx}} > 0$ [^p10]. Note que, pode-se interpolar a amostra S já com $d_{\text{om}}$ parâmetros pelo Teorema 9.3 [^p10]. No entanto, não é garantido que isso possa ser feito usando redes neurais $C_M$-Lipschitz [^p10].

#### 15.4.2 Limite Superior na Lacuna de Generalização
Complementa-se o limite no risco empírico com um limite superior no risco [^p10]. Invocando a notação do Teorema 15.2, tem-se que [^p10],

$$\
g(A_d, C_{\sigma_{\text{ReLU}}}, B, m) = \min \\{ \kappa_{NN}(A_d, m; c_1), \kappa_{\text{Lip}}(A_d, m; c_2) \\},
$$

onde [^p10]

$$\
\kappa_{NN}(A_d, m; c_1) = c_1 \frac{n_{A_d} \log(n_{A_d} \sqrt{m}) + L n_{A_d} \log(d)}{m},
$$

$$\
\kappa_{\text{Lip}}(A_d, m; c_2) = c_2 m^{-\frac{2}{2 + d_0}}.
$$

#### 15.4.3 Limite Superior no Risco
Em seguida, combinam-se (15.4.1) e (15.4.2) para obter um limite superior no risco $\mathcal{R}(\Phi_{A_d})$ [^p11]. Especificamente, define-se [^p11]:

$$\
\overline{\mathcal{R}}(\Phi_{A_d}) := \breve{\mathcal{R}}_S(\Phi_{A_d}) + \min \\{ \kappa_{NN}(A_d, m; c