## O Fenômeno da Dupla Descida

### Introdução
O presente capítulo visa explorar o fenômeno da **dupla descida** (*double descent*) no contexto de redes neurais profundas, um tema que desafia a teoria clássica do aprendizado estatístico [^1]. Como mencionado no capítulo anterior, a generalização em redes neurais profundas é influenciada pelo número de parâmetros da rede em relação ao número de amostras de treinamento [^1]. Tradicionalmente, acreditava-se que aumentar a complexidade do modelo (número de parâmetros) além de um certo ponto sempre levaria ao *overfitting*. No entanto, a prática empírica, especialmente em *deep learning*, demonstra que modelos *overparameterizados* frequentemente exibem boa generalização [^1]. O fenômeno da dupla descida busca explicar essa aparente contradição, mostrando que o erro de teste pode diminuir novamente após um período inicial de *overfitting* [^2].

### Conceitos Fundamentais

O fenômeno da dupla descida descreve como o erro de teste (ou risco) de um modelo inicialmente diminui com o aumento da complexidade do modelo (por exemplo, número de parâmetros), depois aumenta à medida que o modelo começa a sobreajustar os dados de treinamento e, finalmente, diminui novamente à medida que o modelo se torna fortemente *overparameterizado*, ultrapassando o chamado *interpolation threshold* [^1]. Este comportamento desafia a teoria clássica do aprendizado estatístico, que sugere que aumentar a complexidade do modelo além de um certo ponto sempre leva ao *overfitting* [^1].

A Figura 15.2 [^2] ilustra esse comportamento, dividindo o espaço de complexidade do modelo em dois regimes principais:

*   **Regime Clássico:** Neste regime, o modelo é *underparameterizado* (poucos parâmetros em relação aos dados de treinamento). Aumentar a complexidade reduz o erro, mas eventualmente leva ao *overfitting*, onde o modelo se ajusta demais aos dados de treinamento e perde a capacidade de generalizar para dados não vistos.
*   **Regime Moderno (Overparameterizado):** Após ultrapassar o *interpolation threshold*, onde o modelo é capaz de interpolar perfeitamente os dados de treinamento, o erro de teste paradoxalmente começa a diminuir novamente. Isso ocorre porque, neste regime, a *overparameterização* pode atuar como uma forma de regularização implícita, permitindo que o modelo encontre soluções mais robustas e generalizáveis.

#### Least-Squares Regression Revisited

Para entender melhor o fenômeno, o texto revisita a regressão de mínimos quadrados (kernel) [^2]. Considere um conjunto de dados $(x_j, y_j)_{j=1}^m \in \mathbb{R}^d \times \mathbb{R}$ gerado por uma função *ground-truth* $f$, ou seja, $y_j = f(x_j)$ para $j = 1, ..., m$ [^2]. Seja $\phi_j : \mathbb{R}^d \rightarrow \mathbb{R}$, $j \in \mathbb{N}$, uma sequência de funções *ansatz*. Para $n \in \mathbb{N}$, busca-se ajustar uma função $\sum_{i=1}^n w_i \phi_i(x)$ aos dados usando mínimos quadrados lineares [^2]. Para isso, introduz-se o *feature map*:

$$
\mathbb{R}^d \ni x \mapsto \Phi(x) := (\phi_1(x), ..., \phi_n(x))^T \in \mathbb{R}^n.
$$

O objetivo é determinar os coeficientes $w \in \mathbb{R}^n$ que minimizam o risco empírico [^3]:

$$
R_S(w) = \frac{1}{m} \sum_{j=1}^m \left( \sum_{i=1}^n w_i \phi_i(x_j) - y_j \right)^2 = \frac{1}{m} \sum_{j=1}^m \left( \Phi(x_j)^T w - y_j \right)^2.
$$

Definindo a matriz $A_n$ e o vetor $y$ como [^3]:

$$
A_n :=
\begin{pmatrix}
\phi_1(x_1) & \cdots & \phi_n(x_1) \\
\vdots & \ddots & \vdots \\
\phi_1(x_m) & \cdots & \phi_n(x_m)
\end{pmatrix}
\in \mathbb{R}^{m \times n}, \quad y = (y_1, ..., y_m)^T,
$$

tem-se [^3]:

$$
R_S(w) = \frac{1}{m} || A_n w - y ||^2.
$$

Um minimizador único de $R_S(w)$ existe se $A_n$ tiver posto $n$ [^3]. Para um minimizador $w_n$, a função ajustada é [^3]:

$$
f_n(x) := \sum_{j=1}^n w_{n,j} \phi_j(x).
$$

O comportamento de $f_n$ como uma função de $n$ (o número de funções *ansatz*/parâmetros do modelo) é de interesse, distinguindo-se dois casos [^3]:

*   ***Underparameterized* ($n < m$):** Há menos parâmetros ($n$) do que pontos de treinamento ($m$). Geralmente, não é possível interpolar os dados, e $\min_{w \in \mathbb{R}^n} R_S(w) > 0$ [^3].
*   ***Overparameterized* ($n \geq m$):** Há pelo menos tantos parâmetros ($n$) quanto pontos de treinamento ($m$). Se $A_n$ tiver posto completo $m$, existe $w$ tal que $R_S(w) = 0$. Se $n > m$, existem infinitas escolhas de parâmetros $w$ que resultam em risco empírico zero. Alguns levam a melhores funções de previsão $f_n$ do que outros [^3].

No caso *overparameterizado*, existe muitos minimizadores de $R_S$. O algoritmo de treinamento usado para computar um minimizador determina o tipo de função de predição $f_n$ que se obtém. Para observar a dupla descida, ou seja, para obter uma boa generalização para grandes valores de $n$, é necessário escolher cuidadosamente o minimizador [^3]. Em particular, é considerado o minimizador único de norma-2 mínima, definido como [^3]:

$$
w_{n,*} = \left( \arg \min_{\{w \in \mathbb{R}^n : R_S(w) \leq R_S(v) \ \forall v \in \mathbb{R}^n\}} ||w|| \right) \in \mathbb{R}^n.
$$

#### Exemplo Concreto

Para ilustrar, o texto apresenta um exemplo com 40 funções *ansatz* $\phi_1, ..., \phi_{40}$ geradas a partir de um processo Gaussiano, juntamente com a função de Runge $f$ e $m = 18$ pontos de treinamento [^3]. A Figura 15.4 [^5] mostra o ajuste da função em *span*$\{\phi_1, ..., \phi_n\}$ via (15.1.5) e (15.1.4) para diferentes valores de $n$ [^3]:

*   **n = 2:** O modelo não é expressivo o suficiente para aproximar $f$ [^4].
*   **n = 15:** O modelo captura as características principais de $f$. Não consegue interpolar os dados ($n < m$), permitindo um bom equilíbrio entre erro de aproximação e generalização [^4].
*   **n = 18:** No *interpolation threshold*, o modelo interpola os dados ($R_S(w) = 0$). O comportamento entre os pontos de dados é errático, exibindo fortes oscilações (*overfitting*) [^4].
*   **n = 40:** No regime *overparameterizado*, o modelo interpola os dados e oferece a melhor aproximação geral de $f$, devido a uma boa escolha do minimizador de $R_S$, nomeadamente (15.1.5) [^4].

A Figura 15.5(a) [^6] exibe o erro $||f - f_n||_{L^2([-1,1])}$ em função de $n$, mostrando a curva característica da dupla descida [^4]. A Figura 15.5(b) [^6] exibe $||w_{n,*}||$, notando que a norma Euclidiana do vetor de coeficientes também atinge o pico no *interpolation threshold* [^4].

A norma dos coeficientes $||w_{n,*}||$ exibe comportamento similar ao erro $L^2$, atingindo o pico no *interpolation threshold* $n = 18$ [^6]. Grandes pesos estão associados a grandes derivadas ou comportamento oscilatório, o que pode levar a uma má generalização [^6].

A Proposição 15.1 [^6] fornece uma explicação para o comportamento observado de $||w_{n,*}||$. Assumindo que $x_1, ..., x_m$ e $(\phi_j)_{j \in \mathbb{N}}$ são tais que $A_n$ em (15.1.2) tem posto completo $n$ para todo $n \leq m$, e dado $y \in \mathbb{R}^m$, denotando por $w_{n,*}(y)$ o vetor em (15.1.5), então [^6]:

$$
\sup_{||y||=1} ||w_{n,*}(y)|| \text{ é monotonicamente }
\begin{cases}
\text{crescente para } n < m, \\
\text{decrescente para } n \geq m.
\end{cases}
$$

A prova [^6] considera os casos $n \geq m$ e $n < m$ separadamente, utilizando propriedades do posto da matriz $A_n$ e da decomposição em valores singulares.

### Justificativa Teórica

Uma possível explicação para o fenômeno da dupla descida em redes neurais é baseada na ideia de que grandes redes neurais *overparameterizadas* tendem a ser Lipschitz contínuas com uma constante de Lipschitz independente do tamanho [^7]. Isso é uma consequência de redes neurais tipicamente terem pesos relativamente pequenos. O texto considera a classe de redes neurais $\mathcal{N}(\sigma; A, B)$ para uma arquitetura $A$ de profundidade $d \in \mathbb{N}$ e largura $L \in \mathbb{N}$ [^7].

O texto então discute como estudar a capacidade de generalização de funções de Lipschitz através dos resultados de aprendizado baseados em números de cobertura do Capítulo 14 [^8].

### Dupla Descida para Aprendizado de Redes Neurais

Para entender o fenômeno da dupla descida no contexto do Teorema 15.2, são feitas algumas simplificações para obter uma fórmula para um limite superior do risco [^10]. Assume-se que os dados $S = (x_i, y_i)_{i=1}^m \in \mathbb{R}^{d_0} \times \mathbb{R}$ provêm de uma função contínua $C_M$-Lipschitz. Além disso, fixa-se uma profundidade $L \in \mathbb{N}$ e considera-se, para $d \in \mathbb{N}$, arquiteturas da forma $(\sigma_{ReLU}; A_d)$, onde $A_d = (d_0, d, ..., d, 1)$ [^10].

O número de parâmetros para esta arquitetura é limitado por [^10]:

$$
n_{A_d} = (d_0 + 1)d + (L - 1)(d + 1)d + d + 1.
$$

Para derivar um limite superior para o risco, começa-se por limitar superiormente o risco empírico e, em seguida, aplicar o Teorema 15.2 para estabelecer um limite superior para o *generalization gap* [^10]. Em combinação, estas estimativas fornecem um limite superior para o risco. Será então observado que este limite superior segue a curva da dupla descida na Figura 15.2 [^10].

#### Limite Superior para o Risco Empírico

É estabelecido um limite superior para $R_S(\Phi)$ para $\Phi \in \mathcal{N}^*(\sigma_{ReLU}; A_d, B) \cap Lip(C_M)$ [^10]. Para $B > C_M$, pode-se aplicar o Teorema 9.6 e concluir que, com uma rede neural de profundidade suficiente, é possível interpolar $m$ pontos de uma função $C_M$-Lipschitz com uma rede neural em $Lip(C_M)$, se $n_A \geq C_{int} \log(m)^{d_{om}}$ [^10].

Para simplificar a exposição, assume-se $C_{int} = 1$ [^10]. Assim, $R_S(\Phi) = 0$ assim que $n_A \geq \log(m)^{d_{om}}$ [^10].

Adicionalmente, dependendo das propriedades de suavidade dos dados, o erro de interpolação pode decair com alguma taxa [^10]. Para simplificar, escolhe-se que $R_S(\Phi) = O(n_A^{-1})$ para $n_A$ significativamente menor do que $\log(m)^{d_{om}}$ [^10]. Combinando estas duas suposições, pode-se fazer o seguinte *Ansatz* para o risco empírico de $\Phi_{A_d} \in \mathcal{N}^*(\sigma_{ReLU}; A_d, B) \cap Lip(C_M)$ [^10]:

$$
\hat{R}_S(\Phi_{A_d}) \leq \hat{R}_S(\Phi_{A_d}) := C_{approx} \max\{0, n_A^{-1} - (\log(m)^{d_{om}})^{-1}\},
$$

para uma constante $C_{approx} > 0$ [^10].

#### Limite Superior para o *Generalization Gap*

Complementa-se o limite para o risco empírico com um limite superior para o risco [^10]. Invocando a notação do Teorema 15.2, tem-se que [^10]:

$$
g(A_d, C_{\sigma_{ReLU}}, B, m) = \min\{\kappa_{NN}(A_d, m; c_1), \kappa_{Lip}(A_d, m; c_2)\},
$$

onde [^10]:

$$
\kappa_{NN}(A_d, m; c_1) := c_1 \frac{n_{A_d} \log(n_{A_d} \sqrt{m}) + Ln_{A_d} \log(d)}{m},
$$

$$
\kappa_{Lip}(A_d, m; c_2) := c_2 m^{-\frac{2}{d_0} + 40},
$$

para algumas constantes $c_1, c_2 > 0$ [^10].

#### Limite Superior para o Risco

Em seguida, combinam-se (15.4.1) e (15.4.2) para obter um limite superior para o risco $R(\Phi_{A_d})$ [^11]. Especificamente, define-se [^11]:

$$
\hat{R}(\Phi_{A_d}) := \hat{R}_S(\Phi_{A_d}) + \min\{\kappa_{NN}(A_d, m; c_1), \kappa_{Lip}(A_d, m; c_2)\} + 4C_c \sqrt{\frac{\log(4/\delta)}{m}}.
$$

A Figura 15.6 [^11] mostra o limite superior para o risco dado por (15.4.3) (excluindo os termos que não dependem da arquitetura). O limite superior assemelha-se claramente ao fenômeno da dupla descida da Figura 15.2 [^11].

### Conclusão

Este capítulo explorou o fenômeno da dupla descida, que desafia a intuição clássica de que aumentar a complexidade do modelo sempre leva ao *overfitting* [^1]. Através da análise da regressão de mínimos quadrados e da apresentação de uma justificativa teórica baseada na continuidade de Lipschitz das redes neurais, foi demonstrado como o erro de teste pode diminuir novamente em regimes *overparameterizados* [^2, 7]. Embora o modelo apresentado simplifique alguns aspectos do fenômeno, ele captura a essência da dupla descida e fornece *insights* valiosos sobre o comportamento de redes neurais profundas [^11]. A compreensão deste fenômeno é crucial para o desenvolvimento de modelos mais robustos e generalizáveis em *deep learning* [^1].

### Referências
[^1]: Chapter 15 - Generalization in the overparameterized regime
[^2]: 15.1 The double descent phenomenon
[^3]: 15.1.1 Least-squares regression revisited
[^4]: 15.1.2 An example
[^5]: Figure 15.4: Fit of the m = 18 red data points using the ansatz functions 1,..., η from Figure 15.3, employing equations (15.1.5) and (15.1.4) for different numbers of ansatz functions n.
[^6]: 15.2 Size of weights
[^7]: 15.3 Theoretical justification
[^8]: We study the generalization capacity of Lipschitz functions through the covering-number-based learning results of Chapter 14.
[^9]: An assumption of the type B ≤ св· (dCo)−1, i.e. a scaling of the weights by the reciprocal 1/d of the width, is not unreasonable in practice: Standard initialization schemes such as LeCun [127] or He [85] initialization, use random weights with variance scaled inverse proportional to the input dimension of each layer.
[^10]: 15.4 Double descent for neural network learning
[^11]: 15.4.3 Upper bound on risk
[^12]: Figure 15.6: Upper bound on R(ΦĀ』) derived in (15.4.3). For better visibility the part corresponding to y-values between 0.0012 and 0.0022 is not shown. The vertical dashed line indicates the interpolation threshold according to Theorem 9.3.

<!-- END -->