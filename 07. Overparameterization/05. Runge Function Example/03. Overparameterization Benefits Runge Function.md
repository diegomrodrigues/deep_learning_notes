## Generalização no Regime de Overparametrização: O Exemplo da Função de Runge

### Introdução
Este capítulo explora o fenômeno da generalização no regime de *overparameterization*, utilizando o exemplo concreto da Função de Runge para ilustrar os conceitos. Como discutido nos capítulos anteriores, a escolha da arquitetura de uma rede neural é crucial para obter boa generalização [^1]. Em particular, o balanço entre erros de aproximação e generalização é fundamental para um bom desempenho [^1]. No entanto, redes neurais *overparameterized*, que possuem um número de parâmetros maior do que o número de amostras de treinamento, apresentam um comportamento que desafia a teoria estatística tradicional [^2]. Este capítulo investiga como a *overparameterization* pode, surpreendentemente, levar a uma melhor generalização, um fenômeno conhecido como *double descent* [^2].

### Conceitos Fundamentais
Para entender como a *overparameterization* afeta a generalização, consideraremos o problema de regressão por mínimos quadrados (kernel) [^2], conforme introduzido na Seção 11.2 [^2]. Dado um conjunto de dados $(x_j, y_j)_{j=1}^m \in \mathbb{R}^d \times \mathbb{R}$ gerado por uma função *ground-truth* $f$, ou seja, $y_j = f(x_j)$ para $j = 1, ..., m$ [^2]. Definimos uma sequência de funções *ansatz* $\phi_j: \mathbb{R}^d \rightarrow \mathbb{R}$, com $j \in \mathbb{N}$ [^2]. Para $n \in \mathbb{N}$, buscamos ajustar uma função $\sum_{i=1}^n w_i \phi_i(x)$ aos dados usando mínimos quadrados lineares [^2]. Para isso, introduzimos o *feature map*:
$$
\mathbb{R}^d \ni x \mapsto \Phi(x) := (\phi_1(x), ..., \phi_n(x))^T \in \mathbb{R}^n
$$[^2]
O objetivo é determinar os coeficientes $w \in \mathbb{R}^n$ que minimizam o risco empírico [^3]:
$$
R_S(w) = \frac{1}{m} \sum_{j=1}^m \left( \sum_{i=1}^n w_i \phi_i(x_j) - y_j \right)^2 = \frac{1}{m} ||A_n w - y||^2
$$[^3]
onde $A_n$ é a matriz definida como [^3]:
$$
A_n :=
\begin{pmatrix}
\phi_1(x_1) & \cdots & \phi_n(x_1) \\
\vdots & \ddots & \vdots \\
\phi_1(x_m) & \cdots & \phi_n(x_m)
\end{pmatrix} \in \mathbb{R}^{m \times n}
$$
e $y = (y_1, ..., y_m)^T$ [^3].

Conforme discutido nas Seções 11.1-11.2 [^3], um minimizador único de $R_S(w)$ existe apenas se $A_n$ tiver *rank* $n$ [^3]. Para um minimizador $w_n$, a função ajustada é dada por [^3]:
$$
f_n(x) := \sum_{j=1}^n w_{n,j} \phi_j(x)
$$[^3]

É crucial distinguir dois regimes:
1.  ***Underparameterized***: Quando $n < m$, temos menos parâmetros do que pontos de treinamento [^3]. Isso implica que, em geral, não podemos interpolar os dados, e $\min_{w \in \mathbb{R}^n} R_S(w) > 0$ [^3].
2.  ***Overparameterized***: Quando $n \geq m$, temos pelo menos tantos parâmetros quanto pontos de treinamento [^3]. Se $A_n$ tem *rank* $m$, então existe $w$ tal que $R_S(w) = 0$, ou seja, podemos interpolar os dados [^3]. Se $n > m$, existem infinitas escolhas de parâmetros $w$ que resultam em risco empírico zero [^3].

No regime de *overparameterization*, a escolha do algoritmo de treinamento para calcular o minimizador determina o tipo de função de predição $f_n$ que obtemos [^3]. Para observar o fenômeno do *double descent*, ou seja, para obter boa generalização para valores grandes de $n$, precisamos escolher o minimizador cuidadosamente [^3]. Consideramos o minimizador único com a norma $L_2$ mínima, definido como [^3]:
$$
w_{n,*} = \left( \arg \min_{\{w \in \mathbb{R}^n | R_S(w) \leq R_S(v) \forall v \in \mathbb{R}^n\}} ||w|| \right) \in \mathbb{R}^n
$$[^3]

#### Exemplo Concreto: Função de Runge
Para ilustrar esses conceitos, o texto fornece um exemplo concreto utilizando a Função de Runge [^3]. Um conjunto de 40 funções *ansatz* $\phi_1, ..., \phi_{40}$ são amostradas de um processo Gaussiano [^3]. Além disso, são utilizados $m = 18$ pontos equidistantes como dados de treinamento [^3]. Uma função no espaço *span*$\{\phi_1, ..., \phi_n\}$ é ajustada via a equação (15.1.5) e (15.1.4) [^3].

Os resultados mostram que [^4]:

*   Para $n = 2$, o modelo não é expressivo o suficiente para aproximar a Função de Runge [^4].
*   Para $n = 15$, o modelo captura as características principais da função, mas não consegue interpolar os dados [^4]. Há um bom balanço entre erro de aproximação e generalização [^4].
*   Para $n = 18$, estamos no limiar de interpolação. O modelo interpola os dados, mas exibe oscilações erráticas entre os pontos [^4], indicando *overfitting* [^4].
*   Para $n = 40$, estamos no regime de *overparameterization*. O modelo interpola os dados e fornece a melhor aproximação geral à Função de Runge [^4].

A Figura 15.5(a) [^4] exibe o erro $||f - f_n||_{L^2([-1,1])}$ em função de $n$, mostrando a curva característica do *double descent* [^4]. Após atingir o pico no limiar de interpolação, o erro começa a diminuir novamente no regime de *overparameterization* [^4].

### Conclusão
O exemplo da Função de Runge demonstra que, no regime de *overparameterization*, um modelo pode interpolar os dados de treinamento e, surpreendentemente, fornecer a melhor aproximação geral da função *ground-truth* [^4]. Esse fenômeno, conhecido como *double descent*, desafia a intuição tradicional de que *overfitting* sempre leva a uma generalização ruim [^2]. A escolha cuidadosa do minimizador, como o minimizador de norma $L_2$ mínima, é crucial para obter esse comportamento [^3]. Embora a natureza precisa das curvas de convergência dependa de vários fatores, como a distribuição dos dados e a escolha das funções *ansatz* [^4], o exemplo ilustra o potencial da *overparameterization* para melhorar a generalização em certos cenários [^4].

### Referências
[^1]: Capítulo 15, p. 206
[^2]: Capítulo 15, p. 207
[^3]: Capítulo 15, p. 208
[^4]: Capítulo 15, p. 209

<!-- END -->