## Tamanho dos Pesos e Generalização

### Introdução
Este capítulo explora a relação entre o tamanho dos pesos em redes neurais e a capacidade de generalização. Como vimos anteriormente [^1], a generalização em redes neurais profundas treinadas por minimização do risco empírico depende do número de parâmetros da rede em relação ao número de amostras de treinamento. Este capítulo aprofunda essa análise, focando no papel específico do tamanho dos pesos e introduzindo o fenômeno do double descent [^2].

### Conceitos Fundamentais

O tamanho dos pesos (coeficientes) em uma rede neural está relacionado à complexidade e à capacidade de generalização da rede [^2, 6]. Pesos grandes são geralmente indesejáveis porque estão frequentemente associados a grandes derivadas ou comportamento oscilatório na função de predição, o que pode levar a uma generalização ruim [^6, 7, 8, 9].

**Norma dos Coeficientes e Overfitting:**
A norma dos coeficientes, especialmente a norma L2, pode fornecer insights sobre o comportamento da rede. É observado que o erro L2, que mede a diferença entre a função prevista e a função verdadeira, frequentemente atinge o pico no *interpolation threshold* [^2, 6], indicando que o modelo está *overfitting* os dados de treinamento.

**Relação com o Double Descent:**
O fenômeno do *double descent* [^2] descreve como o risco (erro) de um modelo, inicialmente, diminui conforme a complexidade aumenta (regime clássico), atinge um pico no *interpolation threshold* e, então, começa a diminuir novamente (regime moderno) [^2]. A norma dos pesos também exibe um comportamento semelhante, atingindo um pico no *interpolation threshold* [^6].

**Análise Matemática:**
Considerando um problema de regressão de mínimos quadrados [^2], onde o objetivo é encontrar coeficientes $w \in \mathbb{R}^n$ que minimizem o risco empírico:
$$
R_s(w) = \frac{1}{m} \sum_{j=1}^{m} \left( y_j - \sum_{i=1}^{n} w_i \phi_i(x_j) \right)^2 = \frac{1}{m} || A_n w - y ||^2
$$
onde $A_n$ é a matriz de design, $y$ é o vetor de rótulos e $\phi_i(x)$ são as funções base [^2].

A solução de norma mínima para este problema é dada por [^3]:
$$
w_{n,*} = A_n^{\dagger} y
$$
onde $A_n^{\dagger}$ é a pseudo-inversa de $A_n$ [^3]. A norma de $w_{n,*}$ pode ser escrita como [^3]:
$$
||w_{n,*}|| = ||A_n^{\dagger} y||
$$
A análise da norma de $w_{n,*}$ em relação ao número de parâmetros $n$ revela que ela aumenta até o *interpolation threshold* e, então, diminui [^3].

**Proposition 15.1:** Assumindo que $x_1, ..., x_m$ e $(\phi_j)_{j \in \mathbb{N}}$ são tais que $A_n$ em (15.1.2) tem posto completo para todo $n \leq m$. Dado $y \in \mathbb{R}^m$, denotamos por $w_{n,*}(y)$ o vetor em (15.1.5). Então [^6]:

$$
\sup_{||y||=1} ||w_{n,*}(y)|| \text{ é monotonicamente } \begin{cases} \text{crescente para } n < m \\\\ \text{decrescente para } n \geq m \end{cases}
$$

**Prova:** Começamos com o caso $n \geq m$. Por hipótese, $A_m$ tem posto completo $m$, e portanto $A_n$ tem posto $m$ para todo $n \geq m$, veja (15.1.2). Em particular, existe $w_n \in \mathbb{R}^n$ tal que $A_n w_n = y$. Agora fixamos $y \in \mathbb{R}^m$ e seja $w_n$ qualquer vetor tal que $A_n w_n = y$. Então $w_{n+1} := (w_n, 0) \in \mathbb{R}^{n+1}$ satisfaz $A_{n+1} w_{n+1} = y$ e $||w_{n+1}|| = ||w_n||$. Assim, necessariamente $||w_{n+1,*}|| \leq ||w_{n,*}||$ para as soluções de norma mínima definidas em (15.1.5). Como isso vale para todo $y$, obtemos a afirmação para $n \geq m$.
Agora seja $n < m$. Lembre-se de que a solução de norma mínima pode ser escrita através da pseudo-inversa [^6]:

$$
w_{n,*}(y) = A_n^{\dagger} y,
$$
onde [^6]
$$
A_n^{\dagger} = V_n \begin{pmatrix} \frac{1}{\sigma_{n,1}} & & & \\ & \ddots & & \\ & & \frac{1}{\sigma_{n,n}} & \\ & & & 0 \\ & & & \vdots \\ & & & 0 \end{pmatrix} U_n^T \in \mathbb{R}^{n \times m}
$$
onde $A_n = U_n \Sigma_n V_n^T$ é a decomposição em valores singulares de $A_n$, e [^6]
$$
\Sigma_n = \begin{pmatrix} \sigma_{n,1} & & & & \\ & \ddots & & & \\ & & \sigma_{n,n} & & \\ & & & 0 & \\ & & & & 0 \end{pmatrix} \in \mathbb{R}^{m \times n}
$$
contém os valores singulares $\sigma_{n,1} \geq \dots \geq \sigma_{n,n} > 0$ de $A_n \in \mathbb{R}^{m \times n}$ ordenados em ordem decrescente. Já que $V_n \in \mathbb{R}^{n \times n}$ e $U_n \in \mathbb{R}^{m \times m}$ são matrizes ortogonais, temos [^6]
$$
\sup_{||y||=1} ||w_{n,*}(y)|| = \sup_{||y||=1} ||A_n^{\dagger} y|| = \frac{1}{\sigma_{n,n}}.
$$
Finalmente, já que o valor singular mínimo $\sigma_{n,n}$ de $A_n$ pode ser escrito como [^6]
$$
\sigma_{n,n} = \inf_{\substack{x \in \mathbb{R}^n \\ ||x||=1}} ||A_n x|| \geq \inf_{\substack{x \in \mathbb{R}^{n+1} \\ ||x||=1}} ||A_{n+1} x|| = \sigma_{n+1,n+1},
$$
observamos que $n \mapsto \sigma_{n,n}$ é monotonicamente decrescente para $n \leq m$. Isso conclui a prova. $\blacksquare$

**Implicações para a Generalização:**
O pico na norma dos pesos no *interpolation threshold* sugere que, neste ponto, a rede está aprendendo padrões específicos dos dados de treinamento, incluindo ruído, em vez de aprender padrões gerais [^6, 7, 8, 9]. Isso leva a um *overfitting* e a uma capacidade de generalização ruim [^6, 7, 8, 9]. No entanto, no regime *overparameterized*, a norma dos pesos diminui, indicando que a rede está aprendendo representações mais suaves e generalizáveis [^6, 7, 8, 9].

### Conclusão

A relação entre o tamanho dos pesos e a generalização é complexa e multifacetada [^6, 7, 8, 9]. Embora pesos grandes possam levar ao *overfitting*, o regime *overparameterized* oferece a possibilidade de aprender representações mais generalizáveis [^6, 7, 8, 9]. A escolha de algoritmos de treinamento que favoreçam pesos menores, juntamente com técnicas de regularização, pode melhorar a capacidade de generalização das redes neurais [^6, 7, 8, 9]. Além disso, entender e controlar a norma dos pesos pode ajudar a mitigar os efeitos do *double descent* e a otimizar o desempenho das redes neurais em dados não vistos [^6, 7, 8, 9].

### Referências
[^1]: Capítulo 14
[^2]: Página 207
[^3]: Página 208
[^4]: Página 209
[^5]: Página 210
[^6]: Página 211
[^7]: Página 7
[^8]: Página 8
[^9]: Página 9
<!-- END -->