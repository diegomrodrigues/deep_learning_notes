## O Tamanho dos Pesos e a Generalização no Regime de Sobreparametrização

### Introdução
Este capítulo aprofunda a análise da generalização no regime de sobreparametrização, com foco no comportamento dos pesos em modelos que interpolam os dados de treinamento. Como discutido anteriormente, a capacidade de generalização de redes neurais profundas está intrinsecamente ligada ao número de parâmetros da rede em relação ao número de amostras de treinamento [^1]. Exploraremos como a norma Euclidiana do vetor de coeficientes se comporta em relação ao *interpolation threshold* e como essa relação impacta a generalização.

### Conceitos Fundamentais
Em modelos de aprendizado de máquina, o tamanho dos pesos desempenha um papel crucial na capacidade de generalização. Pesos excessivamente grandes podem levar a comportamentos oscilatórios e a um *overfitting*, prejudicando a performance do modelo em dados não vistos. A seção 15.2 destaca a observação de que a norma dos coeficientes $||w_{n,*}||$ exibe um comportamento semelhante ao erro L2, atingindo um pico no *interpolation threshold* [^6]. Este fenômeno sugere que, nesse ponto, o modelo requer pesos maiores para ajustar perfeitamente os dados de treinamento.

**Proposição 15.1:** Assume que $x_1,...,x_m$ e $(\phi_j)_{j \in \mathbb{N}}$ são tais que $A_n$ em (15.1.2) tem posto completo $n$ para todo $n \le m$. Dado $y \in \mathbb{R}^m$, denotamos por $w_{n,*}(y)$ o vetor em (15.1.5). Então,

$$
\sup_{||y||=1} ||w_{n,*}(y)|| \text{ é monotonicamente }
\begin{cases}
\text{crescente para } n < m \\\\
\text{decrescente para } n \ge m
\end{cases}
$$

*Prova*: Começamos com o caso $n \ge m$. Por hipótese, $A_m$ tem posto completo $m$, e assim $A_n$ tem posto $m$ para todo $n \ge m$, ver (15.1.2). Em particular, existe $w_n \in \mathbb{R}^n$ tal que $A_n w_n = y$. Agora fixamos $y \in \mathbb{R}^m$ e seja $w_n$ qualquer vetor. Então $w_{n+1} := (w_n, 0) \in \mathbb{R}^{n+1}$ satisfaz $A_{n+1}w_{n+1} = y$ e $||w_{n+1}|| = ||w_n||$. Assim, necessariamente $||w_{n+1,*}|| \le ||w_{n,*}||$ para as soluções de norma mínima definidas em (15.1.5). Como isso vale para todo $y$, obtemos a afirmação para $n \ge m$.

Agora seja $n < m$. Recorde que a solução de norma mínima pode ser escrita através da pseudo-inversa

$$w_{n,*}(y) = A_n^\dagger y,$$
ver por exemplo Exercício 11.32. Aqui,

$$A_n^\dagger = V_n
\begin{pmatrix}
\sigma_{n,1}^{-1} & & & 0 \\\\
& \ddots & & \vdots \\\\
& & \sigma_{n,n}^{-1} & 0
\end{pmatrix}
U_n^T \in \mathbb{R}^{n \times m},$$

onde $A_n = U_n \Sigma_n V_n^T$ é a decomposição em valores singulares de $A_n$, e

$$\Sigma_n = \begin{pmatrix}
\sigma_{n,1} & & & \\\\
& \ddots & & \\\\
& & \sigma_{n,n} & \\\\
& & & 0 \\\\
& & & \vdots \\\\
& & & 0
\end{pmatrix} \in \mathbb{R}^{m \times n}$$

contém os valores singulares $\sigma_{n,1} \ge \dots \ge \sigma_{n,n} > 0$ de $A_n \in \mathbb{R}^{m \times n}$ ordenados por tamanho decrescente. Como $V_n \in \mathbb{R}^{n \times n}$ e $U_n \in \mathbb{R}^{m \times m}$ são matrizes ortogonais, temos

$$\sup_{||y||=1} ||w_{n,*}(y)|| = \sup_{||y||=1} ||A_n^\dagger y|| = \sigma_{n,n}^{-1}.$$

Finalmente, como o valor singular mínimo $\sigma_{n,n}$ de $A_n$ pode ser escrito como

$$\sigma_{n,n} = \inf_{\substack{x \in \mathbb{R}^n \\\\ ||x|| = 1}} ||A_n x|| \ge \inf_{\substack{x \in \mathbb{R}^{n+1} \\\\ ||x|| = 1}} ||A_{n+1} x|| = \sigma_{n+1,n+1},$$

observamos que $n \mapsto \sigma_{n,n}$ é monotonicamente decrescente para $n \le m$. Isso conclui a prova. $\blacksquare$

A proposição 15.1 [^6] fornece uma explicação teórica para o comportamento observado dos pesos. Sob certas condições, a norma da solução de norma mínima aumenta monotonicamente para $n < m$ e diminui monotonicamente para $n \ge m$. Isso corrobora a ideia de que, na interpolação, o modelo precisa de pesos maiores para se ajustar perfeitamente aos dados.

A figura 15.5 [^6] ilustra essa relação, mostrando o pico da norma dos coeficientes no *interpolation threshold* ($n = 18$). Isso reforça a importância de escolher um modelo com um número adequado de parâmetros para evitar o *overfitting* e garantir uma boa generalização.

### Conclusão
O tamanho dos pesos é um indicador crítico da capacidade de generalização de um modelo. A tendência da norma Euclidiana do vetor de coeficientes de atingir um pico no *interpolation threshold* destaca a necessidade de equilibrar a capacidade do modelo de se ajustar aos dados de treinamento com a sua habilidade de generalizar para dados não vistos. A proposição 15.1 oferece uma base teórica para entender esse comportamento, fornecendo insights valiosos para a seleção e otimização de modelos de aprendizado de máquina.

### Referências
[^1]: Capítulo 15, Generalization in the overparameterized regime, p. 206.
[^6]: Capítulo 15, Size of weights, p. 211.
<!-- END -->