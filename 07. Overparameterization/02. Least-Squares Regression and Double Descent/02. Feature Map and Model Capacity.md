## O Mapa de Características e a Capacidade do Modelo na Regressão de Mínimos Quadrados

### Introdução
Este capítulo explora o conceito do **mapa de características** no contexto da regressão de mínimos quadrados e sua relação com o fenômeno do double descent [^2]. O mapa de características transforma os dados de entrada em um espaço de dimensão superior, permitindo que o modelo capture relações mais complexas nos dados [^2]. A escolha do mapa de características e o número de *ansatz functions* (funções de base) influenciam a capacidade do modelo e sua habilidade de interpolar os dados de treinamento [^2]. Analisaremos os casos *underparameterized* (subparametrizado) e *overparameterized* (superparametrizado) e como o algoritmo de treinamento influencia o desempenho da generalização. Este capítulo se baseia nos conceitos de regressão de mínimos quadrados (kernel) introduzidos na Seção 11.2 [^2].

### Conceitos Fundamentais

#### Mapa de Características
Na regressão de mínimos quadrados, introduzimos o mapa de características para transformar os dados de entrada em um espaço de dimensão superior [^2]. Dado um conjunto de dados $(x_j, y_j)_{j=1}^m \subset \mathbb{R}^d \times \mathbb{R}$ gerado por uma função *ground-truth* $f$, ou seja, $y_j = f(x_j)$ para $j = 1, ..., m$ [^2], definimos uma sequência de *ansatz functions* $\phi_j : \mathbb{R}^d \rightarrow \mathbb{R}$, $j \in \mathbb{N}$ [^2]. Para $n \in \mathbb{N}$, desejamos ajustar uma função $\sum_{i=1}^n w_i \phi_i(x)$ aos dados usando mínimos quadrados lineares [^2]. Para isso, introduzimos o mapa de características:
$$\
\mathbb{R}^d \ni x \mapsto \Phi(x) := (\phi_1(x), ..., \phi_n(x))^T \in \mathbb{R}^n \text{[^2]}.\
$$
O objetivo é determinar os coeficientes $w \in \mathbb{R}^n$ que minimizam o risco empírico [^3]:
$$\
R_S(w) = \frac{1}{m} \sum_{j=1}^m \left( \sum_{i=1}^n w_i \phi_i(x_j) - y_j \right)^2 = \frac{1}{m} ||A_n w - y||^2 \text{[^3]},\
$$
onde $A_n$ é a matriz [^3]:
$$\
A_n :=\
\begin{pmatrix}\
\phi_1(x_1) & \cdots & \phi_n(x_1) \\\\\
\vdots & \ddots & \vdots \\\\\
\phi_1(x_m) & \cdots & \phi_n(x_m)\
\end{pmatrix}\
\in \mathbb{R}^{m \times n} \text{[^3]},\
$$
e $y = (y_1, ..., y_m)^T$ [^3].

Conforme discutido nas Seções 11.1-11.2, um minimizador único de $R_S(w)$ existe apenas se $A_n$ tiver posto $n$ [^3]. Para um minimizador $w_n$, a função ajustada é [^3]:
$$\
f_n(x) := \sum_{j=1}^n w_{n,j} \phi_j(x) \text{[^3]}.\
$$
Estamos interessados no comportamento de $f_n$ em função de $n$, o número de *ansatz functions* (parâmetros do modelo) [^3]. Distinguimos dois casos [^3]:

#### Caso Underparameterized (Subparametrizado)
Se $n < m$, temos menos parâmetros $n$ do que pontos de treinamento $m$ [^3]. Para o problema de mínimos quadrados de minimizar $R_S$, isso significa que existem mais condições $m$ do que parâmetros livres $n$ [^3]. Assim, em geral, não podemos interpolar os dados, e temos $\min_{w \in \mathbb{R}^n} R_S(w) > 0$ [^3].

#### Caso Overparameterized (Superparametrizado)
Se $n \geq m$, temos pelo menos tantos parâmetros $n$ quanto pontos de treinamento $m$ [^3]. Se os $x_j$ e os $\phi_j$ são tais que $A_n \in \mathbb{R}^{m \times n}$ tem posto completo $m$, então existe $w$ tal que $R_S(w) = 0$ [^3]. Se $n > m$, então $A_n$ necessariamente tem um kernel não trivial, e existem infinitas escolhas de parâmetros $w$ que resultam em risco empírico zero $R_S$ [^3]. Alguns levam a melhores funções de predição $f_n$ e outros levam a piores [^3].

No caso superparametrizado, existem muitos minimizadores de $R_S$ [^3]. O algoritmo de treinamento que usamos para computar um minimizador determina o tipo de função de predição $f_n$ que obtemos [^3]. Para observar o double descent, ou seja, para alcançar uma boa generalização para $n$ grande, precisamos escolher o minimizador cuidadosamente [^3]. Consideramos o minimizador único de norma-2 mínima, definido como [^3]:
$$\
w_{n,*} = \left( \underset{w \in \mathbb{R}^n}{\text{argmin}} \\{ ||w|| : R_S(w) \leq R_S(v) \\ \forall v \in \mathbb{R}^n \\} \right) \in \mathbb{R}^n \text{[^3]}.\
$$

#### Exemplo Concreto
Na Figura 15.3 [^4], um conjunto de 40 *ansatz functions* $\phi_1, ..., \phi_{40}$ são plotadas, as quais são retiradas de um processo Gaussiano [^4]. Adicionalmente, a figura mostra um plot da função de Runge $f$, e $m=18$ pontos equidistantes que são usados como pontos de treinamento [^4]. Então, uma função no espaço gerado por $\\{\phi_1, ..., \phi_n\\}$ é ajustada via (15.1.5) e (15.1.4) [^4]. O resultado é exibido na Figura 15.4 [^5].

*   $n = 2$: O modelo só pode representar funções em span{$\phi_1, \phi_2$}. Não é expressivo o suficiente para dar uma aproximação significativa de $f$ [^4].
*   $n = 15$: O modelo tem expressividade suficiente para capturar as principais características de $f$ [^4]. Como $n = 15 < 18 = m$, ele ainda não consegue interpolar os dados [^4]. Assim, permite alcançar um bom equilíbrio entre o erro de aproximação e generalização, o que corresponde ao cenário discutido no Capítulo 14 [^4].
*   $n = 18$: Estamos no limite de interpolação [^4]. O modelo é capaz de interpolar os dados, e existe um $w$ único tal que $R_S(w) = 0$ [^4]. No entanto, entre os pontos de dados, o comportamento do preditor $f_{18}$ parece errático e exibe fortes oscilações [^4]. Isso é conhecido como *overfitting* e é esperado devido à análise no Capítulo 14 [^4]; enquanto o erro de aproximação nos pontos de dados melhorou em comparação com o caso $n = 15$, o erro de generalização piorou [^4].
*   $n = 40$: Este é o regime superparametrizado, onde temos significativamente mais parâmetros do que pontos de dados [^4]. A predição $f_{40}$ interpola os dados e parece ser a melhor aproximação geral de $f$ até agora, devido a uma "boa" escolha do minimizador de $R_S$, ou seja, (15.1.5) [^4]. Também notamos que, embora muito bom, o ajuste não é perfeito [^4]. Não podemos esperar uma melhoria significativa no desempenho ao aumentar ainda mais $n$, uma vez que neste ponto o principal fator limitante é a quantidade de dados disponíveis [^4]. Veja também a Figura 15.5 (a) [^6].

A Figura 15.5 (a) exibe o erro $||f - f_n||_{L^2([-1,1])}$ sobre $n$ [^6]. Observa-se a curva característica do double descent, onde o erro inicialmente diminui, após atingir o pico no limite de interpolação, que é marcado pela linha vermelha tracejada [^6]. Posteriormente, no regime superparametrizado, ele começa a diminuir novamente [^6]. A Figura 15.5 (b) exibe $||w_{n,*}||$ [^6]. Observe como a norma Euclidiana do vetor de coeficientes também atinge o pico no limite de interpolação [^6].

A natureza precisa das curvas de convergência depende fortemente de vários fatores, como a distribuição e o número de pontos de treinamento $m$, a verdade fundamental $f$ e a escolha das funções *ansatz* $\phi_j$ (por exemplo, o kernel específico usado para gerar os $\phi_j$ na Figura 15.3 (a)) [^4, ^6].

### Conclusão
Em resumo, a escolha do mapa de características e o número de parâmetros no modelo de regressão de mínimos quadrados têm um impacto significativo na capacidade do modelo de interpolar os dados de treinamento e generalizar para novos dados [^2, ^3, ^4, ^6]. O fenômeno do double descent ilustra como, no regime superparametrizado, a escolha cuidadosa do minimizador do risco empírico pode levar a um melhor desempenho de generalização [^3, ^4, ^6]. A análise apresentada neste capítulo fornece insights sobre a complexa interação entre a capacidade do modelo, o algoritmo de treinamento e a generalização no contexto da regressão de mínimos quadrados, complementando a discussão sobre generalização em redes neurais profundas apresentada no Capítulo 14 [^1, ^4].

### Referências
[^1]: Capítulo 14: Generalization in the overparameterized regime.
[^2]: Seção 15.1.1: Least-squares regression revisited.
[^3]: Seção 15.1.1: Least-squares regression revisited.
[^4]: Seção 15.1.2: An example.
[^5]: Figura 15.4: Fit of the m = 18 red data points using the ansatz functions ...
[^6]: Figura 15.5: The L2-error for the fitted functions in Figure 15.4, and the l²-norm of the corresponding coefficient vector wn,∗ defined in (15.1.5).

<!-- END -->