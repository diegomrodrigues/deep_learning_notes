## Least-Squares Regression and Double Descent: A Deeper Dive

### Introdução
Este capítulo explora a **regressão por mínimos quadrados** no contexto do fenômeno da **dupla descendência**, um tema central na teoria da generalização de redes neurais profundas [^2, ^1]. Como vimos anteriormente [^2], o sucesso do aprendizado profundo em regimes de superparametrização desafia a teoria estatística tradicional. A regressão por mínimos quadrados oferece um modelo simplificado para analisar como a complexidade do modelo (número de parâmetros) afeta a capacidade de ajustar os dados de treinamento e generalizar para dados não vistos. Este capítulo se baseia nos conceitos de regressão por mínimos quadrados (kernel) introduzidos na Seção 11.2 [^2], a teoria de generalização para deep neural networks treinadas por minimização do risco empírico discutida no capítulo anterior [^1] e o conceito de double descent phenomenon [^2].

### Conceitos Fundamentais
A **regressão por mínimos quadrados** busca encontrar a função que minimiza a soma dos quadrados das diferenças entre os valores previstos e os valores reais em um conjunto de dados [^2]. Formalmente, dado um conjunto de dados $(x_j, y_j)_{j=1}^m \in \mathbb{R}^d \times \mathbb{R}$ gerado por uma função *ground-truth* $f$, ou seja, $y_j = f(x_j)$ para $j = 1, ..., m$ [^2], o objetivo é encontrar uma função $\sum_{i=1}^n w_i \phi_i(x)$ que se ajuste aos dados usando mínimos quadrados lineares [^2]. Aqui, $\phi_j: \mathbb{R}^d \rightarrow \mathbb{R}$, $j \in \mathbb{N}$, representa uma sequência de **funções ansatz** [^2].

Para isso, introduzimos o *feature map* [^2]:
$$
\mathbb{R}^d \ni x \mapsto \Phi(x) := (\phi_1(x), ..., \phi_n(x))^T \in \mathbb{R}^n.
$$
O objetivo é determinar os coeficientes $w \in \mathbb{R}^n$ que minimizem o risco empírico [^3]:
$$
R_S(w) = \frac{1}{m} \sum_{j=1}^m \left( \sum_{i=1}^n w_i \phi_i(x_j) - y_j \right)^2 = \frac{1}{m} \sum_{j=1}^m \left( \Phi(x_j)^T w - y_j \right)^2.
$$
Definindo a matriz $A_n$ como [^3]:
$$
A_n := \begin{pmatrix}
\phi_1(x_1) & \cdots & \phi_n(x_1) \\
\vdots & \ddots & \vdots \\
\phi_1(x_m) & \cdots & \phi_n(x_m)
\end{pmatrix} = \begin{pmatrix}
\Phi(x_1)^T \\
\vdots \\
\Phi(x_m)^T
\end{pmatrix} \in \mathbb{R}^{m \times n},
$$
e $y = (y_1, ..., y_m)^T$, temos [^3]:
$$
R_S(w) = \frac{1}{m} ||A_n w - y||^2.
$$
Conforme discutido nas Seções 11.1-11.2 [^2], um minimizador único de $R_S(w)$ existe apenas se $A_n$ tiver *rank* $n$ [^3]. Para um minimizador $w_n$, a função ajustada é dada por [^3]:
$$
f_n(x) := \sum_{j=1}^n w_{n,j} \phi_j(x).
$$
Estamos interessados no comportamento de $f_n$ como uma função de $n$ (o número de funções ansatz/parâmetros do nosso modelo) [^3]. Distinguimos entre dois casos [^3]:

*   **Underparameterized:** Se $n < m$, temos menos parâmetros $n$ do que pontos de treinamento $m$ [^3]. Para o problema de mínimos quadrados de minimizar $R_S$, isso significa que existem mais condições $m$ do que parâmetros livres $n$ [^3]. Assim, em geral, não podemos interpolar os dados, e temos $\min_{w \in \mathbb{R}^n} R_S(w) > 0$ [^3].

*   **Overparameterized:** Se $n \geq m$, então temos pelo menos tantos parâmetros $n$ quanto pontos de treinamento $m$ [^3]. Se os $x_j$ e os $\phi_j$ são tais que $A_n \in \mathbb{R}^{m \times n}$ tem *full rank* $m$, então existe $w$ tal que $R_S(w) = 0$ [^3]. Se $n > m$, então $A_n$ necessariamente tem um *kernel* não trivial, e existem infinitas escolhas de parâmetros $w$ que produzem risco empírico zero $R_S$ [^3]. Alguns deles levam a melhores, e alguns levam a piores funções de predição $f_n$ em (15.1.4) [^3].

No caso *overparameterized*, existem muitos minimizadores de $R_S$ [^3]. O algoritmo de treinamento que usamos para calcular um minimizador determina o tipo de função de predição $f_n$ que obtemos [^3]. Para observar a **dupla descendência**, ou seja, para obter uma boa generalização para grandes $n$, precisamos escolher o minimizador cuidadosamente [^3]. Consideramos o minimizador único de norma-2 mínima, definido como [^3]:

$$
w_{n,*} = \left( \arg\min_{\{w \in \mathbb{R}^n \mid R_S(w) \leq R_S(v) \ \forall v \in \mathbb{R}^n\}} ||w|| \right) \in \mathbb{R}^n.
$$

A análise da dupla descendência envolve a observação do comportamento do erro de generalização (e.g., $||f - f_n||_{L^2}$) em função do número de parâmetros $n$ [^4]. Inicialmente, o erro diminui (regime clássico), mas após atingir um pico no *interpolation threshold* (onde $n \approx m$), o erro começa a aumentar (overfitting) e, surpreendentemente, volta a diminuir à medida que $n$ aumenta ainda mais (regime de superparametrização) [^4].

### Conclusão
A regressão por mínimos quadrados fornece uma estrutura útil para entender o fenômeno da dupla descendência [^2]. Ao variar o número de funções ansatz (parâmetros), podemos observar como a capacidade do modelo de interpolar os dados de treinamento influencia sua capacidade de generalizar [^3, ^4]. A escolha cuidadosa do minimizador (e.g., norma-2 mínima) é crucial para alcançar uma boa generalização no regime de superparametrização [^3]. A análise detalhada do comportamento do erro de generalização e da norma dos pesos em função do número de parâmetros revela insights importantes sobre a complexidade do modelo e sua capacidade de aprendizado [^4, ^6].

### Referências
[^1]: Capítulo 15: Generalization in the overparameterized regime.
[^2]: Seção 15.1: The double descent phenomenon.
[^3]: Seção 15.1.1: Least-squares regression revisited.
[^4]: Seção 15.1.2: An example.
[^5]: Figura 15.2: Illustration of the double descent phenomenon.
[^6]: Seção 15.2: Size of weights.

<!-- END -->