## Underparameterization e Overparameterization na Regressão por Mínimos Quadrados

### Introdução
Este capítulo explora o fenômeno da **underparameterization** e **overparameterization** no contexto da regressão por mínimos quadrados, um tópico fundamental para entender o comportamento de modelos de aprendizado de máquina, especialmente em relação ao fenômeno do double descent [^2]. A regressão por mínimos quadrados (kernel) foi introduzida na Seção 11.2 [^2]. O sucesso do deep learning em um regime não coberto pela teoria estatística tradicional intrigou pesquisadores por algum tempo [^2].

### Conceitos Fundamentais

Na regressão por mínimos quadrados, o objetivo é encontrar os coeficientes $w \in \mathbb{R}^n$ que minimizam o risco empírico, dado por [^3]:

$$R_s(w) = \frac{1}{m} \sum_{j=1}^m \left( \sum_{i=1}^n w_i \phi_i(x_j) - y_j \right)^2 = \frac{1}{m} ||A_n w - y||^2$$.

Onde:
*   $m$ é o número de pontos de treinamento
*   $n$ é o número de funções ansatz
*   $\phi_j: \mathbb{R}^d \rightarrow \mathbb{R}, j \in \mathbb{N}$, é uma sequência de funções ansatz [^2]. Para $n \in \mathbb{N}$, desejamos ajustar uma função $\sum_{i=1}^n w_i \phi_i(x)$ aos dados usando mínimos quadrados lineares [^2].
*   $x_j$ são os pontos de entrada
*   $y_j$ são os valores de saída correspondentes
*   $A_n$ é a matriz de design de dimensão $m \times n$, definida como [^3]:

$$A_n := \begin{pmatrix}
\phi_1(x_1) & \dots & \phi_n(x_1) \\
\vdots & \ddots & \vdots \\
\phi_1(x_m) & \dots & \phi_n(x_m)
\end{pmatrix} \in \mathbb{R}^{m \times n}$$.
*   $y = (y_1, \dots, y_m)^T$ [^3]

A natureza do ajuste do modelo depende crucialmente da relação entre $n$ e $m$ [^3]:

*   **Underparameterization (Subparametrização):** Ocorre quando o número de funções ansatz $n$ é menor que o número de pontos de treinamento $m$ ($n < m$) [^1, 3]. Nesse cenário, o modelo não tem capacidade suficiente para se ajustar perfeitamente aos dados de treinamento. Isso significa que o modelo tem menos graus de liberdade do que as condições impostas pelos dados, resultando em um erro de treinamento não nulo [^3]. Matematicamente, isso implica que $\min_{w \in \mathbb{R}^n} R_s(w) > 0$ [^3]. Para o problema de mínimos quadrados de minimizar $R_s$, isso significa que existem mais condições $m$ do que parâmetros livres $n$ [^3]. Assim, em geral, não podemos interpolar os dados [^3].

*   **Overparameterization (Superparametrização):** Ocorre quando o número de funções ansatz $n$ é maior ou igual ao número de pontos de treinamento $m$ ($n \geq m$) [^1, 3]. Nesse caso, o modelo tem capacidade suficiente para se ajustar perfeitamente aos dados de treinamento, resultando em um erro de treinamento próximo de zero [^1, 3]. No entanto, essa capacidade excessiva pode levar ao *overfitting* se o modelo não for escolhido cuidadosamente [^1]. Se $x$ e $\phi_j$ são tais que $A_n \in \mathbb{R}^{m \times n}$ tem posto completo $m$, então existe $w$ tal que $R_s(w) = 0$ [^3]. Se $n > m$, então $A_n$ necessariamente tem um kernel não trivial, e existem infinitas escolhas de parâmetros $w$ que produzem risco empírico zero $R_s$ [^3]. Algumas delas levam a melhores funções de previsão $f_n$ em (15.1.4), e algumas levam a piores [^3].

    A função ajustada para um minimizador $w_n$ é lida [^3]
    $$f_n(x) := \sum_{j=1}^n w_{n,j} \phi_j(x)$$.

    No caso superparametrizado, existem muitos minimizadores de $R_s$ [^3]. O algoritmo de treinamento que usamos para computar um minimizador determina o tipo de função de predição $f_n$ que obtemos [^3]. Para observar o double descent, isto é, para alcançar uma boa generalização para $n$ grande, precisamos escolher o minimizador cuidadosamente [^3]. No seguinte, consideramos o minimizador único de norma 2 mínima, que é definido como [^3]

    $$w_{n,*} = (\text{argmin}_{\{w \in \mathbb{R}^n \mid R_s(w) \leq R_s(v) \ \forall v \in \mathbb{R}^n\}} ||w||) \in \mathbb{R}^n$$.

### Conclusão

A distinção entre underparameterization e overparameterization é crucial para entender o trade-off entre viés e variância em modelos de aprendizado de máquina. Enquanto modelos underparameterized podem sofrer de alto viés devido à sua incapacidade de capturar a complexidade dos dados, modelos overparameterized correm o risco de overfitting, generalizando mal para dados não vistos. A escolha adequada do número de parâmetros, ou equivalentemente, o número de funções ansatz, é um passo crítico na construção de modelos de regressão por mínimos quadrados eficazes. Em [^3], um exemplo concreto é considerado. A análise do double descent, conforme explorado em [^2], oferece uma perspectiva mais profunda sobre como a overparameterization pode, surpreendentemente, levar a uma melhor generalização em certos cenários.

### Referências
[^1]: Capítulo 15, Generalization in the overparameterized regime, página 206.
[^2]: Capítulo 15, Generalization in the overparameterized regime, página 207.
[^3]: Capítulo 15, Generalization in the overparameterized regime, página 208.
<!-- END -->