## Capítulo 9: Interpolação em Redes Neurais

### Introdução
O problema de aprendizado associado à minimização do risco empírico, conforme mencionado em (1.2.3), concentra-se em minimizar o erro resultante da avaliação de uma rede neural em um conjunto *finito* de pontos (de treinamento) [^1]. Em contraste, os resultados anteriores de aproximação focaram em alcançar erros uniformemente pequenos em todo o domínio. Este capítulo explora o caso extremo do problema de aproximação, investigando as condições sob as quais é possível encontrar uma rede neural que coincida com a função alvo *f* em todos os pontos de treinamento [^1]. Este conceito é conhecido como **interpolação**.

### Conceitos Fundamentais
A **interpolação** em redes neurais envolve encontrar uma função dentro de um conjunto dado de funções H que corresponda exatamente a uma função alvo *f* em pontos de treinamento específicos $(x_i, y_i)$ [^1]. Formalmente, significa encontrar uma função $h \in H$ tal que $h(x_i) = y_i$ para todo *i*. A definição formal é dada por [^1]:

**Definição 9.1 (Interpolação):** Seja $d, m \in \mathbb{N}$, e seja $\Omega \subseteq \mathbb{R}^d$. Dizemos que um conjunto de funções $H \subseteq \{h: \Omega \rightarrow \mathbb{R}\}$ *interpola* *m* pontos em $\Omega$ se, para todo $S = \{(x_i, y_i)\}_{i=1}^m \subseteq \Omega \times \mathbb{R}$, tal que $x_i \neq x_j$ para $i \neq j$, existe uma função $h \in H$ tal que $h(x_i) = y_i$ para todo $i = 1, ..., m$.

Conhecer as propriedades de interpolação de uma arquitetura representa informações extremamente valiosas por duas razões [^1]:

*   Considere uma arquitetura que interpola *m* pontos e deixe o número de amostras de treinamento ser limitado por *m*. Então (1.2.3) sempre tem uma solução.
*   Considere novamente uma arquitetura que interpola *m* pontos e assuma que o número de amostras de treinamento é menor que *m*. Então, para cada ponto não no conjunto de treinamento e cada $y \in \mathbb{R}$, existe um minimizador *h* de (1.2.3) que satisfaz $h(x) = y$. Como consequência, sem restrições adicionais (muitas das quais discutiremos abaixo), tal arquitetura não pode generalizar para dados não vistos.

A existência de soluções para o problema de interpolação não segue trivialmente dos resultados de aproximação fornecidos nos capítulos anteriores [^1].

**Interpolação Universal (Teorema 9.3):** Sejam $d, n \in \mathbb{N}$ e seja $\sigma \in \mathcal{M}$ que não seja um polinômio. Então $\mathcal{N}_{\sigma}(0,1,n)$ interpola $n+1$ pontos em $\mathbb{R}^d$ [^2].

*Prova:* Fixe $(x_i)_{i=1}^{n+1} \subset \mathbb{R}^d$ arbitrário. Mostraremos que para qualquer $(y_i)_{i=1}^{n+1} \subset \mathbb{R}$ existem pesos e bias $(w_j)_{j=1}^n \subset \mathbb{R}^d$, $(b_j)_{j=1}^n$, $(\upsilon_j)_{j=1}^n \subset \mathbb{R}$, $c \in \mathbb{R}$ tais que [^2]:

$$\Phi(x_i) := \sum_{j=1}^{n} \upsilon_j \sigma(w_j^T x_i + b_j) + c = y_i \quad \text{para todo } i = 1, ..., n+1. \qquad (9.1.1)$$

Como $\Phi \in \mathcal{N}_{\sigma}(0,1,n)$, isso conclui a prova. Denotando [^2]:

$$A := \begin{pmatrix} 1 & \sigma(w_1^T x_1 + b_1) & \dots & \sigma(w_n^T x_1 + b_n) \\ \vdots & \vdots & \ddots & \vdots \\ 1 & \sigma(w_1^T x_{n+1} + b_1) & \dots & \sigma(w_n^T x_{n+1} + b_n) \end{pmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}. \qquad (9.1.2)$$

Então, *A* sendo regular implica que para cada $(y_i)_{i=1}^{n+1}$, existem *c* e $(\upsilon_j)_{j=1}^n$ tais que (9.1.1) se mantém. Portanto, é suficiente encontrar $(w_j)_{j=1}^n$ e $(b_j)_{j=1}^n$ tais que *A* seja regular. $\blacksquare$

O Teorema 3.8 pode ser usado para dar uma garantia de que uma arquitetura de tamanho fixo produz conjuntos de redes neurais que permitem a interpolação de *m* pontos [^2].

**Interpolação Ótima e Reconstrução (Seção 9.2):**
Considere um domínio limitado $\Omega \subseteq \mathbb{R}^d$, uma função $f : \Omega \rightarrow \mathbb{R}$, pontos distintos $x_1, ..., x_m \subseteq \Omega$ e valores de função correspondentes $y_i := f(x_i)$. O objetivo é aproximar *f* com base apenas nos pares de dados $(x_i, y_i)$, $i = 1, ..., m$ [^3]. Sob certas condições em *f*, as redes neurais ReLU podem expressar uma reconstrução "ótima" que também acaba sendo um interpolante dos dados.

**Motivação (Seção 9.2.1):**
Redes neurais com $m-1 \in \mathbb{N}$ neurônios escondidos podem interpolar *m* pontos para cada função de ativação razoável [^3]. No entanto, nem todos os interpolantes são igualmente adequados para uma determinada aplicação.

Uma maneira de formalizar a suposição de que *f* não "exibe oscilações extremas" é assumir que a constante de Lipschitz:

$$Lip(f) := \sup_{x \neq y} \frac{|f(x) - f(y)|}{||x - y||}$$

de *f* é limitada por um valor fixo $M \in \mathbb{R}$ [^3]. Aqui, $||\cdot||$ denota uma norma fixa arbitrária em $\mathbb{R}^d$. Para cada função $f : \Omega \rightarrow \mathbb{R}$ que satisfaz:

$$f(x_i) = y_i \quad \text{para todo } i = 1, ..., m, \qquad (9.2.1)$$

temos [^3]:

$$Lip(f) = \sup_{x \neq y \in \Omega} \frac{|f(x) - f(y)|}{||x - y||} \geq \sup_{i \neq j} \frac{|y_i - y_j|}{||x_i - x_j||} =: \overline{M}. \qquad (9.2.2)$$

### Conclusão

Este capítulo forneceu uma visão geral da interpolação em redes neurais, definindo formalmente o conceito e explorando as condições sob as quais a interpolação é possível. O teorema da interpolação universal (Teorema 9.3) e a discussão sobre interpolação ótima e reconstrução (Seção 9.2) destacaram os aspectos teóricos e práticos da interpolação em redes neurais. Também foi demonstrado que, sob certas suposições, as redes neurais ReLU podem expressar uma reconstrução "ótima" que também acaba sendo um interpolante dos dados.
Embora a interpolação possa garantir um ajuste perfeito aos dados de treinamento, também pode levar à falta de generalização para dados não vistos [^1].

### Referências
[^1]: Definition 9.1 (Interpolation). Let $d, m \in \mathbb{N}$, and let $\Omega \subseteq \mathbb{R}^d$. We say that a set of functions $H \subseteq \{h: \Omega \rightarrow \mathbb{R}\}$ interpolates $m$ points in $\Omega$, if for every $S = (x_i, y_i)_{i=1}^m \subseteq \Omega \times \mathbb{R}$, such that $x_i \neq x_j$ for $i \neq j$, there exists a function $h \in H$ such that $h(x_i) = y_i$ for all $i = 1, ..., m$.
[^2]: Theorem 9.3 (Universal Interpolation Theorem). Let $d, n \in \mathbb{N}$ and let $\sigma \in \mathcal{M}$ not be a polynomial. Then $\mathcal{N}_{\sigma}(0,1,n)$ interpolates $n+1$ points in $\mathbb{R}^d$.
[^3]: Consider a bounded domain $\Omega \subseteq \mathbb{R}^d$, a function $f : \Omega \rightarrow \mathbb{R}$, distinct points $x_1, ..., x_m \subseteq \Omega$, and corresponding function values $y_i := f(x_i)$. Our objective is to approximate $f$ based solely on the data pairs $(x_i, y_i)$, $i = 1, ..., m$.

<!-- END -->