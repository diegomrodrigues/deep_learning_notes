## Propriedades de Interpolação e Generalização em Redes Neurais

### Introdução
Este capítulo explora as propriedades de **interpolação** em **redes neurais**, um conceito fundamental para entender a capacidade de uma arquitetura de se ajustar aos dados de treinamento e seu potencial para **generalização** [^1]. Conforme mencionado, a capacidade de uma rede neural de interpolar um conjunto de pontos está intimamente ligada à sua habilidade de minimizar o risco empírico, um problema central no aprendizado de máquina [^1]. Discutiremos como a interpolação se relaciona com a **dimensão VC**, uma medida de complexidade do modelo, e como a densidade de um conjunto de funções não garante a interpolação [^1]. Exploraremos também o conceito de **interpolação universal** e condições sob as quais as redes neurais podem interpolar um número arbitrário de pontos [^2]. Além disso, analisaremos a **interpolação ótima** e a **reconstrução** de funções Lipschitz contínuas usando redes neurais ReLU [^3].

### Conceitos Fundamentais

#### Definição de Interpolação
Formalmente, um conjunto de funções $H$ interpola $m$ pontos em $\Omega \subseteq \mathbb{R}^d$ se, para cada conjunto de dados $S = \\{(x_i, y_i)\\}_{i=1}^m \subseteq \Omega \times \mathbb{R}$, com $x_i \neq x_j$ para $i \neq j$, existe uma função $h \in H$ tal que $h(x_i) = y_i$ para todo $i = 1, ..., m$ [^1].

#### Interpolação e Generalização
Uma arquitetura que pode interpolar *qualquer* conjunto de pontos pode estar sujeita a **overfitting**, falhando em generalizar para dados não vistos devido à falta de restrições [^1]. A existência de soluções de interpolação está relacionada à **dimensão VC**, que quantifica o número de pontos que uma rede neural com uma dada arquitetura pode interpolar, indicando sua complexidade e capacidade de se ajustar aos dados [^1].

#### A Relação entre Aproximação e Interpolação
É importante notar que a capacidade de **aproximação** não implica necessariamente a capacidade de **interpolação**. A densidade de um conjunto $H$ em $C([0, 1])$ não garante que $H$ possa interpolar sequer um único ponto em $[0, 1]$ [^1].

#### Interpolação Universal
O **Teorema da Aproximação Universal** (Capítulo 3) estabelece que redes neurais rasas podem aproximar qualquer função contínua com precisão arbitrária, desde que a largura da rede seja suficientemente grande [^2]. Surpreendentemente, este teorema pode ser usado para garantir que uma arquitetura de tamanho fixo permite que conjuntos de redes neurais interpolarem $m$ pontos.

**Teorema 9.3 (Teorema da Interpolação Universal)** Seja $d, n \in \mathbb{N}$ e seja $\sigma \in M$ (conjunto de funções de ativação permitidas) que não seja um polinômio. Então $N_1(\sigma, 1, n)$ (classe de redes neurais rasas de largura $n$) interpola $n + 1$ pontos em $\mathbb{R}^d$ [^2].

*Prova:*
Sejam $(x_i)_{i=1}^{n+1} \subset \mathbb{R}^d$ arbitrários. Mostraremos que para qualquer $(y_i)_{i=1}^{n+1} \subset \mathbb{R}$, existem pesos e bias $(w_j)_{j=1}^n \subset \mathbb{R}^d, (b_j)_{j=1}^n, (v_j)_{j=1}^n \subset \mathbb{R}, c \in \mathbb{R}$ tais que
$$\Phi(x_i) := \sum_{j=1}^n v_j \sigma(w_j^T x_i + b_j) + c = y_i \quad \text{para todo} \quad i = 1, ..., n+1.$$
Como $\Phi \in N_1(\sigma, 1, n)$, isso conclui a prova. Denotando
$$A := \begin{pmatrix} 1 & \sigma(w_1^T x_1 + b_1) & \dots & \sigma(w_n^T x_1 + b_n) \\\\ \vdots & \vdots & \ddots & \vdots \\\\ 1 & \sigma(w_1^T x_{n+1} + b_1) & \dots & \sigma(w_n^T x_{n+1} + b_n) \end{pmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}.$$
Se $A$ for regular, então para cada $(y_i)_{i=1}^{n+1}$, existem $c$ e $(v_j)_{j=1}^n$ tais que a equação acima se mantém. Portanto, basta encontrar $(w_j)_{j=1}^n$ e $(b_j)_{j=1}^n$ tais que $A$ seja regular. Para isso, procedemos por indução sobre $k = 0, ..., n$ para mostrar que existem $(w_j)_{j=1}^n$ e $(b_j)_{j=1}^n$ tais que as primeiras $k + 1$ colunas de $A$ são linearmente independentes. O caso $k = 0$ é trivial. Agora, seja $0 < k < n$ e assuma que as primeiras $k$ colunas de $A$ são linearmente independentes. Desejamos encontrar $w_k, b_k$ tais que as primeiras $k + 1$ colunas sejam linearmente independentes. Suponha que tal $w_k, b_k$ não exista e denote por $Y_k \subseteq \mathbb{R}^{n+1}$ o espaço gerado pelas primeiras $k$ colunas de $A$. Então, para todo $w \in \mathbb{R}^n, b \in \mathbb{R}$, o vetor $(\sigma(w^T x_i + b))_{i=1}^{n+1} \in \mathbb{R}^{n+1}$ deve pertencer a $Y_k$. Fixe $y = (y_i)_{i=1}^{n+1} \in \mathbb{R}^{n+1} \setminus Y_k$. Então

$$\inf_{\Phi \in N_1(\sigma, 1, n)} ||(\Phi(x_i))_{i=1}^{n+1} - y||_2 = \inf_{v_j, w_j, b_j, c} \sum_{i=1}^{n+1} (\sum_{j=1}^n v_j \sigma(w_j^T x_i + b_j) + c - y_i)^2 > \inf_{\tilde{y} \in Y_k} ||\tilde{y} - y||_2 > 0.$$

Como podemos encontrar uma função contínua $f: \mathbb{R}^d \rightarrow \mathbb{R}$ tal que $f(x_i) = y_i$ para todo $i = 1, ..., n+1$, isso contradiz o Teorema 3.8 [^2]. $\blacksquare$

#### Interpolação Ótima e Reconstrução
Nem todos os interpolantes são igualmente adequados para uma dada aplicação [^3]. Em geral, não há como determinar qual constitui uma melhor aproximação para $f$. Em particular, dada a informação limitada sobre $f$, não podemos reconstruir com precisão quaisquer características adicionais que possam existir entre os pontos de interpolação $x_1, ..., x_m$ [^3].

Uma forma de formalizar a suposição de que $f$ não exibe oscilações extremas é assumir que a constante de Lipschitz
$$Lip(f) := \sup_{x \neq y} \frac{|f(x) - f(y)|}{||x - y||}$$\né limitada por um valor fixo $M \in \mathbb{R}$ [^3]. Aqui, $|| \cdot ||$ denota uma norma fixa arbitrária em $\mathbb{R}^d$.

**Problema 9.4.** Desejamos encontrar um elemento
$$ \Phi \in \underset{h: \Omega \rightarrow \mathbb{R}}{\operatorname{argmin}} \sup_{\substack{f \in Lip_M(\Omega) \\\\ f \text{ satisfaz (9.2.1)}}} \sup_{x \in \Omega} |f(x) - h(x)|. $$
O próximo teorema mostra que uma função $\Phi$ como em (9.2.4) realmente existe [^4]. Isso não apenas permite uma fórmula explícita, mas também pertence a $Lip_M(\Omega)$ e, adicionalmente, interpola os dados. Portanto, não é apenas uma reconstrução ótima, mas também um interpolante ótimo.

**Teorema 9.5.** Sejam $m, d \in \mathbb{N}, \Omega \subseteq \mathbb{R}^d, f: \Omega \rightarrow \mathbb{R}$, e sejam $x_1, ..., x_m \in \Omega, y_1, ..., y_m \in \mathbb{R}$ que satisfazem (9.2.1) e (9.2.2) com $M > 0$. Além disso, seja $M > \tilde{M}$. Então, o Problema 9.4 tem pelo menos uma solução dada por
$$ \Phi(x) := \frac{1}{2} (f_{\text{upper}}(x) + f_{\text{lower}}(x)) \quad \text{para} \quad x \in \Omega, $$
onde
$$ f_{\text{upper}}(x) := \min_{k=1, ..., m} (y_k + M ||x - x_k||) $$
$$ f_{\text{lower}}(x) := \max_{k=1, ..., m} (y_k - M ||x - x_k||). $$
Além disso, $\Phi \in Lip_M(\Omega)$ e interpola os dados (i.e., satisfaz (9.2.1)) [^5].

### Conclusão
Este capítulo explorou as propriedades de interpolação em redes neurais, destacando a importância de entender a capacidade de uma arquitetura de se ajustar aos dados de treinamento e seu potencial para generalização [^1]. A interpolação universal e a interpolação ótima foram discutidas, fornecendo insights sobre as condições sob as quais as redes neurais podem interpolar dados e reconstruir funções [^2, 3]. A relação entre interpolação, dimensão VC e generalização foi enfatizada, mostrando como a complexidade do modelo afeta sua capacidade de se ajustar aos dados e generalizar para dados não vistos [^1].

### Referências
[^1]: Capítulo 9, página 102.
[^2]: Capítulo 9, página 103.
[^3]: Capítulo 9, página 104.
[^4]: Capítulo 9, página 105.
[^5]: Capítulo 9, página 106.
<!-- END -->