## Capítulo 9.1: Interpolação Universal em Redes Neurais

### Introdução
Este capítulo explora as propriedades de interpolação de redes neurais, um tópico fundamental para entender sua capacidade de se ajustar a dados e generalizar a partir deles. Em contraste com os resultados de aproximação que visam minimizar o erro uniformemente em todo o domínio, a interpolação foca na capacidade da rede de coincidir exatamente com os valores de uma função alvo em um conjunto finito de pontos de treinamento [^1]. Este capítulo se concentra no caso extremo da interpolação, investigando as condições sob as quais uma rede neural pode interpolar um conjunto arbitrário de pontos. Anteriormente, no Capítulo 3, Teorema 3.8, foi visto que redes neurais rasas podem aproximar qualquer função contínua com precisão arbitrária, desde que a largura da rede seja suficientemente grande. Agora, exploraremos como essa capacidade de aproximação se relaciona com a capacidade de interpolar um número finito de pontos.

### Conceitos Fundamentais
A interpolação em redes neurais levanta a questão de quais condições sobre a função de ativação e a arquitetura da rede permitem que ela interpole *m* pontos em $\mathbb{N}$ [^2]. O **Teorema da Interpolação Universal** (Teorema 9.3) fornece uma resposta a essa pergunta, garantindo que, para uma função de ativação não polinomial $\sigma$ pertencente ao conjunto de funções de ativação permitidas *M*, a classe $N(\sigma, 1, n)$ de redes neurais rasas com largura *n* pode interpolar *n + 1* pontos em $\mathbb{R}^d$ [^2].

**Teorema 9.3 (Teorema da Interpolação Universal)**. *Sejam d, n ∈ N e seja σ ∈ M não polinomial. Então N(σ, 1, n) interpola n + 1 pontos em $\mathbb{R}^d$.* [^2]

A prova deste teorema envolve demonstrar que, para qualquer conjunto de pontos $(x_i, y_i)_{i=1}^{n+1} \subseteq \mathbb{R}^d \times \mathbb{R}$, existem pesos $(w_j)_{j=1}^{n}$, bias $(b_j)_{j=1}^{n}$ e uma constante *c* tais que a saída da rede neural $\Phi(x_i)$ coincide com $y_i$ para todo *i* [^2]:
$$\
\Phi(x_i) = \sum_{j=1}^{n} v_j \sigma(w_j \cdot x_i + b_j) + c = y_i, \quad \text{para todo } i = 1, \dots, n+1. \quad (9.1.1)
$$
A chave para a prova é mostrar que a matriz *A*, construída com as ativações $\sigma(w_j \cdot x_i + b_j)$, é regular. A regularidade de *A* implica que o sistema de equações lineares pode ser resolvido para encontrar os pesos e biases que interpolam os dados [^2]. A matriz *A* é definida como:
$$\
A := \begin{bmatrix}
1 & \sigma(w_1 \cdot x_1 + b_1) & \dots & \sigma(w_n \cdot x_1 + b_n) \\\\
\vdots & \vdots & \ddots & \vdots \\\\
1 & \sigma(w_1 \cdot x_{n+1} + b_1) & \dots & \sigma(w_n \cdot x_{n+1} + b_n)
\end{bmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}. \quad (9.1.2)
$$
A prova prossegue por indução sobre *k* para mostrar que existem $(w_j)_{j=1}^{n}$ e $(b_j)_{j=1}^{n}$ tais que as primeiras *k + 1* colunas de *A* são linearmente independentes. O caso *k = 0* é trivial. Assumindo que as primeiras *k* colunas são linearmente independentes, o objetivo é encontrar $w_k$ e $b_k$ tais que as primeiras *k + 1* colunas sejam linearmente independentes [^2].

Se tais $w_k$ e $b_k$ não existirem, então o vetor $(\sigma(w \cdot x_i + b))_{i=1}^{n+1}$ deve pertencer ao espaço $Y_k$ gerado pelas primeiras *k* colunas de *A* para todo *w ∈ $\mathbb{R}^n$* e *b ∈ $\mathbb{R}$*. Isso leva a uma contradição com o Teorema 3.8, pois implicaria que a função de ativação pode ser expressa como uma combinação linear das colunas existentes, o que não é possível se $\sigma$ não for um polinômio [^2]. Portanto, existem $w_k$ e $b_k$ que tornam as primeiras *k + 1* colunas linearmente independentes.

### Conclusão
O Teorema da Interpolação Universal estabelece uma condição fundamental para a capacidade de interpolação de redes neurais rasas. Ele demonstra que, com uma largura suficiente e uma função de ativação apropriada, essas redes podem interpolar um número específico de pontos. Este resultado, juntamente com os resultados de aproximação universal, fornece uma base teórica sólida para entender o poder expressivo das redes neurais. No entanto, é importante notar que a interpolação, embora necessária, não é suficiente para garantir uma boa generalização. A capacidade de interpolar um grande número de pontos pode levar a um sobreajuste, como mencionado na introdução do capítulo, o que motiva a exploração de técnicas de regularização e outros critérios para selecionar interpolantes que generalizem bem para dados não vistos. Os próximos capítulos podem explorar essas técnicas e critérios em mais detalhes.

### Referências
[^1]: Chapter 9, Interpolation, page 102
[^2]: Chapter 9.1, Universal interpolation, page 103
<!-- END -->