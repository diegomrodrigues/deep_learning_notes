{
  "topics": [
    {
      "topic": "Wide Neural Networks and Training",
      "sub_topics": [
        "Training neural networks involves adjusting parameters to minimize a loss function, commonly the square loss objective, which quantifies the difference between the network's predictions and the actual data. Gradient descent, an iterative optimization algorithm with a constant step size h, is used to update the network's parameters w by taking steps proportional to the negative gradient of the loss function with respect to the parameters, creating a sequence of parameters (wk)k\u2208N. The objective is to understand how \u03a6(x, wk) evolves as k increases, aiming to find parameters w that minimize the difference between the neural network's output \u03a6(x, w) and the true output y for a given set of data pairs (xi, yi).",
        "Overparametrization, where the number of network parameters greatly exceeds the number of data points, can lead to strong regularization effects and enable gradient descent to find global minimizers of the loss function. Recent research suggests that neural network behavior tends to linearize in the parameters as the network width increases, allowing techniques from linear models to be applied to the training of neural networks. This analysis focuses on the case of strong overparametrization, where the network width increases while the number of data points m remains fixed. This scenario is crucial for understanding the behavior of very large neural networks. The chapter explores conditions under which gradient descent can find global minimizers for wide neural networks, particularly in cases of strong overparametrization where network width increases while keeping the number of data points fixed.",
        "Linear least-squares regression provides a foundational understanding of linear models, where the goal is to fit a linear function to the data by minimizing the sum of squared differences between the predicted and actual values; this serves as a basis for understanding more complex neural networks. The analysis involves linear least-squares regression, where the objective is to fit a linear function to the data by minimizing the square norm of the difference between the linear transformation of the parameters and the target values. For linear mappings w \u2192 \u03a6(x, w), the objective function is convex, and gradient descent can find a global minimizer, but this is generally not true for typical non-linear neural network architectures.",
        "Kernel methods introduce nonlinearity by mapping the input data into a higher-dimensional feature space, where linear models can capture complex relationships; this allows for more expressive models while retaining linearity in the parameters. The concept of a feature map allows for more expressive models capable of capturing complicated structures beyond linearity, where the nonlinearity of the feature map enables the model to learn more complex relationships in the data. The kernel trick allows for computing a minimal H-norm minimizer w in the possibly infinite-dimensional Hilbert space H, making the computation feasible. The kernel trick does not require explicit knowledge of the feature map \u03c6 nor of the minimum norm solution w* \u2208 H, but it is sufficient to choose a kernel map K. The kernel trick allows for computing a minimal H-norm minimizer in the possibly infinite-dimensional Hilbert space by revisiting the foundational representer theorem, which simplifies the computation.",
        "Common kernel functions include polynomial kernels, radial basis function (RBF) kernels, and Laplace kernels, each with different properties that make them suitable for various types of data and relationships.",
        "Tangent kernel involves linearizing the model around an initialization point and using the Taylor approximation to simplify the situation, leading to the definition of the empirical tangent kernel. The tangent kernel linearizes the neural network model around an initialization point, providing a simplified representation that captures the local behavior of the network; this linearization facilitates analysis and optimization. For general models \u03a6(x, w) where w \u2192 \u03a6(x, w) is not linear, the square loss objective function is generally not convex, and first-order methods may not be directly applicable. To simplify the situation, the model is linearized around an initialization point wo using a first-order Taylor approximation: \u03a6lin(x, w) = \u03a6(x, wo) + \u2207w\u03a6(x, wo)T(w \u2013 wo). The tangent kernel arises from the first-order Taylor approximation (the tangent) of the original model around initialization wo and depends on the choice of wo. Training the linearized model with gradient descent yields the kernel least-squares estimator with kernel \ud835\udeaan plus an additional term depending on wo. The empirical tangent kernel Kn(x, x') is defined as the inner product of the gradients of the model with respect to the parameters w, evaluated at the initial parameters wo, i.e., Kn(x, x') = (\u2207w\u03a6(x, wo), \u2207w\u03a6(x', wo)). If the full model is sufficiently close to its linearization, then gradient descent applied to the full model will find a global minimizer. This relies on assumptions about the kernel matrix of the empirical tangent kernel, bounds on the derivatives, and a relationship between the constants to ensure closeness to the linearization.",
        "Convergence to global minimizers is investigated under the assumption that the objective function is close enough to its linearization, ensuring that gradient descent can still find global minimizers.",
        "LeCun initialization sets the variance of the weights in each layer to be reciprocal to the input dimension of the layer, thereby normalizing the output variance across all network nodes; this initialization strategy is commonly used to improve training stability and performance. LeCun initialization sets the variance of the weights in each layer to be reciprocal to the input dimension of the layer, thereby normalizing the output variance across all network nodes. The initial parameters are randomly initialized with specific distributions for the weights and biases.",
        "Training dynamics for LeCun initialization focuses on the implications of Theorem 11.13 for wide neural networks, specifically shallow networks with one hidden layer, considering the architecture and initialization of the network.",
        "Neural tangent kernel analysis investigates the empirical tangent kernel of the shallow network, showing that it converges in the infinite width limit towards a specific kernel known as the neural tangent kernel (NTK).",
        "Normalized initialization involves scaling the weights in each layer to normalize the output variance across all network nodes, affecting the training dynamics and the contribution of different terms in the NTK."
      ]
    },
    {
      "topic": "Linear Least-Squares Regression",
      "sub_topics": [
        "Linear least-squares regression involves fitting a linear function \u03a6(x,w) = xTw to data by minimizing the square loss objective function. The objective function f(w) is convex, and gradient descent algorithms can be applied to find a global minimizer. The solution can be expressed as minimizing ||Aw \u2013 y||2, where A is a matrix constructed from the input data and y is the vector of target values.",
        "If the matrix A is invertible, the unique minimizer is given by w* = A\u207b\u00b9y; otherwise, if A is not full rank, a minimum norm solution is sought to ensure uniqueness.",
        "The minimum norm solution w\u2217 can be represented as a superposition of the input data points (xi)1m, lying in the span of these vectors. To ensure uniqueness when rank(A) < d, the minimum norm solution (or minimum 2-norm solution) is sought. This solution minimizes the norm of w among all possible minimizers of f. The minimum 2-norm solution of the linear least-squares problem is unique and can be represented as a superposition of the input data points (xi)1m. This solution lies in the space spanned by these input data points.",
        "Gradient descent converges to the minimum norm solution for the quadratic objective, provided the initial condition wo is within the span of the input data points. This convergence implicitly regularizes the problem."
      ]
    },
    {
      "topic": "Kernel Least-Squares Regression",
      "sub_topics": [
        "Kernel methods introduce nonlinearity in the input x while retaining linearity in the parameter w by using a feature map \u03c6: Rd \u2192 H, where H is a Hilbert space, and considering the model \u03a6(x, w) = \u27e8\u03c6(x), w\u27e9H. Kernel least-squares regression introduces nonlinearity in x while retaining linearity in the parameter w. This is achieved through a feature map \u03c6: Rd \u2192 H, where H is a Hilbert space, and the model is defined as \u03a6(x, w) = (\u03c6(x), w)H. The nonlinearity of the feature map allows for more expressive models capable of capturing complex structures beyond linearity in the data.",
        "The objective is to determine a minimizer of the function f(w) = \u03a3j=1m (\u27e8\u03c6(xj), w\u27e9 - yj)2, with a minimum H-norm solution to ensure uniqueness and regularize the problem. To ensure uniqueness and regularize the problem, the minimum H-norm solution is considered. This solution minimizes the H-norm of w among all possible minimizers of the objective function f(w).",
        "The kernel trick allows computation in the possibly infinite-dimensional Hilbert space H by using a kernel K(x, x') = \u27e8\u03c6(x), \u03c6(x')\u27e9H, which defines the inner product in the feature space without explicitly computing the feature map \u03c6.",
        "Common examples of kernels include the polynomial kernel K(x, x') = (xTx' + c)r, the radial basis function (RBF) kernel K(x, x') = exp(-c||x - x'||2), and the Laplace kernel K(x, x') = exp(-c||x - x'||).",
        "The kernel least squares estimator is defined as \u03a6(x, w*) = (\u03c6(x), w*)H, where w* is the minimum H-norm solution. The representer theorem states that there is a unique minimum H-norm solution w* \u2208 H of the kernel least-squares problem. This solution lies in the span of the feature maps of the input data points, i.e., H := span{\u03c6(x1), ..., \u03c6(xm)}."
      ]
    },
    {
      "topic": "Tangent Kernel Analysis",
      "sub_topics": [
        "For general models \u03a6(x, w) where w \u2192 \u03a6(x, w) is not linear, the square loss objective function is generally not convex, and first-order methods may not be directly applicable.",
        "To simplify the situation, the model is linearized around an initialization point wo using a first-order Taylor approximation: \u03a6lin(x, w) = \u03a6(x, wo) + \u2207w\u03a6(x, wo)T(w \u2013 wo).",
        "Minimizing the square loss for this linearized model corresponds to a kernel least squares regression with a feature map \u03c6(x) = \u2207w\u03a6(x, wo), and the corresponding kernel is the empirical tangent kernel Kn(x, x') = \u27e8\u2207w\u03a6(x, wo), \u2207w\u03a6(x', wo)\u27e9. The empirical tangent kernel Kn(x, x') is defined as the inner product of the gradients of the model with respect to the parameters w, evaluated at the initial parameters wo, i.e., Kn(x, x') = (\u2207w\u03a6(x, wo), \u2207w\u03a6(x', wo)).",
        "The tangent kernel depends on the choice of the initialization wo, and training the linearized model with gradient descent yields the kernel least-squares estimator with kernel \ud835\udeaan plus an additional term depending on wo.",
        "The regularity of the kernel matrix in Assumption 11.12 (a) is equivalent to the gradients of the model with respect to the parameters having full rank m \u2264 n. This condition guarantees that there exists w such that the linearized model can interpolate the data.",
        "The closeness condition of \u03a6 and \u03a6lin is formalized in Assumption 11.12 (b), which assumes that w \u2192 \u03a6(xi, w) is L-smooth in a ball of radius r > 0 around wo, for all i = 1, ..., m. This allows to control how far \u03a6(xi, w) and \u03a6lin(xi, w) and their derivatives may deviate from each other for w in this ball.",
        "Assumption 11.12 (c) ties together all constants, ensuring the full model to be sufficiently close to its linearization in a large enough neighborhood of w0. This condition is crucial for the convergence of gradient descent to a global minimizer."
      ]
    },
    {
      "topic": "LeCun Initialization and Training Dynamics",
      "sub_topics": [
        "LeCun initialization sets the variance of the weights in each layer to be reciprocal to the input dimension of the layer, thereby normalizing the output variance across all network nodes. The initial parameters are randomly initialized with specific distributions for the weights and biases.",
        "The neural tangent kernel (NTK) is a specific kernel that arises in the infinite width limit of neural networks. Its precise formula depends on the architecture and initialization. For LeCun initialization, the NTK is denoted by KLC.",
        "The theorem provides conditions under which the empirical tangent kernel converges to the neural tangent kernel in the infinite width limit. It requires that the activation function and its derivative satisfy certain bounds and that the input data is well-behaved.",
        "The theorem states that under certain assumptions, gradient descent converges to a global minimizer and the limiting network achieves zero loss, i.e., interpolates the data. Moreover, during training the network weights remain close to initialization if the network width n is large."
      ]
    }
  ]
}