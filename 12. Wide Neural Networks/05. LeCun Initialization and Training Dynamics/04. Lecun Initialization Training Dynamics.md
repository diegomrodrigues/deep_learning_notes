## Convergence Analysis of Gradient Descent with LeCun Initialization in Wide Neural Networks

### Introdução
Este capítulo explora a dinâmica do treinamento de redes neurais de grande largura, focando na situação em que temos pares de dados $(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$, com $i \in \{1, ..., m\}$, e desejamos treinar uma rede neural $\Phi(x, w)$ dependendo da entrada $x \in \mathbb{R}^d$ e dos parâmetros $w \in \mathbb{R}^n$, minimizando o objetivo de perda quadrática definido como [^1]:

$$f(w) := \sum_{i=1}^{m} (\Phi(x_i, w) - y_i)^2$$

Este capítulo investiga se o *gradient descent* aplicado a $f(w)$ encontra um minimizador global sob certas condições, e se a rede limitante interpola os dados [^8]. Além disso, durante o treinamento, os pesos da rede permanecem próximos da inicialização se a largura da rede $n$ for grande [^8]. Essa análise trata o caso de forte *overparametrization*, especificamente quando a largura da rede aumenta enquanto o número de pontos de dados $m$ é mantido fixo [^1].

### Conceitos Fundamentais

**Teorema da Convergência Global (Teorema 11.13)**
Sob certas condições, o *gradient descent* converge para um minimizador global e a rede limitante alcança perda zero, ou seja, interpola os dados [^8]. Além disso, durante o treinamento, os pesos da rede permanecem próximos da inicialização se a largura da rede $n$ for grande [^8].

**Suposições Necessárias (Assumption 11.12)**
Para garantir a convergência global, algumas suposições são necessárias [^8]:
1.  $\Phi \in C^1(\mathbb{R}^d \times \mathbb{R}^n)$ e $w_0 \in \mathbb{R}^n$.
2.  Existem constantes $r > 0$, $U, L < \infty$ e $0 < \lambda_{min} < \lambda_{max} < \infty$ tais que:
    *   (a) A matriz do kernel tangente empírico $(K_n(x_i, x_j))_{i,j=1}^m = ((\nabla_w\Phi(x_i, w_0), \nabla_w\Phi(x_j, w_0)))_{i,j=1}^m \in \mathbb{R}^{m \times m}$ é regular e seus autovalores pertencem a $[\lambda_{min}, \lambda_{max}]$ [^8, 9].
    *   (b) Para todo $i \in \{1, ..., m\}$:
        *   $\| \nabla_w \Phi(x_i, w) \| \leq U$ para todo $w \in B_r(w_0)$ [^9].
        *   $\| \nabla_w \Phi(x_i, w) - \nabla_w \Phi(x_i, v) \| \leq L \| w - v \|$ para todo $w, v \in B_r(w_0)$ [^9].
    *   (c) $L < \frac{\lambda_{min}}{12m^{3/2}U^2 \sqrt{f(w_0)}}$ e $r = \frac{2\sqrt{m}U}{\lambda_{min}} \sqrt{f(w_0)}$ [^9].

**Taxa de Aprendizagem**
A taxa de aprendizagem $h$ deve ser escolhida de acordo com a seguinte condição [^10]:

$$h < \frac{1}{\lambda_{min} + \lambda_{max}}$$

**Convergência e Estabilidade (Teorema 11.13)**
Sob as condições de Assumption 11.12 e com a taxa de aprendizagem $h$ apropriada, o seguinte é válido para todo $k \in \mathbb{N}$ [^10]:
* $\|w_k - w_0\| \leq \frac{2\sqrt{m}U}{\lambda_{min}} \sqrt{f(w_0)}$ [^10].
* $f(w_k) \leq (1 - h\lambda_{min})^{2k} f(w_0)$ [^10].

**Inicialização de LeCun (LeCun Initialization)**
É uma técnica de inicialização para redes neurais que visa normalizar a variância da saída de cada nó na inicialização. A seguinte condição na distribuição usada para esta inicialização será assumida ao longo do restante da Seção 11.5 [^12]:
* Assumption 11.14. A distribuição $D$ em $\mathbb{R}$ tem expectativa zero, variância um e momentos finitos até a ordem oito [^12].

Para indicar explicitamente a expectativa e a variância na notação, também escrevemos $D(0, 1)$ em vez de $D$, e para $\mu \in \mathbb{R}$ e $s > 0$ usamos $D(\mu, s^2)$ para denotar a medida escalonada e deslocada correspondente com expectativa $\mu$ e variância $s^2$; assim, se $X \sim D(0, 1)$ então $\mu + sX \sim D(\mu, s^2)$ [^12]. A inicialização de LeCun [127] define a variância dos pesos em cada camada para ser recíproca da dimensão de entrada da camada, normalizando assim a variância de saída em todos os nós da rede [^12]. Os parâmetros iniciais são, portanto, inicializados aleatoriamente com componentes [^12]:
$$U_{0;ij} \stackrel{iid}{\sim} D(0, \frac{1}{d}), \quad v_{0;i} \stackrel{iid}{\sim} D(0, 1), \quad b_{0;i}, c_0 = 0,$$
independentemente para todo $i = 1, ..., n, j = 1, ..., d$ [^12].

**Proximidade ao Modelo Linearizado (Proximity to linearized model)**
A análise até agora foi baseada na linearização $\Phi^{lin}$ descrevendo o comportamento da rede completa bem em uma vizinhança dos parâmetros iniciais $w_0$ [^17]. Além disso, o Teorema 11.23 afirma que os parâmetros permanecem em uma vizinhança $O(n^{-1/2})$ de $w_0$ durante o treinamento [^17]. Isso sugere que o modelo completo treinado $\lim_{k \to \infty} \Phi(x, w_k)$ produz previsões semelhantes ao modelo linearizado treinado [^17].

### Conclusão
Este capítulo forneceu uma análise rigorosa da convergência do *gradient descent* em redes neurais amplas com inicialização de LeCun. As principais conclusões são que, sob certas condições de regularidade e com uma taxa de aprendizagem apropriada, o *gradient descent* converge para um minimizador global, e a rede limitante interpola os dados. Além disso, os pesos da rede permanecem próximos da inicialização durante o treinamento se a largura da rede for grande. A análise também destaca a importância da inicialização de LeCun para garantir a convergência e a estabilidade do treinamento.

### Referências
[^1]: Capítulo 11, p. 139
[^8]: Capítulo 11, p. 146
[^9]: Capítulo 11, p. 147
[^10]: Capítulo 11, p. 148
[^12]: Capítulo 11, p. 150
[^17]: Capítulo 11, p. 157
<!-- END -->