## O Teorema do Representante em Regressão Kernel Least-Squares

### Introdução
Este capítulo aprofunda o conceito de **regressão Kernel Least-Squares (KRLS)**, com foco no **Teorema do Representante**. KRLS é uma técnica poderosa para modelagem não linear que mantém a linearidade nos parâmetros, permitindo o uso eficiente de métodos de otimização [^1]. O Teorema do Representante é fundamental para KRLS, pois garante que a solução ótima para o problema de regressão pode ser expressa como uma combinação linear das funções de características dos pontos de dados de entrada [^1]. Isso simplifica enormemente o problema de otimização, reduzindo-o de um espaço de Hilbert possivelmente infinito-dimensional para um espaço de dimensão finita, limitada pelo número de amostras de treinamento [^3].

### Conceitos Fundamentais
O estimador de **Kernel Least Squares** é definido como $\Phi(x, w^*) = (\phi(x), w^*)_H$, onde $w^*$ é a solução de norma-H mínima [^1]. Em outras palavras, busca-se o vetor $w^*$ no espaço de Hilbert $H$ que minimiza o erro quadrático médio, ao mesmo tempo em que mantém a norma de $w^*$ a menor possível [^3]. A minimização da norma-H atua como uma forma de regularização, prevenindo o overfitting e promovendo a generalização [^2].

O **Teorema do Representante** afirma que existe uma solução única $w^* \in H$ de norma-H mínima para o problema de kernel least-squares [^1]. Mais importante, essa solução reside no espaço gerado pelos mapas de características dos pontos de dados de entrada, ou seja, $H := \text{span}\{\phi(x_1), ..., \phi(x_m)\}$ [^1]. Isso significa que, em vez de procurar a solução ótima em todo o espaço de Hilbert $H$, podemos restringir nossa busca ao subespaço gerado pelas combinações lineares das características dos dados de treinamento [^3].

**Formalização Matemática**

Dado um conjunto de dados $\{(x_i, y_i)\}_{i=1}^m$, onde $x_i \in \mathbb{R}^d$ e $y_i \in \mathbb{R}$, o objetivo da regressão kernel least-squares é encontrar uma função $f(x) = (\phi(x), w^*)_H$ que minimize o seguinte funcional de risco:

$$
f(w) := \sum_{j=1}^m ((\phi(x_j), w)_H - y_j)^2,
$$

sujeito a $w \in H$ [^3]. O Teorema do Representante garante que a solução $w^*$ tem a forma:

$$
w^* = \sum_{i=1}^m \alpha_i \phi(x_i),
$$

onde $\alpha_i \in \mathbb{R}$ são coeficientes a serem determinados [^5]. Substituindo esta expressão na função de predição, obtemos:

$$
\Phi(x, w^*) = (\phi(x), w^*)_H = \sum_{i=1}^m \alpha_i (\phi(x), \phi(x_i))_H = \sum_{i=1}^m \alpha_i K(x, x_i),
$$

onde $K(x, x_i) = (\phi(x), \phi(x_i))_H$ é a função kernel [^6]. Essa formulação transforma o problema de otimização original em um problema de encontrar os coeficientes $\alpha_i$ que minimizam o erro quadrático médio [^6].

**Prova do Teorema do Representante**
A prova do Teorema do Representante, conforme apresentada na Proposição 11.2 [^2], envolve decompor o espaço de Hilbert $H$ em dois subespaços ortogonais: $H$, o espaço gerado pelos mapas de características dos pontos de dados, e seu complemento ortogonal $H^\perp$ [^2]. Qualquer vetor $w \in H$ pode ser decomposto como $w = \tilde{w} + \hat{w}$, onde $\tilde{w} \in H$ e $\hat{w} \in H^\perp$ [^2]. A chave da prova é mostrar que a componente $\hat{w}$ não contribui para a função de predição e, portanto, pode ser definida como zero sem perda de otimalidade [^2].

**Exemplo:**

Considere um kernel Gaussiano (RBF): $K(x, x') = \exp(-c||x - x'||^2)$ [^7]. Neste caso, o Teorema do Representante nos diz que a função de regressão pode ser escrita como:

$$
f(x) = \sum_{i=1}^m \alpha_i \exp(-c||x - x_i||^2).
$$
Os coeficientes $\alpha_i$ são determinados minimizando o erro quadrático médio no conjunto de treinamento [^7].

### Conclusão
O Teorema do Representante é uma ferramenta crucial na regressão kernel least-squares, permitindo uma representação eficiente das soluções ótimas [^1]. Ele reduz a complexidade do problema de otimização, restringindo a busca da solução a um subespaço de dimensão finita, o que facilita a aplicação de métodos de otimização eficientes [^3]. Além disso, a formulação resultante em termos de funções kernel permite lidar com dados de alta dimensão e relações não lineares de forma eficaz [^6]. A escolha do kernel apropriado e a otimização dos coeficientes $\alpha_i$ são passos críticos para obter um bom desempenho em problemas de regressão com KRLS [^7].

### Referências
[^1]: Page 3, Paragraph 1
[^2]: Page 2, Proposition 11.2
[^3]: Page 3, Paragraph 1
[^4]: Page 5, Theorem 11.7
[^5]: Page 5, Paragraph 3
[^6]: Page 6, Definition 11.8
[^7]: Page 7, Example 11.10
<!-- END -->