## Kernel Least-Squares Regression: Feature Maps and Nonlinearity

### Introdução
Este capítulo explora o uso de **feature maps** para introduzir não-linearidade em modelos de regressão linear, especificamente no contexto de **Kernel Least-Squares Regression** [^1]. A regressão linear, embora simples e computacionalmente eficiente, muitas vezes falha em capturar relações complexas presentes em dados reais [^1]. Os métodos de kernel oferecem uma maneira de superar essa limitação, mantendo a linearidade nos parâmetros do modelo, ao mesmo tempo em que permitem modelar relações não-lineares nos dados de entrada [^1].

### Conceitos Fundamentais
O conceito central da **Kernel Least-Squares Regression** é o uso de uma **feature map** $\phi: \mathbb{R}^d \rightarrow \mathcal{H}$, onde $\mathcal{H}$ é um espaço de Hilbert [^1]. Essa **feature map** transforma os dados de entrada $x \in \mathbb{R}^d$ em um espaço de Hilbert de dimensão possivelmente superior, onde as relações podem ser mais facilmente modeladas linearmente. O modelo é definido como [^1]:

$$\Phi(x, w) = \langle \phi(x), w \rangle_{\mathcal{H}}$$

onde $w \in \mathcal{H}$ são os parâmetros do modelo. A não-linearidade da **feature map** $\phi$ permite que o modelo $\Phi(x, w)$ capture estruturas complexas além da linearidade nos dados [^1].

**Espaços de Hilbert e Produtos Internos**

Um **espaço de Hilbert** é um espaço vetorial que possui um produto interno e é completo, ou seja, toda sequência de Cauchy converge para um elemento dentro do espaço. O produto interno $\langle \cdot, \cdot \rangle_{\mathcal{H}}$ define uma noção de ângulo e comprimento em $\mathcal{H}$, permitindo a aplicação de técnicas lineares nesse espaço, mesmo quando a **feature map** $\phi$ é não-linear.

**Kernel Trick**

Um dos aspectos mais poderosos dos métodos de kernel é o **kernel trick**. Em vez de calcular explicitamente a **feature map** $\phi(x)$, podemos definir um kernel $K(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}$ [^1]. O kernel calcula o produto interno no espaço de Hilbert sem precisar conhecer $\phi(x)$ explicitamente. Isso é particularmente útil quando $\mathcal{H}$ tem dimensão muito alta ou infinita, tornando o cálculo direto de $\phi(x)$ inviável [^1].

**Exemplos de Feature Maps e Kernels [^1]**

*   **Polynomial Kernel:** $K(x, x') = (x^T x' + c)^r$, onde $c \geq 0$ e $r \in \mathbb{N}$.
*   **Radial Basis Function (RBF) Kernel:** $K(x, x') = \exp(-c||x - x'||^2)$, onde $c > 0$.
*   **Laplace Kernel:** $K(x, x') = \exp(-c||x - x||)$, onde $c > 0$.

**Regularização e Unicidade [^1]**
Para garantir a unicidade da solução e regularizar o problema, busca-se a solução de norma mínima no espaço de Hilbert:

$$w^* = \underset{w \in \mathcal{H}}{\text{argmin}} \\{f(w) \leq f(v) \\ \forall v \in \mathcal{H}\\} ||w||_{\mathcal{H}}$$

onde $f(w)$ é a função objetivo, definida como [^1]:

$$f(w) = \sum_{j=1}^m (\langle \phi(x_j), w \rangle_{\mathcal{H}} - y_j)^2$$

**Representer Theorem [^5]**
O **Representer Theorem** garante que a solução ótima $w^*$ pode ser expressa como uma combinação linear das **feature maps** dos dados de treinamento [^5]:

$$w^* = \sum_{i=1}^m \alpha_i \phi(x_i)$$

Isso reduz o problema de otimização de um espaço de Hilbert possivelmente infinito para um espaço de dimensão finita, facilitando a computação da solução [^5].

**Algoritmo para Kernel Least-Squares [^6]**
1.  Compute a matriz de kernel $G = (K(x_i, x_j))_{i,j=1}^m$ [^6].
2.  Determine o minimizador $\alpha \in \mathbb{R}^m$ de $||G\alpha - y||^2$ [^6].
3.  Avalie o modelo $\Phi(x, w^*)$ usando $\Phi(x, w^*) = \sum_{j=1}^m \alpha_j K(x, x_j)$ [^6].

### Conclusão
A **Kernel Least-Squares Regression** oferece uma abordagem poderosa para modelar relações não-lineares em dados, mantendo a linearidade nos parâmetros do modelo através do uso de **feature maps** e do **kernel trick**. O **Representer Theorem** simplifica o problema de otimização, permitindo a solução em espaços de dimensão finita. A escolha do kernel apropriado é crucial para o desempenho do modelo, e diferentes kernels podem ser mais adequados para diferentes tipos de dados e problemas [^1].

### Referências
[^1]: Capítulo 11, Seção 11.2
[^5]: Capítulo 11, Teorema 11.7
[^6]: Capítulo 11, Seção 11.2, passos do algoritmo

<!-- END -->