## Kernel Least-Squares Regression: Regularização e Solução de Norma Mínima

### Introdução
Este capítulo aprofunda o conceito de **Kernel Least-Squares Regression (KLSR)**, focando na importância da **regularização** e na obtenção de uma **solução de norma mínima** [^1]. O KLSR, apresentado na seção 11.2 [^1], visa estender a regressão linear a modelos não lineares, mantendo a linearidade no parâmetro *w*. A regularização, neste contexto, é crucial para garantir a unicidade da solução e evitar o *overfitting*, especialmente quando a dimensionalidade do espaço de características é alta ou infinita [^3]. Exploraremos como a minimização da norma de Hilbert (H-norm) de *w* desempenha um papel fundamental na obtenção de soluções estáveis e generalizáveis.

### Conceitos Fundamentais
O objetivo central é determinar um minimizador da função [^3]:
$$f(w) = \sum_{j=1}^m (\langle \phi(x_j), w \rangle - y_j)^2,$$
onde $\phi$ é um *feature map* que mapeia os dados de entrada $x_j$ para um espaço de Hilbert H, e *w* é o parâmetro a ser otimizado. Para garantir a unicidade e regularizar o problema, considera-se a solução de norma H mínima [^3], ou seja:
$$w_* := \text{argmin}_{w \in H | f(w) \leq f(v) \forall v \in H} ||w||_H.$$
Essa solução minimiza a H-norm de *w* entre todos os minimizadores possíveis da função objetivo $f(w)$ [^3].

**Regularização por Norma Mínima:**
A condição de minimizar a H-norm atua como uma forma de **regularização** [^3]. Em vez de encontrar um minimizador "arbitrário" de $f(w)$, buscamos aquele com a menor complexidade, medida pela sua norma em H. Esta abordagem é particularmente útil quando o *feature map* $\phi$ mapeia os dados para um espaço de alta dimensionalidade, onde soluções com normas maiores podem levar a *overfitting*.

**Representação da Solução:**
A **Proposição 11.2** [^2] e o **Teorema 11.7** [^5] fornecem insights cruciais sobre a estrutura da solução de norma mínima. O Teorema 11.7, em particular, afirma que existe uma única solução de norma H mínima $w_* \in H$ de (11.2.2) [^5]. Além disso, com $\tilde{H} := \text{span}\{\phi(x_1), ..., \phi(x_m)\}$, essa solução é o elemento único:
$$w_* = \text{argmin}_{w \in \tilde{H}} f(w) \in \tilde{H}.$$
Este resultado é fundamental porque restringe a busca da solução ao subespaço $\tilde{H}$, que é gerado pelas imagens dos dados de treinamento através do *feature map* $\phi$. Isso significa que $w_*$ pode ser expresso como uma combinação linear dos $\phi(x_i)$, simplificando significativamente o problema de otimização.

**Kernel Trick:**
A representação da solução em termos de combinações lineares dos $\phi(x_i)$ leva naturalmente ao **kernel trick**. Definindo um kernel $K(x, x') := \langle \phi(x), \phi(x') \rangle_H$ [^6], podemos reescrever a função objetivo e a solução em termos de *K*, sem a necessidade de explicitar o *feature map* $\phi$ ou o espaço de Hilbert H. Isso permite trabalhar com espaços de características de dimensão infinita, desde que o kernel *K* seja bem definido.

**Algoritmo para Kernel Least-Squares:**
A **Proposição 11.9** [^6] sugere o seguinte algoritmo para computar o estimador KLSR:
1. Calcular a matriz do kernel $G = (K(x_i, x_j))_{i,j=1}^m$ [^7].
2. Determinar um minimizador $\alpha \in \mathbb{R}^m$ de $||G\alpha - y||^2$ [^7].
3. Avaliar $\Phi(x, w_*)$ via $\Phi(x, w_*) = \sum_{j=1}^m \alpha_j K(x, x_j)$ [^7].

### Conclusão
A regularização via minimização da H-norm é uma técnica essencial no contexto de Kernel Least-Squares Regression. Ela garante a unicidade da solução e evita o *overfitting*, permitindo a construção de modelos robustos e generalizáveis. O kernel trick, juntamente com o teorema do representador, simplifica significativamente o problema de otimização, permitindo trabalhar com espaços de características de dimensão infinita. A combinação dessas técnicas torna o KLSR uma ferramenta poderosa para modelagem não linear.

### Referências
[^1]: Página 139
[^2]: Página 140
[^3]: Página 141
[^4]: Página 141
[^5]: Página 143
[^6]: Página 144
[^7]: Página 145
<!-- END -->