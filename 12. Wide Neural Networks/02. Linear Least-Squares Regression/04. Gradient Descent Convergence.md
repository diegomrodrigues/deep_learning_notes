## Convergência do Gradiente Descendente para a Solução de Norma Mínima em Regressão Linear por Mínimos Quadrados

### Introdução

Este capítulo aprofunda-se no comportamento do gradiente descendente aplicado à regressão linear por mínimos quadrados, explorando como ele converge para a solução de norma mínima sob certas condições. Conforme estabelecido no Capítulo 10, o gradiente descendente é uma técnica fundamental para encontrar minimizadores de funções convexas [^1]. Em particular, analisaremos como a inicialização dos parâmetros dentro do espaço gerado pelos pontos de dados de entrada influencia a solução para a qual o algoritmo converge, e como isso leva a uma forma implícita de regularização. Este resultado é crucial para entender o comportamento de redes neurais de largura infinita, onde a linearização do modelo torna-se uma aproximação válida [^1].

### Conceitos Fundamentais

Na regressão linear por mínimos quadrados, dada uma coleção de pares de dados $(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$, com $i \in \{1, ..., m\}$ [^1], o objetivo é encontrar um vetor de parâmetros $w \in \mathbb{R}^d$ que minimize a função de custo do erro quadrático médio:

$$
f(w) = \sum_{i=1}^{m} (\Phi(x_i, w) - y_i)^2,
$$

onde $\Phi(x, w) = x^T w$ é uma função linear [^1]. Podemos expressar essa função de custo em forma matricial, definindo:

$$
A = \begin{bmatrix} x_1^T \\ \vdots \\ x_m^T \end{bmatrix} \in \mathbb{R}^{m \times d} \quad \text{e} \quad y = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix} \in \mathbb{R}^m.
$$

Assim, a função de custo se torna:

$$
f(w) = ||Aw - y||^2.
$$

Se a matriz $A$ for invertível, a solução que minimiza $f(w)$ é dada por $w^* = A^{-1}y$ [^1]. No entanto, se $A$ não for invertível (ou seja, $\text{rank}(A) < d$), existem infinitas soluções que minimizam $f(w)$ [^1]. Para garantir a unicidade, procuramos a solução de norma mínima:

$$
w^* = \text{argmin}_{w \in \mathbb{R}^d \mid f(w) \le f(v) \ \forall v \in \mathbb{R}^d} ||w||.
$$

Essa condição de minimizar a norma $L_2$ atua como uma forma de **regularização** [^3], selecionando a solução mais "simples" entre todas as possíveis.

**Teorema da Convergência do Gradiente Descendente:**

O teorema a seguir, extraído de [^3], formaliza a convergência do gradiente descendente para a solução de norma mínima sob certas condições.

**Teorema 11.3:** Seja $A \in \mathbb{R}^{m \times d}$ como definido anteriormente, e seja $w_0 = \tilde{w}_0 + \hat{w}_0$, onde $\tilde{w}_0 \in H = \text{span}\{x_1, ..., x_m\}$ e $\hat{w}_0 \in H^\perp$ [^3]. Fixe $h \in (0, 1/(2\sigma_{\text{max}}(A)^2))$ e defina a sequência iterativa:

$$
w_{k+1} = w_k - h \nabla f(w_k) \quad \text{para todo} \ k \in \mathbb{N},
$$

onde $f$ é a função de custo definida em (11.1.2) [^2]. Então:

$$
\lim_{k \to \infty} w_k = w^* + \hat{w}_0,
$$

onde $w^*$ é a solução de norma mínima em $H$ [^3].

*Prova (Esboço):*

O gradiente da função objetivo é dado por $\nabla f(w) = 2A^T(Aw - y)$ [^3]. Se $w_0 \in H$, então as iterações do gradiente descendente permanecerão no subespaço $H$ [^3]. Para um tamanho de passo $h$ suficientemente pequeno, $f(w_k)$ converge para 0. Pela Proposição 11.2 [^2], existe apenas um minimizador em $H$, que corresponde à solução de norma mínima. Portanto, $w_k$ converge para a solução de norma mínima [^3]. $\blacksquare$

**Lemma 1:** *Se $w_0 \in H$, então $w_k \in H$ para todo $k \in \mathbb{N}$.*

*Prova:* Por indução. Para $k=0$, $w_0 \in H$ por hipótese. Assuma que $w_k \in H$. Então, $Aw_k$ é uma combinação linear das colunas de $A$, e portanto $A^T(Aw_k - y)$ é uma combinação linear das linhas de $A^T$, que são exatamente os vetores $x_i$. Portanto, $w_{k+1} = w_k - h\nabla f(w_k) = w_k - 2hA^T(Aw_k - y)$ é uma combinação linear dos $x_i$, e assim $w_{k+1} \in H$. $\blacksquare$

**Lemma 2:** *Se $h \in (0, 1/(2\sigma_{\text{max}}(A)^2))$, então $f(w_k)$ é uma sequência não crescente e converge para 0.*

*Prova:* Esta é uma consequência direta do Exercício 10.34 e do Teorema 10.11 [^1], que estabelecem a convergência do gradiente descendente para funções convexas com tamanho de passo adequado. $\blacksquare$

**Corolário:** Se a condição inicial $w_0$ for o vetor zero, $w_0 = 0$, então o gradiente descendente converge para a solução de norma mínima $w^*$.

*Prova:* Se $w_0 = 0$, então $w_0 \in H$, pois $0$ é uma combinação linear trivial dos vetores $x_i$. Portanto, pelo Teorema 11.3, o gradiente descendente converge para a solução de norma mínima. $\blacksquare$

### Conclusão

A convergência do gradiente descendente para a solução de norma mínima em regressão linear por mínimos quadrados é um resultado importante que destaca como a inicialização e o próprio algoritmo podem introduzir uma forma implícita de regularização [^3]. Ao inicializar os parâmetros dentro do espaço gerado pelos dados de entrada, o gradiente descendente automaticamente seleciona a solução mais simples (em termos de norma $L_2$) que minimiza o erro quadrático médio. Este princípio é fundamental para entender o comportamento de modelos mais complexos, como redes neurais de largura infinita, onde a linearização e a regularização implícita desempenham um papel crucial [^1]. A análise apresentada aqui fornece uma base sólida para explorar tópicos mais avançados, como os métodos de kernel e a generalização em modelos de aprendizado de máquina.

### Referências
[^1]: Capítulo 10
[^2]: (11.1.2)
[^3]: Teorema 11.3
<!-- END -->