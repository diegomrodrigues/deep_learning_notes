## Solução de Norma Mínima em Regressão Linear por Mínimos Quadrados

### Introdução
Em regressão linear por mínimos quadrados, o objetivo é encontrar um vetor de parâmetros **w** que minimize a soma dos quadrados dos erros entre as predições do modelo linear e os valores observados [^1]. Quando a matriz do sistema linear não possui posto completo, a solução que minimiza o erro não é única. Nesses casos, busca-se a **solução de norma mínima**, que garante a unicidade e possui propriedades interessantes [^2]. Este capítulo explora em detalhes a solução de norma mínima, sua representação e sua relevância no contexto da regressão linear por mínimos quadrados.

### Conceitos Fundamentais
#### Existência e Unicidade
Considere o problema de mínimos quadrados linear definido por:
$$
f(w) = ||Aw - y||^2,
$$
onde $A \in \mathbb{R}^{m \times d}$ é a matriz de design, $y \in \mathbb{R}^m$ é o vetor de observações e $w \in \mathbb{R}^d$ é o vetor de parâmetros a ser estimado [^2]. Se a matriz $A$ não for invertível, ou seja, se $rank(A) < d$, então o núcleo de $A$ (denotado por $ker(A)$) é não trivial, o que implica que existem infinitos minimizadores de $f$ [^2].

Para garantir a unicidade da solução, define-se a **solução de norma mínima** como:
$$
w^* := \underset{w \in \mathbb{R}^d | f(w) \leq f(v) \forall v \in \mathbb{R}^d}{\operatorname{argmin}} ||w||.
$$
Essa solução minimiza a norma de $w$ entre todos os possíveis minimizadores de $f$ [^2].

**Proposição 11.2 [^2]:** *Seja $A \in \mathbb{R}^{m \times d}$ e $y \in \mathbb{R}^m$ como em (11.1.1). Existe uma única solução de norma mínima 2 de (11.1.2). Denotando $H := span\{x_1, ..., x_m\} \subseteq \mathbb{R}^d$, é o elemento único*
$$
w^* = \underset{\tilde{w} \in H}{\operatorname{argmin}} f(\tilde{w}) \in H.
$$

*Prova.* Seja $C \subseteq \mathbb{R}^m$ o espaço gerado pelas colunas de $A$. Então, $C$ é fechado e convexo, e portanto $y^* = \underset{\tilde{y} \in C}{\operatorname{argmin}} ||y - \tilde{y}||$ existe e é único (esta é uma propriedade fundamental dos espaços de Hilbert, veja Teorema B.14). Em particular, o conjunto $M = \{w \in \mathbb{R}^d | Aw = y^*\} \subseteq \mathbb{R}^d$ de minimizadores de $f$ é não vazio. Claramente, $M$ também é fechado e convexo. Pelo mesmo argumento anterior, $w^* = \underset{w \in M}{\operatorname{argmin}} ||w^*||$ existe e é único.

Resta mostrar (11.1.4). Denotemos por $w^*$ a solução de norma mínima e decomponhamos $w^* = \tilde{w} + \hat{w}$ com $\tilde{w} \in H$ e $\hat{w} \in H^\perp$. Temos $Aw^* = A\tilde{w}$ e $||w^*||^2 = ||\tilde{w}||^2 + ||\hat{w}||^2$. Como $w^*$ é a solução de norma mínima, deve-se ter $\hat{w} = 0$. Assim, $w^* \in H$. Finalmente, suponha que exista um minimizador $v$ de $f$ em $H$ diferente de $w^*$. Então $0 \neq w^* - v \in H$, e como $H$ é gerado pelas linhas de $A$, temos $A(w^* - v) \neq 0$. Assim, $y^* = Aw^* \neq Av$, o que contradiz o fato de que $v$ minimiza $f$. $\blacksquare$

#### Representação como Superposição
Uma propriedade fundamental da solução de norma mínima é que ela pode ser representada como uma **superposição dos pontos de dados de entrada** $(x_i)_{i=1}^m$ [^2].  Isto significa que a solução ótima $w^*$ reside no espaço gerado por esses vetores, ou seja, $w^* \in span\{x_1, ..., x_m\}$.

Formalmente, se $H = span\{x_1, ..., x_m\} \subseteq \mathbb{R}^d$, então a solução de norma mínima é o elemento único:
$$
w^* = \underset{\tilde{w} \in H}{\operatorname{argmin}} f(\tilde{w}) \in H.
$$

Esta representação tem implicações importantes em termos de **regularização**, pois a busca pela solução de norma mínima impõe uma restrição sobre o espaço de soluções, evitando soluções com normas excessivamente grandes [^3].

#### Regularização e Convergência do Gradiente Descendente
A busca pela solução de norma mínima atua como uma forma de **regularização**, favorecendo soluções mais "simples" em termos de norma [^3].  Interessantemente, o algoritmo de gradiente descendente converge para a solução de norma mínima do objetivo quadrático (11.1.2), desde que o ponto inicial $w_0$ esteja dentro do espaço $H = span\{x_1, ..., x_m\}$ [^3]. Isto significa que o gradiente descendente não encontra um minimizador "arbitrário", mas sim aquele que implicitamente regulariza o problema.

**Teorema 11.3 [^3]:** *Seja $A \in \mathbb{R}^{m \times d}$ como em (11.1.1), seja $w_0 = \tilde{w}_0 + \hat{w}_0$ onde $\tilde{w}_0 \in H$ e $\hat{w}_0 \in H^\perp$. Fixe $h \in (0, 1/(2\sigma_{max}(A)^2))$ e defina*
$$
w_{k+1} := w_k - h\nabla f(w_k) \quad \text{para todo } k \in \mathbb{N}
$$
*com $f$ em (11.1.2). Então*
$$
\lim_{k \to \infty} w_k = w^* + \hat{w}_0.
$$

O teorema acima mostra que, se o ponto inicial tiver uma componente fora de $H$, essa componente permanecerá inalterada ao longo das iterações, e o gradiente descendente convergirá para a solução de norma mínima dentro de $H$, somada a essa componente inicial.

### Conclusão
A solução de norma mínima em regressão linear por mínimos quadrados oferece uma abordagem elegante para garantir a unicidade da solução quando o sistema linear é subdeterminado. Sua representação como uma superposição dos pontos de dados de entrada e sua propriedade de ser implicitamente encontrada pelo gradiente descendente a tornam uma ferramenta valiosa em diversas aplicações. A condição de minimizar a norma atua como um regularizador, evitando soluções complexas e promovendo a generalização do modelo.

### Referências
[^1]: Capítulo 11, Seção 11.1, p. 139.
[^2]: Capítulo 11, Seção 11.1, p. 140.
[^3]: Capítulo 11, Seção 11.1, p. 141.
<!-- END -->