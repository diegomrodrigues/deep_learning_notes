## Soluções de Mínima Norma e Regressão Linear por Mínimos Quadrados

### Introdução
Este capítulo explora a solução para o problema de **regressão linear por mínimos quadrados**, com foco especial nas situações onde a matriz não é invertível. Em particular, analisaremos como a busca por uma solução de mínima norma garante a unicidade da solução quando a matriz do sistema não possui posto completo [^1]. Este tópico complementa o entendimento de métodos de regressão linear, que são fundamentais em diversas áreas, incluindo o treinamento de redes neurais [^1].

### Conceitos Fundamentais

Na regressão linear por mínimos quadrados, o objetivo é encontrar um vetor de pesos $w \in \mathbb{R}^d$ que minimize a função objetivo de erro quadrático [^1]:

$$
f(w) = \sum_{i=1}^{m} (\Phi(x_i, w) - y_i)^2
$$

onde $(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$ são os pares de dados de treinamento, $i \in \{1, ..., m\}$, e $\Phi(x, w)$ é uma função linear que define o modelo. No caso linear, $\Phi(x, w) = x^T w$ [^1].

A função objetivo pode ser expressa em forma matricial como [^1]:

$$
f(w) = ||Aw - y||^2
$$

onde $A \in \mathbb{R}^{m \times d}$ é a matriz de design, construída com os vetores de entrada $x_i$ [^1]:

$$
A = \begin{bmatrix} x_1^T \\ \vdots \\ x_m^T \end{bmatrix}
$$

e $y \in \mathbb{R}^m$ é o vetor de saídas [^1]:

$$
y = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix}
$$

Se a matriz $A$ for invertível, a solução que minimiza $f(w)$ é única e dada por [^2]:

$$
w^* = A^{-1}y
$$

No entanto, se $A$ não for invertível (ou seja, não possui posto completo), existem infinitas soluções que minimizam $f(w)$ [^2]. Isso ocorre quando `rank(A) < d`, implicando que `ker(A) ≠ {0}` [^2]. Para garantir a unicidade da solução, busca-se a solução de **mínima norma**, também conhecida como solução de **mínimo 2-norma** [^2]:

$$
w^* := \underset{w \in \mathbb{R}^d | f(w) \leq f(v) \forall v \in \mathbb{R}^d}{\text{argmin}} ||w||
$$

A solução de mínima norma é única [^2]. Seja $H := \text{span}\{x_1, ..., x_m\} \subseteq \mathbb{R}^d$ o espaço gerado pelos vetores de entrada. A solução de mínima norma $w^*$ pertence a $H$ e é dada por [^2]:

$$
w^* = \underset{\tilde{w} \in H}{\text{argmin}} f(\tilde{w}) \in H
$$

**Proposição 11.2** [^2] garante a existência e unicidade da solução de mínima norma. A prova envolve a demonstração de que o conjunto de minimizadores $M = \{w \in \mathbb{R}^d | Aw = y^*\}$ é fechado e convexo, garantindo a existência de um único elemento $w^* = \text{argmin}_{w \in M} ||w||$ [^2]. Além disso, demonstra-se que $w^* \in H$ por contradição.

**Teorema 11.3** [^3] estabelece que o método do gradiente descendente converge para a solução de mínima norma, desde que o ponto inicial $w_0$ pertença ao espaço $H$. Especificamente, dado um passo de aprendizado $h \in (0, 1/(2\sigma_{\text{max}}(A)^2))$ e a iteração $w_{k+1} := w_k - h\nabla f(w_k)$, então [^3]:

$$
\lim_{k \to \infty} w_k = \tilde{w} + \hat{w}_0
$$

onde $\tilde{w}$ é a solução de mínima norma e $w_0 = \tilde{w}_0 + \hat{w}_0$ com $\tilde{w}_0 \in H$ e $\hat{w}_0 \in H^\perp$ [^3].

### Conclusão

A regressão linear por mínimos quadrados é uma técnica fundamental em aprendizado de máquina. A análise da solução de mínima norma é crucial quando a matriz de design não é invertível, garantindo a unicidade da solução. O método do gradiente descendente, sob certas condições, converge para essa solução de mínima norma, demonstrando sua importância prática [^3]. Este entendimento é essencial para a aplicação e adaptação de modelos lineares em cenários complexos, incluindo a inicialização e treinamento de redes neurais [^1].

### Referências
[^1]: Página 139 do texto fornecido.
[^2]: Página 140 do texto fornecido.
[^3]: Página 141 do texto fornecido.
<!-- END -->