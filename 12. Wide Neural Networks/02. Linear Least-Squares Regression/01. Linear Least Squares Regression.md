## Linear Least-Squares Regression: Convexity and Minimization

### Introdução
Este capítulo explora a regressão linear por mínimos quadrados, um dos algoritmos mais simples em machine learning [^1]. Dado um conjunto de pares de dados, o objetivo é ajustar uma função linear aos dados minimizando a função de perda quadrática [^1]. Como a função objetivo é convexa, algoritmos de descida de gradiente podem ser aplicados para encontrar um minimizador global [^1]. A solução pode ser expressa como a minimização de $||Aw – y||^2$, onde $A$ é uma matriz construída a partir dos dados de entrada e $y$ é o vetor de valores alvo [^1].

### Conceitos Fundamentais

A regressão linear por mínimos quadrados busca ajustar uma função linear $\Phi(x, w) = x^T w$ aos dados minimizando a função objetivo de perda quadrática [^1]. Formalmente, dado um conjunto de dados $\\{(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}\\}_{i=1}^m$ [^1], o objetivo é encontrar o vetor de parâmetros $w \in \mathbb{R}^d$ que minimize a seguinte função [^1]:

$$f(w) = \sum_{i=1}^m (\Phi(x_i, w) - y_i)^2 = \sum_{i=1}^m (x_i^T w - y_i)^2$$ [^1]

Esta função representa a soma dos quadrados das diferenças entre os valores previstos pelo modelo linear e os valores reais [^1]. Em notação matricial, podemos definir a matriz $A \in \mathbb{R}^{m \times d}$ e o vetor $y \in \mathbb{R}^m$ como [^1]:

$$A = \begin{bmatrix} x_1^T \\\\ \vdots \\\\ x_m^T \end{bmatrix}, \quad y = \begin{bmatrix} y_1 \\\\ \vdots \\\\ y_m \end{bmatrix}$$ [^1]

Com essas definições, a função objetivo pode ser reescrita como [^1]:

$$f(w) = ||Aw - y||^2$$ [^1]

Como a função $f(w)$ é quadrática e, portanto, convexa [^1], podemos encontrar o minimizador global derivando a função em relação a $w$ e igualando a zero. O gradiente de $f(w)$ é dado por [^1]:

$$\nabla f(w) = 2A^T(Aw - y)$$ [^3]

Igualando o gradiente a zero, obtemos a equação normal:

$$A^T A w = A^T y$$

Se a matriz $A^T A$ for inversível, a solução para $w$ é dada por:

$$w^* = (A^T A)^{-1} A^T y$$

Esta solução minimiza a função de perda quadrática [^1]. No entanto, se $A^T A$ não for inversível, o que ocorre quando as colunas de $A$ são linearmente dependentes ou quando o número de features $d$ é maior que o número de amostras $m$, existem infinitas soluções [^1]. Nesse caso, buscamos a solução de norma mínima, ou seja, a solução que minimiza $||w||$ entre todas as soluções possíveis [^1]. Formalmente, procuramos [^1]:

$$w^* = \arg\min_{w \in \mathbb{R}^d \\, | \\, f(w) \le f(v) \\, \forall v \in \mathbb{R}^d} ||w||$$ [^1]

A solução de norma mínima pode ser obtida usando a pseudo-inversa de Moore-Penrose de $A$, denotada por $A^\dagger$ [^1]:

$$w^* = A^\dagger y$$

A pseudo-inversa generaliza a inversa para matrizes não quadradas e singulares [^1]. Uma propriedade importante da solução de norma mínima é que ela pertence ao espaço gerado pelas linhas de $A$, ou seja, $w^* \in \mathcal{H} = \text{span}\\{x_1, \dots, x_m\\} \subseteq \mathbb{R}^d$ [^1]. Isso significa que $w^*$ pode ser expresso como uma combinação linear dos vetores de entrada $x_i$ [^1].

**Proposição 11.2:** Seja $A \in \mathbb{R}^{m \times d}$ e $y \in \mathbb{R}^m$ como em (11.1.1) [^1]. Existe uma única solução de norma mínima de (11.1.2) [^1]. Denotando $\mathcal{H} := \text{span}\\{x_1, \dots, x_m\\} \subseteq \mathbb{R}^d$, ela é o único elemento [^1]:

$$w^* = \arg\min_{\tilde{w} \in \mathcal{H}} f(\tilde{w}) \in \mathcal{H}$$ [^1]

*Prova:* Seja $\mathcal{C} \subseteq \mathbb{R}^m$ o espaço gerado pelas colunas de $A$ [^1]. Então $\mathcal{C}$ é fechado e convexo, e portanto $y^* = \arg\min_{\tilde{y} \in \mathcal{C}} ||y - \tilde{y}||$ existe e é único (esta é uma propriedade fundamental dos espaços de Hilbert, veja o Teorema B.14) [^1]. Em particular, o conjunto $M = \\{w \in \mathbb{R}^d \\, | \\, Aw = y^*\\} \subseteq \mathbb{R}^d$ de minimizadores de $f$ não é vazio [^1]. Claramente $M$ também é fechado e convexo [^1]. Pelo mesmo argumento anterior, $w^* = \arg\min_{w \in M} ||w||$ existe e é único [^1].

Resta mostrar (11.1.4) [^1]. Denote por $w^*$ a solução de norma mínima e decomponha $w^* = \tilde{w} + \hat{w}$ com $\tilde{w} \in \mathcal{H}$ e $\hat{w} \in \mathcal{H}^\perp$ [^1]. Temos $Aw^* = A\tilde{w}$ e $||w^*||^2 = ||\tilde{w}||^2 + ||\hat{w}||^2$ [^1]. Como $w^*$ é a solução de norma mínima, deve valer $\hat{w} = 0$. Assim, $w^* \in \mathcal{H}$ [^1]. Finalmente, suponha que exista um minimizador $v$ de $f$ em $\mathcal{H}$ diferente de $w^*$ [^1]. Então $0 \neq w^* - v \in \mathcal{H}$, e como $\mathcal{H}$ é gerado pelas linhas de $A$, temos $A(w^* - v) \neq 0$ [^1]. Assim, $y^* = Aw^* \neq Av$, o que contradiz o fato de que $v$ minimiza $f$ [^1]. $\blacksquare$

A condição de minimizar a norma 2 é uma forma de regularização [^3]. É interessante notar que o método de descida do gradiente converge para a solução de norma mínima para o objetivo quadrático (11.1.2), desde que $w_0$ seja inicializado dentro de $\mathcal{H} = \text{span}\\{x_1, \dots, x_m\\}$ (e.g., $w_0 = 0$) [^3]. Portanto, não encontra um minimizador "arbitrário", mas regulariza implicitamente o problema nesse sentido [^3]. Em seguida, $\sigma_{\text{max}}(A)$ denota o valor singular máximo de $A$ [^3].

**Teorema 11.3:** Seja $A \in \mathbb{R}^{m \times d}$ como em (11.1.1), seja $w_0 = \tilde{w}_0 + \hat{w}_0$ onde $\tilde{w}_0 \in \mathcal{H}$ e $\hat{w}_0 \in \mathcal{H}^\perp$ [^3]. Fixe $h \in (0, 1/(2\sigma_{\text{max}}(A)^2))$ e defina [^3]:

$$w_{k+1} := w_k - h\nabla f(w_k) \quad \text{para todo } k \in \mathbb{N}$$ [^3]

com $f$ em (11.1.2) [^3]. Então [^3]:

$$\lim_{k \to \infty} w_k = w^* + \hat{w}_0$$ [^3]

*Esboço da Prova:* Esboçamos o argumento no caso $w_0 \in \mathcal{H}$ e deixamos a prova completa para o leitor, veja o Exercício 11.32 [^3]. Note que $\mathcal{H}$ é o espaço gerado pelas linhas de $A$ (ou as colunas de $A^T$) [^3]. O gradiente da função objetivo é igual a [^3]:

$$\nabla f(w) = 2A^T(Aw - y)$$ [^3]

Portanto, se $w_0 \in \mathcal{H}$, então as iterações do método de descida do gradiente nunca saem do subespaço $\mathcal{H}$ [^3]. Pelo Exercício 10.34 e Teorema 10.11, para um tamanho de passo suficientemente pequeno, vale $f(w_k) \to 0$ [^3]. Pela Proposição 11.2, existe apenas um minimizador em $\mathcal{H}$, correspondendo à solução de norma mínima [^3]. Assim, $w_k$ converge para a solução de norma mínima [^3].

### Conclusão

Este capítulo apresentou a regressão linear por mínimos quadrados, destacando a convexidade da função objetivo e a convergência dos algoritmos de descida de gradiente [^1]. Discutimos a importância da solução de norma mínima quando a matriz $A^T A$ não é inversível e apresentamos a proposição que garante que a solução de norma mínima pertence ao espaço gerado pelos vetores de entrada [^1]. Por fim, apresentamos o teorema que garante a convergência do método de descida do gradiente para a solução de norma mínima [^3].

### Referências
[^1]: Página 139, 140 do documento original.
[^2]: Página 140 do documento original.
[^3]: Página 141 do documento original.

<!-- END -->