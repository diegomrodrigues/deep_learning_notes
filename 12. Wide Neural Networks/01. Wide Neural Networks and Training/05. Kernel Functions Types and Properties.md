## Kernel Functions in Wide Neural Networks

### Introdução
Como vimos anteriormente, a análise de redes neurais largas (wide neural networks) frequentemente se beneficia da teoria de kernels [^1]. Este capítulo explora as funções de kernel comuns e suas propriedades, que são cruciais para entender o comportamento e as capacidades dessas redes. A escolha do kernel impacta diretamente a capacidade da rede de aprender diferentes tipos de relações nos dados [^1].

### Conceitos Fundamentais
As funções de kernel são usadas para definir o produto interno no *feature space* $\mathcal{H}$ sem explicitamente mapear os dados para esse espaço [^3]. A função de kernel $K(x, x')$ mede a similaridade entre dois pontos $x$ e $x'$ no espaço de entrada. A escolha do kernel influencia a expressividade do modelo, permitindo capturar relações complexas e não lineares [^3].

**Definição de Kernel** [^6]: Uma função simétrica $K : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ é chamada de kernel se, para quaisquer $x_1, ..., x_m \in \mathbb{R}^d$, a matriz de kernel $G = (K(x_i, x_j))_{i,j=1}^m \in \mathbb{R}^{m \times m}$ é simétrica e positiva semi-definida.

**Representer Theorem:** O *Representer Theorem* [^5] é fundamental para métodos de kernel. Ele afirma que a solução para muitos problemas de aprendizado de máquina pode ser escrita como uma combinação linear das funções de kernel avaliadas nos pontos de treinamento.

**Kernel Trick:** O *kernel trick* [^7] permite utilizar algoritmos lineares em espaços de alta dimensão (ou até infinitos) sem calcular explicitamente as coordenadas nesses espaços, apenas especificando a função de kernel $K(x, x')$.

Funções de kernel comuns incluem:
*   **Polynomial Kernel** [^7]:
    $$K(x, x') = (x^T x' + c)^r$$
    onde $c \geq 0$ e $r \in \mathbb{N}$. Os polynomial kernels são úteis para modelar relações polinomiais entre os dados.
*   **Radial Basis Function (RBF) Kernel** [^7]:
    $$K(x, x') = \exp(-c||x - x'||^2)$$
    onde $c > 0$. Os RBF kernels são capazes de modelar relações complexas e não lineares e são frequentemente usados quando não há conhecimento prévio sobre a estrutura dos dados.
*   **Laplace Kernel** [^7]:
    $$K(x, x') = \exp(-c||x - x'||)$$
    onde $c > 0$. Os Laplace kernels são similares aos RBF kernels, mas são menos sensíveis a outliers.

**Exemplo: Gaussian Process** [^21]: Redes neurais no limite de largura infinita se relacionam com *Gaussian Processes*. Um *Gaussian Process* é definido por uma função média $m: \mathbb{R}^d \rightarrow \mathbb{R}$ e uma função de covariância $c: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$.

**Tangent Kernel:** O *Tangent Kernel* [^8] é uma aproximação linear da rede neural em torno da inicialização. Ele é definido como:

$\qquad K_n(x, x') = (\nabla_w \Phi(x, w_0), \nabla_w \Phi(x', w_0))$

Onde $\Phi$ é a rede neural e $w_0$ é a inicialização dos pesos.

### Conclusão

A escolha da função de kernel é uma etapa crucial na aplicação de métodos de kernel para analisar e treinar redes neurais largas [^1]. Cada tipo de kernel possui propriedades únicas que o tornam adequado para diferentes tipos de dados e relações [^1]. Compreender essas propriedades é essencial para construir modelos eficazes e interpretar seus resultados [^1]. Além disso, a conexão com *Gaussian Processes* oferece uma perspectiva teórica valiosa sobre o comportamento dessas redes no limite de largura infinita [^21].

### Referências
[^1]: Texto fornecido.
[^2]: 11.0.1a
[^3]: 11.2 Kernel least-squares
[^4]: Proposition 11.2. Let A ∈ Rm×d and y ∈ Rm be as in (11.1.1). There exists a unique minimum 2-norm solution of (11.1.2). Denoting H := span{x1,...,xm} ⊆ Rd, it is the unique element
[^5]: Theorem 11.7 (Representer theorem). There is a unique minimum H-norm solution w∗ ∈ H of (11.2.2).
[^6]: Definition 11.8. A symmetric function K : Rd × Rd → R is called a kernel if for any x1,...,xm ∈ Rd the kernel matrix G = (K(xi, xj))mj=1 ∈ Rmxm is symmetric positive semidefinite.
[^7]: Example 11.10. Common examples of kernels include the polynomial kernel
[^8]: 11.3 Tangent kernel
[^21]: 11.5.5 Connection to Gaussian processes
<!-- END -->