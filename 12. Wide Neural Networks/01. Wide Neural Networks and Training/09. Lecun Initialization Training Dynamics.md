## 11.5 Training Dynamics for LeCun Initialization

### Introdução
Este capítulo explora a dinâmica de treinamento de redes neurais largas, focando especificamente nas implicações do Teorema 11.13 [^10] para redes neurais amplas, especialmente redes rasas com uma camada oculta. Consideraremos a arquitetura e a inicialização da rede. Como vimos anteriormente, o Teorema 11.13 estabelece condições sob as quais o método do gradiente descendente converge para um minimizador global sob certas condições de regularidade e proximidade à linearização. Agora, aplicaremos este resultado para entender o comportamento de redes neurais largas com inicialização LeCun.

### Conceitos Fundamentais
#### 11.5.1 Arquitetura
Considere uma rede neural $\Phi: \mathbb{R}^d \rightarrow \mathbb{R}$ de profundidade um e largura $n \in \mathbb{N}$ do tipo [^12]:
$$\
\Phi(x, w) = v^T\sigma(Ux + b) + c \quad (11.5.1)
$$\
onde $x \in \mathbb{R}^d$ é a entrada, $U \in \mathbb{R}^{n \times d}$, $v \in \mathbb{R}^n$, $b \in \mathbb{R}^n$ e $c \in \mathbb{R}$ são os parâmetros, que coletamos no vetor $w = (U, b, v, c) \in \mathbb{R}^{n(d+2)+1}$ (com $U$ adequadamente remodelado). Para referência futura, observamos que [^12]:
$$\
\begin{aligned}
\nabla_U \Phi(x, w) &= (v \odot \sigma'(Ux + b))x^T \in \mathbb{R}^{n \times d} \\\\
\nabla_b \Phi(x, w) &= v \odot \sigma'(Ux + b) \in \mathbb{R}^n \\\\
\nabla_v \Phi(x, w) &= \sigma(Ux + b) \in \mathbb{R}^n \\\\
\nabla_c \Phi(x, w) &= 1 \in \mathbb{R}
\end{aligned} \quad (11.5.2)
$$\
onde $\odot$ denota o produto de Hadamard. Também escrevemos $\nabla_w \Phi(x, w) \in \mathbb{R}^{n(d+2)+1}$ para denotar o gradiente completo em relação a todos os parâmetros.

Na prática, é comum inicializar os pesos aleatoriamente, e nesta seção consideramos a chamada inicialização de LeCun. A seguinte condição sobre a distribuição usada para esta inicialização será assumida ao longo do restante da Seção 11.5 [^12]:

**Assumption 11.14.** A distribuição $\mathcal{D}$ em $\mathbb{R}$ tem expectativa zero, variância um e momentos finitos até a ordem oito.

Para indicar explicitamente a expectativa e a variância na notação, também escrevemos $\mathcal{D}(0, 1)$ em vez de $\mathcal{D}$, e para $\mu \in \mathbb{R}$ e $s > 0$ usamos $\mathcal{D}(\mu, s^2)$ para denotar a medida escalonada e deslocada correspondente com expectativa $\mu$ e variância $s^2$; assim, se $X \sim \mathcal{D}(0, 1)$ então $\mu + sX \sim \mathcal{D}(\mu, s^2)$. A inicialização de LeCun [127] define a variância dos pesos em cada camada para ser recíproca da dimensão de entrada da camada, normalizando assim a variância de saída em todos os nós da rede. Os parâmetros iniciais [^12]:
$$\
w_0 = (U_0, b_0, v_0, c_0) \quad (11.5.3)
$$\
são, portanto, inicializados aleatoriamente com componentes [^12]:
$$\
U_{0;ij} \overset{iid}{\sim} \mathcal{D}(0, \frac{1}{d}), \quad v_{0;i} \overset{iid}{\sim} \mathcal{D}(0, 1), \quad b_{0;i}, c_0 = 0,
$$\
independentemente para todo $i = 1, ..., n, j = 1, ..., d$. Para um $s > 0$ fixo, pode-se escolher variâncias $s^2/d$ e $s^2/n$ em (11.5.3), o que exigiria apenas pequenas modificações no restante desta seção. Os bias são definidos como zero por simplicidade, com inicialização não nula discutida nos exercícios. Todas as expectativas e probabilidades na Seção 11.5 são entendidas em relação a esta inicialização aleatória.

**Example 11.15.** Exemplos típicos para $\mathcal{D}(0, 1)$ são a distribuição normal padrão em $\mathbb{R}$ ou a distribuição uniforme em $[-\sqrt{3}, \sqrt{3}]$.

#### 11.5.2 Kernel Tangente Neural
Começamos nossa análise investigando o kernel tangente empírico [^13]:
$$\
K_n(x, z) = \langle \nabla_w \Phi(x, w_0), \nabla_w \Phi(z, w_0) \rangle
$$\
da rede rasa (11.5.1). Escalonado adequadamente, ele converge no limite de largura infinita $n \rightarrow \infty$ para um kernel específico conhecido como o kernel tangente neural (NTK). Sua fórmula precisa depende da arquitetura e inicialização. Para a inicialização de LeCun (11.5.3), denotamos por $K^{LC}$.

**Theorem 11.16.** Seja $R < \infty$ tal que $|\sigma(x)| \le R \cdot (1 + |x|)$ e $|\sigma'(x)| \le R \cdot (1 + |x|)$ para todo $x \in \mathbb{R}$. Para quaisquer $x, z \in \mathbb{R}^d$ e $u_i \overset{iid}{\sim} \mathcal{D}(0, 1/d), i = 1, ..., d$, então vale [^13]:
$$\
\lim_{n \rightarrow \infty} \frac{1}{n} K_n(x, z) = \mathbb{E}[\sigma'(u^T x)\sigma'(u^T z)] =: K^{LC}(x, z)
$$\
quase certamente. Além disso, para todo $\delta, \epsilon > 0$ existe $n_0(\delta, \epsilon, R) \in \mathbb{N}$ tal que para todo $n \ge n_0$ e todo $x, z \in \mathbb{R}^d$ com $||x||, ||z|| \le R$ [^13]:
$$\
\mathbb{P}[|\frac{1}{n} K_n(x, z) - K^{LC}(x, z)| < \epsilon] \ge 1 - \delta.
$$\
*Prova:* Denotamos $x^{(1)} = U_0x + b_0 \in \mathbb{R}^n$ e $z^{(1)} = U_0z + b_0 \in \mathbb{R}^n$. Devido à inicialização (11.5.3) e nossas suposições sobre $\mathcal{D}(0, 1)$, os componentes [^13]:
$$\
x_i^{(1)} = \sum_{j=1}^d U_{0;ij} x_j \sim u^T x, \quad i = 1, ..., n
$$\
são i.i.d. com o momento $p$-ésimo finito (independente de $n$) para todo $1 < p < 8$. Devido ao limite de crescimento linear em $\sigma$ e $\sigma'$, o mesmo vale para o $(\sigma'(x_i^{(1)}))_{i=1}^n$ e o $(\sigma(x_i^{(1)}))_{i=1}^n$. Da mesma forma, o $(\sigma'(z_i^{(1)}))_{i=1}^n$ e o $(\sigma(z_i^{(1)}))_{i=1}^n$ são coleções de variáveis aleatórias i.i.d. com o momento $p$-ésimo finito para todo $1 < p < 8$. Denotamos $v_i = \sqrt{n} v_{0;i}$ tal que $v_i \overset{iid}{\sim} \mathcal{D}(0, 1)$. Por (11.5.2) [^13]:
$$\
\frac{1}{n} K_n(x, z) = \frac{1}{n} \sum_{i=1}^n v_i^2 \sigma'(x_i^{(1)}) \sigma'(z_i^{(1)}) + 1.
$$\
Como é uma média sobre variáveis aleatórias i.i.d. com variância finita, a lei dos grandes números implica a convergência quase certa desta expressão para [^14]:
$$\
\mathbb{E}[v_i^2 \sigma'(x_i^{(1)}) \sigma'(z_i^{(1)})] = \mathbb{E}[v_i^2] \mathbb{E}[\sigma'(x_i^{(1)}) \sigma'(z_i^{(1)})] = \mathbb{E}[\sigma'(u^T x) \sigma'(u^T z)],
$$\
onde usamos que $v_i$ é independente de $\sigma'(x_i^{(1)})$ e $\sigma'(z_i^{(1)})$. Pelo mesmo argumento [^14]:
$$\
\frac{1}{n} \sum_{i=1}^n \sigma(x_i^{(1)}) \sigma(z_i^{(1)}) \rightarrow \mathbb{E}[\sigma(u^T x) \sigma(u^T z)]
$$\
quase certamente quando $n \rightarrow \infty$. Isso mostra a primeira afirmação. A existência de $n_0$ segue-se de forma semelhante por uma aplicação do Teorema A.22. $\blacksquare$

**Example 11.17 (K^{LC} for ReLU).** Seja $\sigma(x) = \max\\{0, x\\}$ e seja $\mathcal{D}(0, 1)$ a distribuição normal padrão. Para $x, z \in \mathbb{R}^d$, denotamos por [^14]:
$$\
\theta = \arccos \left(\frac{x^T z}{||x|| ||z||}\right)
$$\
o ângulo entre esses vetores. Então, de acordo com [36, Appendix A], vale com $u_i \overset{iid}{\sim} \mathcal{D}(0, 1), i = 1, ..., d$ [^14]:
$$\
K^{LC}(x, z) = \mathbb{E}[\sigma'(u^T x) \sigma'(u^T z)] = \frac{||x|| ||z||}{2\pi} (\sin(\theta) + (\pi - \theta) \cos(\theta)).
$$\

#### 11.5.3 Gradiente Descendente
Agora, procedemos de forma semelhante a [129, App. G], para mostrar que o Teorema 11.13 é aplicável à rede neural ampla (11.5.1) com alta probabilidade sob inicialização aleatória (11.5.3). Isso implicará que o gradiente descendente pode encontrar minimizadores globais ao treinar redes neurais amplas. Trabalhamos sob as seguintes suposições sobre a função de ativação e os dados de treinamento [^14]:

**Assumption 11.18.** Existem $R < \infty$ e $0 < \Lambda_{\min}^{LC} < \Lambda_{\max}^{LC} < \infty$ tais que
(a) para a função de ativação $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ vale $|\sigma(0)|, \text{Lip}(\sigma), \text{Lip}(\sigma') \le R$,
(b) $||x_i||, |y_i| \le R$ para todos os dados de treinamento $(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}, i = 1, ..., m$,
(c) a matriz do kernel do kernel tangente neural
$$\
(K^{LC}(x_i, x_j))_{i,j=1}^m \in \mathbb{R}^{m \times m}
$$\
é regular e seus autovalores pertencem a $[\Lambda_{\min}^{LC}, \Lambda_{\max}^{LC}]$.

Começamos mostrando a Suposição 11.12 (a) para a presente configuração. Mais precisamente, fornecemos limites para os autovalores do kernel tangente empírico.

#### 11.5.4 Proximidade ao modelo linearizado
A análise até agora foi baseada na linearização $\Phi^{lin}$ descrevendo o comportamento da rede completa bem em uma vizinhança dos parâmetros iniciais $w_0$. Além disso, o Teorema 11.23 afirma que os parâmetros permanecem em uma vizinhança $O(n^{-1/2})$ de $w_0$ durante o treinamento. Isso sugere que o modelo completo treinado $\lim_{k \rightarrow \infty} \Phi(x, w_k)$ produz previsões semelhantes ao modelo linearizado treinado.

Para descrever este fenômeno, adotamos novamente as notações $\Phi^{lin} : \mathbb{R}^d \times \mathbb{R}^n \rightarrow \mathbb{R}$ e $f^{lin}$ de (11.3.1) e (11.3.3). Inicializando $w_0$ de acordo com (11.5.3) e definindo $p_0 = w_0$, o gradiente descendente computa as atualizações dos parâmetros

$$\
w_{k+1} = w_k - h \nabla_w f(w_k), \quad p_{k+1} = p_k - h \nabla_w f^{lin}(p_k)
$$\

### Conclusão
Este capítulo detalhou a dinâmica de treinamento de redes neurais largas com inicialização de LeCun, aplicando o Teorema 11.13 para mostrar a convergência do gradiente descendente para um minimizador global. A análise do kernel tangente neural e a proximidade ao modelo linearizado fornecem insights sobre o comportamento dessas redes. Os resultados indicam que, sob certas condições, o treinamento de redes neurais largas pode ser bem compreendido através de aproximações lineares.

### Referências
[^1]: Capítulo 11, Wide neural networks
[^2]: Seção 11.0, Wide neural networks
[^3]: Seção 11.1, Linear least-squares
[^4]: Seção 11.2, Kernel least-squares
[^5]: Seção 11.3, Tangent kernel
[^6]: Seção 11.4, Convergence to global minimizers
[^7]: Teorema 11.3, Let $A \in R^{m \times d}$ be as in (11.1.1), let $w_0 = \tilde{w}_0 + \hat{w}_0$ where $\tilde{w}_0 \in \mathcal{H}$ and $\hat{w}_0 \in \mathcal{H}^{\perp}$. Fix $h \in (0, 1/(2 \sigma_{\max}(A)^2))$ and set $w_{k+1} := w_k - h \nabla f(w_k)$ for all $k \in \mathbb{N}$ with $f$ in (11.1.2). Then $\lim_{k \rightarrow \infty} w_k = w^* + \hat{w}_0$.
[^8]: Exercício 10.34
[^9]: Teorema 10.11
[^10]: Teorema 11.13, Let Assumption 11.12 be satisfied and fix a positive learning rate $h < \frac{1}{\Lambda_{\min} + \Lambda_{\max}}$. Set for all $k \in \mathbb{N}$, $w_{k+1} = w_k - h \nabla f(w_k)$. It then holds for all $k \in \mathbb{N}$, $||w_k - w_0|| < \frac{2 \sqrt{m} U}{\Lambda_{\min}} \sqrt{f(w_0)}$ and $f(w_k) \le (1 - h \Lambda_{\min})^{2k} f(w_0)$.
[^11]: Seção 11.5, Training dynamics for LeCun initialization
[^12]: Seção 11.5.1, Architecture
[^13]: Seção 11.5.2, Neural tangent kernel
[^14]: Seção 11.5.3, Gradient descent
[^15]: Seção 11.5.4, Proximity to linearized model
[^16]: (11.5.1), $\Phi(x, w) = v^T\sigma(Ux + b) + c$
[^17]: (11.5.2), $\nabla_U \Phi(x, w) = (v \odot \sigma'(Ux + b))x^T$, $\nabla_b \Phi(x, w) = v \odot \sigma'(Ux + b)$, $\nabla_v \Phi(x, w) = \sigma(Ux + b)$, $\nabla_c \Phi(x, w) = 1$
[^18]: (11.5.3), $w_0 = (U_0, b_0, v_0, c_0)$
[^19]: (11.5.3), $U_{0;ij} \overset{iid}{\sim} \mathcal{D}(0, \frac{1}{d})$, $v_{0;i} \overset{iid}{\sim} \mathcal{D}(0, 1)$, $b_{0;i}, c_0 = 0$
[^20]: Teorema 11.16, Let $R < \infty$ such that $|\sigma(x)| \le R \cdot (1 + |x|)$ and $|\sigma'(x)| \le R \cdot (1 + |x|)$ for all $x \in \mathbb{R}$. For any $x, z \in \mathbb{R}^d$ and $u_i \overset{iid}{\sim} \mathcal{D}(0, 1/d), i = 1, ..., d$, then $\lim_{n \rightarrow \infty} \frac{1}{n} K_n(x, z) = \mathbb{E}[\sigma'(u^T x)\sigma'(u^T z)] =: K^{LC}(x, z)$ almost surely. Moreover, for every $\delta, \epsilon > 0$ there exists $n_0(\delta, \epsilon, R) \in \mathbb{N}$ such that for all $n \ge n_0$ and all $x, z \in \mathbb{R}^d$ with $||x||, ||z|| \le R$, $\mathbb{P}[|\frac{1}{n} K_n(x, z) - K^{LC}(x, z)| < \epsilon] \ge 1 - \delta$.
[^21]: Exemplo 11.17, $K^{LC}(x, z) = \mathbb{E}[\sigma'(u^T x) \sigma'(u^T z)] = \frac{||x|| ||z||}{2\pi} (\sin(\theta) + (\pi - \theta) \cos(\theta))$
[^22]: Assumption 11.18, There exist $R < \infty$ and $0 < \Lambda_{\min}^{LC} < \Lambda_{\max}^{LC} < \infty$ such that (a) for the activation function $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ holds $|\sigma(0)|, \text{Lip}(\sigma), \text{Lip}(\sigma') \le R$, (b) $||x_i||, |y_i| \le R$ for all training data $(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}, i = 1, ..., m$, (c) the kernel matrix of the neural tangent kernel $(K^{LC}(x_i, x_j))_{i,j=1}^m \in \mathbb{R}^{m \times m}$ is regular and its eigenvalues belong to $[\Lambda_{\min}^{LC}, \Lambda_{\max}^{LC}]$.
<!-- END -->