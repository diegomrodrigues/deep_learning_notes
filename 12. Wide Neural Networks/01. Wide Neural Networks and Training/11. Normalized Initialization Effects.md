## Inicialização Normalizada e Dinâmica de Treinamento
### Introdução
Este capítulo explora a influência da **inicialização normalizada** no treinamento de redes neurais largas. A inicialização normalizada envolve o dimensionamento dos pesos em cada camada para **normalizar a variância da saída** em todos os nós da rede, afetando a dinâmica de treinamento e a contribuição de diferentes termos no **Kernel Tangente Neural (NTK)** [^161]. Em continuidade com os tópicos anteriores sobre redes neurais largas, este capítulo visa detalhar como a inicialização normalizada impacta o comportamento e a convergência dessas redes.

### Conceitos Fundamentais
A **inicialização normalizada**, especificamente a inicialização de LeCun, visa normalizar a variância da saída dos nós na inicialização (dinâmica *forward*) [^150]. Uma forma de inicialização diferente, consistente com o artigo original do NTK [^161], será discutida aqui, afetando a dinâmica de treinamento da rede.

Considere o gradiente $\nabla_\omega \Phi(x, \omega_0)$ como em (11.5.2) com a inicialização de LeCun. Visto que os componentes de $v$ se comportam como $v_i \overset{iid}{\sim} \mathcal{D}(0, 1/n)$, é fácil verificar em termos da largura $n$ que [^161]:

$E[||\nabla_U \Phi(x, \omega_0)||] = E[||(v \odot \sigma'(Ux + b))x||] = O(1)$
$E[||\nabla_b \Phi(x, \omega_0)||] = E[||v \odot \sigma'(Ux + b)||] = O(1)$
$E[||\nabla_v \Phi(x, \omega_0)||] = E[||\sigma(Ux + b)||] = O(n)$
$E[||\nabla_c \Phi(x, \omega_0)||] = E[||1||] = O(1)$

Como resultado deste escalonamento diferente, o método de descida do gradiente com tamanho de passo $O(n^{-1})$, como no Teorema 11.23, treinará primariamente os pesos $v$ na camada de saída, e mal moverá os parâmetros restantes $U, b$ e $c$ [^148]. Isto também é refletido na expressão para o kernel obtido $K^{LC}$ computado no Teorema 11.16, que corresponde à contribuição do termo $(\nabla_v \Phi, \nabla_v \Phi)$ [^151]. Para métodos de otimização como ADAM, que escalonam cada componente do gradiente individualmente, o mesmo não se mantém [^161].

A inicialização de LeCun tem como objetivo normalizar a variância da saída de todos os nós na inicialização (a dinâmica *forward*). Para também normalizar a variância dos gradientes (a dinâmica *backward*), esta seção discute brevemente uma arquitetura e inicialização diferentes, consistentes com a utilizada no artigo original sobre NTK [^161].

Considere uma rede neural de profundidade um:
$$\Phi(x, w) = \frac{1}{\sqrt{n}} v^T \sigma(\frac{1}{\sqrt{d}}Ux + b) + c,$$\
com input $x \in \mathbb{R}^d$ e parâmetros $U \in \mathbb{R}^{n \times d}$, $v \in \mathbb{R}^n$, $b \in \mathbb{R}^n$ e $c \in \mathbb{R}$. Inicializamos os pesos aleatoriamente de acordo com $\omega_0 = (U_0, b_0, v_0, c_0)$ com parâmetros:

$U_{0_{ij}} \overset{iid}{\sim} \mathcal{D}(0, 1), v_{0_i} \overset{iid}{\sim} \mathcal{D}(0, 1), b_{0_i}, c_0 = 0$.

Na inicialização, (11.6.1), (11.6.2) é equivalente a (11.5.1), (11.5.3). No entanto, para o gradiente, obtemos:

$\nabla_U \Phi(x, w) = n^{-1/2}(v \odot \sigma'(d^{-1/2}Ux + b))d^{-1/2}x \in \mathbb{R}^{n \times d}$
$\nabla_b \Phi(x, w) = n^{-1/2} v \odot \sigma'(d^{-1/2}Ux + b)) \in \mathbb{R}^n$
$\nabla_v \Phi(x, w) = n^{-1/2} \sigma(d^{-1/2}Ux + b) \in \mathbb{R}^n$
$\nabla_c \Phi(x, w) = 1 \in \mathbb{R}$

Ao contrário de (11.5.2), os três gradientes com entradas $O(n)$ são todos escalonados pelo fator $n^{-1/2}$. Isso leva a uma dinâmica de treinamento diferente [^162].

O Kernel Tangente Neural é computado novamente. Diferente da inicialização de LeCun, não é requerido um escalonamento de $1/n$ para obter a convergência de:

$K_n(x, z) = \langle \nabla_\omega \Phi(x, \omega_0), \nabla_\omega \Phi(z, \omega_0) \rangle$

Aqui, e no que se segue, consideramos a configuração (11.6.1)-(11.6.2) para $\Phi$ e $\omega_0$. Isto também é referido como a inicialização NTK, e denotamos o kernel por $K^{NTK}$. Devido às diferentes dinâmicas de treinamento, obtemos termos adicionais no NTK comparado com o Teorema 11.23 [^151].

Seja $R < \infty$ tal que $|\sigma(x)| \leq R \cdot (1 + |x|)$ e $|\sigma'(x)| \leq R \cdot (1 + |x|)$ para todo $x \in \mathbb{R}$, e seja $\mathcal{D}$ que satisfaz a Assunção 11.14. Para quaisquer $x, z \in \mathbb{R}^d$ e $u_i \overset{iid}{\sim} \mathcal{D}(0, 1/d), i = 1, ..., d$, então:

$$ \lim_{n \to \infty} K_n(x, z) = (1 + \frac{x^Tz}{d})E[\sigma'(u^Tx)\sigma'(u^Tz)] + E[\sigma(u^Tx)\sigma(u^Tz)] + 1 =: K^{NTK}(x, z)$$

quase certamente [^162].

### Conclusão
A escolha da inicialização, seja LeCun ou NTK, influencia significativamente a dinâmica de treinamento de redes neurais largas. A inicialização normalizada é crucial para garantir que o treinamento se comporte de maneira previsível e que a rede convirja para uma solução global. A análise detalhada dos gradientes e do kernel tangente neural sob diferentes esquemas de inicialização fornece *insights* valiosos sobre o comportamento dessas redes e orienta o desenvolvimento de técnicas de treinamento mais eficazes. A análise do kernel tangente neural com a inicialização normalizada revela que o treinamento se concentra principalmente nos pesos da camada de saída, enquanto a inicialização NTK garante que todos os gradientes sejam escalonados de forma semelhante, levando a uma dinâmica de treinamento diferente.

### Referências
[^150]: Capítulo 11, Seção 11.5.1
[^161]: Capítulo 11, Seção 11.6
[^148]: Capítulo 11, Teorema 11.23
[^151]: Capítulo 11, Teorema 11.16
[^162]: Capítulo 11, Teorema 11.30
<!-- END -->