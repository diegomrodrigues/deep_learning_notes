## 11.1 Linear Least-Squares Regression

### Introdução
Este capítulo explora a dinâmica do treinamento de redes neurais com grande largura, focando na situação onde temos pares de dados $(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$, com $i \in \{1, ..., m\}$, e desejamos treinar uma rede neural $\Phi(x, w)$ dependendo da entrada $x \in \mathbb{R}^d$ e dos parâmetros $w \in \mathbb{R}$, minimizando o objetivo de perda quadrada definido como [^1]
$$f(w) := \sum_{i=1}^m (\Phi(x_i, w) - y_i)^2.$$
Este capítulo inicia-se com uma revisão dos métodos de mínimos quadrados lineares, que descrevem modelos lineares (em $w$) [^1]. Como estabelecido no capítulo anterior, para mapeamentos lineares $w \rightarrow \Phi(x, w)$, a função objetivo (11.0.1b) é convexa e o método do gradiente descendente encontra um minimizador global [^1].

### Conceitos Fundamentais
Um dos algoritmos de aprendizado de máquina mais simples é a regressão linear por mínimos quadrados. Dados os dados (11.0.1a), a regressão linear tenta ajustar uma função linear $\Phi(x, w) := x^T w$ em termos de $w$ minimizando $f(w)$ em (11.0.1b) [^1]. Com
$$A = \begin{pmatrix} x_1^T \\\\ \vdots \\\\ x_m^T \end{pmatrix} \in \mathbb{R}^{m \times d} \quad \text{e} \quad y = \begin{pmatrix} y_1 \\\\ \vdots \\\\ y_m \end{pmatrix} \in \mathbb{R}^m,$$
temos
$$f(w) = ||Aw - y||^2.$$
Mais geralmente, o *ansatz* $\Phi(x, (w, b)) := w^T x + b$ corresponde a [^1]
$$\Phi(x, (w, b)) = \begin{pmatrix} 1, x^T \end{pmatrix} \begin{pmatrix} b \\\\ w \end{pmatrix}.$$
Portanto, permitir adicionalmente um *bias* pode ser tratado analogamente [^1].

O modelo $\Phi(x, w) = x^T w$ é linear tanto em $x$ quanto em $w$. Em particular, $w \rightarrow f(w)$ é uma função convexa pelo Exercício 10.34, e podemos aplicar os resultados de convergência do Capítulo 10 ao usar algoritmos baseados em gradiente [^1]. Se $A$ é invertível, então $f$ tem um minimizador único dado por $w^* = A^{-1}y$ [^1]. Se $rank(A) = d$, então $f$ é fortemente convexa pelo Exercício 10.34, e ainda existe um minimizador único [^1]. Se, no entanto, $rank(A) < d$, então $ker(A) \neq \{0\}$ e existem infinitos minimizadores de $f$ [^1]. Para garantir a unicidade, procuramos a solução de norma mínima (ou solução de norma 2 mínima) [^1]
$$w^* := \text{argmin}_{\{w \in \mathbb{R}^d | f(w) \leq f(v) \forall v \in \mathbb{R}^d\}} ||w||.$$
A seguinte proposição estabelece a unicidade de $w^*$ e demonstra que ele pode ser representado como uma superposição de $(x_i)_{i=1}^m$ [^1].

**Proposição 11.2.** Seja $A \in \mathbb{R}^{m \times d}$ e $y \in \mathbb{R}^m$ como em (11.1.1). Existe uma solução de norma 2 mínima única de (11.1.2). Denotando $H := \text{span}\{x_1, ..., x_m\} \subseteq \mathbb{R}^d$, é o elemento único
$$w^* = \text{argmin}_{\tilde{w} \in H} f(\tilde{w}) \in H.$$

*Prova*. Começamos com a existência e unicidade. Seja $C \subseteq \mathbb{R}^m$ o espaço gerado pelas colunas de $A$. Então $C$ é fechado e convexo, e portanto $y^* = \text{argmin}_{\tilde{y} \in C} ||y - \tilde{y}||$ existe e é único (esta é uma propriedade fundamental dos espaços de Hilbert, veja o Teorema B.14). Em particular, o conjunto $M = \{w \in \mathbb{R}^d | Aw = y^*\} \subseteq \mathbb{R}^d$ de minimizadores de $f$ não é vazio. Claramente, $M$ também é fechado e convexo. Pelo mesmo argumento de antes, $w^* = \text{argmin}_{w \in M} ||w^*||$ existe e é único.
Resta mostrar (11.1.4). Denote por $w^*$ a solução de norma mínima e decomponha $w^* = \tilde{w} + \hat{w}$ com $\tilde{w} \in H$ e $\hat{w} \in H^{\perp}$. Temos $Aw^* = A\tilde{w}$ e $||w^*||^2 = ||\tilde{w}||^2 + ||\hat{w}||^2$. Como $w^*$ é a solução de norma mínima, deve valer $\hat{w} = 0$. Assim, $w^* \in H$. Finalmente, assuma que existe um minimizador $v$ de $f$ em $H$ diferente de $w^*$. Então $0 \neq w^* - v \in H$, e como $H$ é gerado pelas linhas de $A$, temos $A(w^* - v) \neq 0$. Assim, $y^* = Aw^* \neq Av$, o que contradiz o fato de que $v$ minimiza $f$. $\blacksquare$

A condição de minimizar a norma 2 é uma forma de regularização [^1]. Interessantemente, o gradiente descendente converge para a solução de norma mínima para o objetivo quadrático (11.1.2), desde que $w_0$ seja inicializado dentro de $H = \text{span}\{x_1, ..., x_m\}$ (e.g. $w_0 = 0$) [^1]. Portanto, ele não encontra um minimizador "arbitrário", mas regulariza implicitamente o problema nesse sentido [^1].

### Conclusão

A regressão linear por mínimos quadrados fornece uma compreensão fundamental dos modelos lineares, onde o objetivo é ajustar uma função linear aos dados minimizando a soma das diferenças quadráticas entre os valores previstos e os valores reais [^1]. Isso serve como base para entender redes neurais mais complexas [^1]. A análise envolve regressão linear por mínimos quadrados, onde o objetivo é ajustar uma função linear aos dados minimizando a norma quadrada da diferença entre a transformação linear dos parâmetros e os valores alvo [^1]. Para mapeamentos lineares $w \rightarrow \Phi(x, w)$, a função objetivo é convexa, e o gradiente descendente pode encontrar um minimizador global, mas isso geralmente não é verdade para arquiteturas típicas de redes neurais não lineares [^1].

### Referências
[^1]: Capítulo 11, Wide neural networks.
<!-- END -->