## 11.3 Tangent Kernel

### Introdução
Este capítulo explora a dinâmica do treinamento de redes neurais de grande largura. Conforme estabelecido no capítulo anterior, o gradient descent encontra um minimizador global para mapeamentos lineares $w \rightarrow \Phi(x, w)$ [^1]. No entanto, para arquiteturas típicas de redes neurais, essa linearidade não se mantém [^1]. Pesquisas recentes indicam que o comportamento da rede neural tende a linearizar-se nos parâmetros à medida que a largura da rede aumenta [^1]. Isso permite transferir alguns dos resultados e técnicas do caso linear para o treinamento de redes neurais [^1].  Esta seção introduz o conceito de *Tangent Kernel*, que lineariza o modelo da rede neural em torno de um ponto de inicialização, fornecendo uma representação simplificada que captura o comportamento local da rede [^7].

### Conceitos Fundamentais
Para modelos gerais $\Phi(x, w)$ onde $w \rightarrow \Phi(x, w)$ não é linear, a função objetivo de erro quadrático geralmente não é convexa, e os métodos de primeira ordem podem não ser diretamente aplicáveis [^7]. Para simplificar, o modelo é linearizado em torno de um ponto de inicialização $w_0$ usando uma aproximação de Taylor de primeira ordem [^7]:
$$\Phi_{lin}(x, w) = \Phi(x, w_0) + \nabla_w\Phi(x, w_0)^T(w - w_0) \quad [^7].$$
O *Tangent Kernel* surge da aproximação de Taylor de primeira ordem (a tangente) do modelo original em torno da inicialização $w_0$ e depende da escolha de $w_0$ [^7]. Treinar o modelo linearizado com gradient descent produz o estimador de kernel least-squares com kernel $\Gamma_n$ mais um termo adicional dependendo de $w_0$ [^7].

O *Empirical Tangent Kernel* $K_n(x, x\')$ é definido como o produto interno dos gradientes do modelo em relação aos parâmetros $w$, avaliados nos parâmetros iniciais $w_0$, ou seja [^7]:
$$K_n(x, x\') = (\nabla_w\Phi(x, w_0), \nabla_w\Phi(x\', w_0)) \quad [^7].$$
Se o modelo completo estiver suficientemente próximo de sua linearização, então o gradient descent aplicado ao modelo completo encontrará um minimizador global [^7]. Isso se baseia em suposições sobre a matriz kernel do *Empirical Tangent Kernel*, limites nas derivadas e uma relação entre as constantes para garantir a proximidade da linearização [^7].

A minimização de (11.2.2) e a expressão do estimador de kernel least squares não requerem o conhecimento explícito do feature map $\phi$ ou da solução de norma mínima $w^* \in H$ [^6]. É suficiente escolher um kernel map $K: R^d \times R^d \rightarrow R$; isso é conhecido como o kernel trick [^6]. Dado um kernel $K$, também nos referimos a (11.2.10) como o estimador de kernel least squares sem especificar $H$ ou $\phi$ [^6].

**Exemplos comuns de kernels incluem:**
*   O *polynomial kernel*: $K(x,x\') = (x^Tx\' + c)^r$, onde $c \geq 0$, $r \in N$ [^7].
*   O *radial basis function (RBF) kernel*: $K(x, x\') = exp(-c||x - x\'||^2)$, onde $c > 0$ [^7].
*   O *Laplace kernel*: $K(x, x\') = exp(-c||x - x\'||)$, onde $c > 0$ [^7].

**Observação:** Se $\Omega \subseteq R^d$ é compacto e $K: \Omega \times \Omega \rightarrow R$ é um kernel contínuo, então o teorema de Mercer implica a existência de um espaço de Hilbert $H$ e um feature map $\phi : R^d \rightarrow H$ tal que $K(x,x\') = (\phi(x), \phi(x\'))_H$ para todo $x, x\' \in \Omega$, i.e., K é o kernel correspondente [^7].

### Conclusão
O *Tangent Kernel* oferece uma abordagem para simplificar a análise e otimização de redes neurais, linearizando o modelo em torno de um ponto de inicialização [^7]. A eficácia dessa abordagem depende da proximidade do modelo completo à sua linearização, o que requer certas condições sobre a matriz kernel do *Empirical Tangent Kernel* e os limites nas derivadas [^7]. A análise detalhada do *Tangent Kernel* fornece insights valiosos sobre o comportamento das redes neurais e pode levar a algoritmos de treinamento mais eficientes [^7].

### Referências
[^1]: Página 139, Capítulo 11 "Wide neural networks".
[^6]: Página 145, Capítulo 11 "Wide neural networks".
[^7]: Página 145, Capítulo 11 "Wide neural networks".
<!-- END -->