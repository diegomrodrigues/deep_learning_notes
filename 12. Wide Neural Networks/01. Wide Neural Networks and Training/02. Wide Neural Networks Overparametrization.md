## Overparametrization and Gradient Descent in Wide Neural Networks

### Introdução
Este capítulo explora a dinâmica do treinamento de redes neurais largas, focando especificamente no cenário de **overparametrization**, onde o número de parâmetros da rede excede significativamente o número de pontos de dados [^1]. Este fenômeno tem demonstrado levar a efeitos de regularização fortes e permitir que o *gradient descent* encontre minimizadores globais da função de perda [^1]. A análise se concentra no caso de **strong overparametrization**, no qual a largura da rede aumenta enquanto o número de pontos de dados $m$ permanece fixo [^1]. Este cenário é crucial para entender o comportamento de redes neurais muito grandes [^1].

Como vimos anteriormente, o gradient descent aplicado a funções convexas garante a convergência para um minimizador global [^1]. No entanto, para arquiteturas típicas de redes neurais, a função $w \rightarrow \Phi(x, w)$ não é linear, e essa garantia não se mantém [^1]. Contudo, pesquisas recentes têm demonstrado que o comportamento de redes neurais tende a se linearizar nos parâmetros à medida que a largura da rede aumenta [^1]. Isso permite transferir alguns dos resultados e técnicas do caso linear para o treinamento de redes neurais [^1].

Este capítulo detalha as implicações da strong overparametrization no treinamento de redes neurais, demonstrando como o gradient descent pode encontrar minimizadores globais da função de perda [^1].

### Conceitos Fundamentais

#### Linearização do Modelo
A chave para entender o comportamento de redes neurais largas sob overparametrization reside na linearização do modelo em torno de uma inicialização específica. Dado um modelo geral $\Phi(x, w)$ com entrada $x \in \mathbb{R}^d$ e parâmetros $w \in \mathbb{R}^n$, e uma inicialização $w_0 \in \mathbb{R}^n$, a linearização de primeira ordem é dada por [^7]:

$$\Phi^{lin}(x, w) := \Phi(x, w_0) + \nabla_w \Phi(x, w_0)^T (w - w_0)$$

Minimizar a square loss para o modelo linearizado corresponde a uma regressão linear de mínimos quadrados com o feature map $\varphi(x) = \nabla_w \Phi(x, w_0) \in \mathbb{R}^n$ [^7]. O kernel correspondente é [^7]:

$$K_n(x, x') = (\nabla_w \Phi(x, w_0), \nabla_w \Phi(x', w_0))$$

Este kernel é conhecido como *empirical tangent kernel*, pois surge da aproximação de Taylor de primeira ordem (a tangente) do modelo original em torno da inicialização $w_0$ [^7]. É importante notar que o kernel depende da escolha de $w_0$ [^7].

#### Convergência para Minimizadores Globais

Intuitivamente, se $w \rightarrow \Phi(x, w)$ não é linear, mas "próximo o suficiente" de sua linearização $\Phi^{lin}$, esperamos que a função objetivo seja próxima de uma função convexa, e o gradient descent ainda possa encontrar minimizadores globais [^8]. As condições para que isso ocorra são formalizadas na Assumption 11.12 [^8]:

*   **Regularidade do empirical tangent kernel:** A matriz do kernel $(K_n(x_i, x_j))_{i,j=1}^m \in \mathbb{R}^{m \times m}$ é regular e seus autovalores pertencem ao intervalo $[\lambda_{min}, \lambda_{max}]$, com $0 < \lambda_{min} < \lambda_{max} < \infty$ [^8].
*   **Limitação do gradiente:** $\| \nabla_w \Phi(x_i, w) \| \leq U$ para todo $w \in B_r(w_0)$ [^9].
*   **Suavidade Lipschitz:** $\| \nabla_w \Phi(x_i, w) - \nabla_w \Phi(x_i, v) \| \leq L \| w - v \|$ para todo $w, v \in B_r(w_0)$ [^9].
*   **Relação entre as constantes:** $L < \frac{\lambda_{min}}{12m^{3/2} U^2 \sqrt{f(w_0)}}$ e $r = \frac{2 \sqrt{f(w_0)}}{\sqrt{m} U}$ [^9].

Sob estas condições, o Theorem 11.13 garante que o gradient descent com uma taxa de aprendizado apropriada $h < \frac{1}{\lambda_{min} + \lambda_{max}}$ converge para um minimizador global [^10].

#### Inicialização de LeCun

A inicialização de LeCun é uma técnica comum para inicializar os pesos de redes neurais [^12]. Ela define a variância dos pesos em cada camada como o inverso da dimensão de entrada da camada, normalizando assim a variância da saída em todos os nós da rede [^12]. Formalmente, os parâmetros iniciais são aleatoriamente inicializados com componentes [^12]:

$$\
\begin{aligned}
&U_{0;ij} \stackrel{iid}{\sim} \mathcal{D}(0, 1/d), \quad V_{0;i} \stackrel{iid}{\sim} \mathcal{D}(0, 1/n), \quad b_{0;i}, c_0 = 0
\end{aligned}
$$

onde $\mathcal{D}(0,1)$ representa uma distribuição com média zero e variância um [^12].

#### Neural Tangent Kernel (NTK)

O Neural Tangent Kernel (NTK) surge como o limite do empirical tangent kernel quando a largura da rede tende ao infinito [^13]. Sua fórmula precisa depende da arquitetura e da inicialização da rede [^13]. Para a inicialização de LeCun, o Theorem 11.16 fornece a seguinte expressão para o NTK [^13]:

$$\
\lim_{n \rightarrow \infty} \frac{1}{n} K_n(x, z) = \mathbb{E}[\sigma'(u^T x) \sigma'(u^T z)] =: K^{LC}(x, z)
$$

onde $u_i \stackrel{iid}{\sim} \mathcal{D}(0, 1/d)$ [^13].

### Conclusão

Este capítulo explorou o fenômeno da overparametrization em redes neurais largas, demonstrando como o gradient descent pode encontrar minimizadores globais sob certas condições [^1]. A linearização do modelo em torno da inicialização, a regularidade do empirical tangent kernel, e a técnica de inicialização de LeCun desempenham papéis cruciais neste processo [^1, 12, 13]. Além disso, o capítulo introduziu o Neural Tangent Kernel (NTK) como o limite do empirical tangent kernel quando a largura da rede tende ao infinito, fornecendo uma ferramenta valiosa para analisar o comportamento de redes neurais largas [^13]. Os resultados apresentados aqui fornecem insights importantes sobre o treinamento e a generalização de redes neurais profundas [^1].

### Referências
[^1]: Capítulo 11, p. 139
[^2]: Capítulo 11, p. 139
[^3]: Capítulo 11, p. 139
[^4]: Capítulo 11, p. 139
[^5]: Capítulo 11, p. 139
[^6]: Capítulo 11, p. 139
[^7]: Capítulo 11, p. 145
[^8]: Capítulo 11, p. 146
[^9]: Capítulo 11, p. 147
[^10]: Capítulo 11, p. 148
[^11]: Capítulo 11, p. 148
[^12]: Capítulo 11, p. 150
[^13]: Capítulo 11, p. 151
[^14]: Capítulo 11, p. 152
[^15]: Capítulo 11, p. 153
[^16]: Capítulo 11, p. 154
[^17]: Capítulo 11, p. 155
[^18]: Capítulo 11, p. 156
[^19]: Capítulo 11, p. 157
[^20]: Capítulo 11, p. 158
[^21]: Capítulo 11, p. 159
[^22]: Capítulo 11, p. 160
[^23]: Capítulo 11, p. 161
[^24]: Capítulo 11, p. 162
[^25]: Capítulo 11, p. 163

<!-- END -->