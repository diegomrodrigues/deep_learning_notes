## 11.5 Training Dynamics and LeCun Initialization in Wide Neural Networks

### Introdução
Este capítulo explora as dinâmicas de treinamento de redes neurais amplas, particularmente no contexto da inicialização de LeCun, e sua relação com o kernel tangente neural (NTK) [^1]. Como vimos anteriormente, redes neurais amplas tendem a exibir um comportamento linear à medida que sua largura aumenta, permitindo a transferência de resultados e técnicas do caso linear para o treinamento de redes neurais [^1]. Este capítulo se concentra em redes neurais rasas com apenas uma camada oculta, mas enfatiza que considerações semelhantes se aplicam a redes profundas [^1]. O objetivo é entender como a inicialização de LeCun afeta a estabilidade do treinamento e o desempenho das redes neurais amplas.

### Conceitos Fundamentais

#### 11.5.1 Arquitetura da Rede Neural
Considere uma rede neural com uma camada oculta definida como [^1]:
$$\
\Phi(x, w) = v^T \sigma(Ux + b) + c,
$$
onde:
- $x \in \mathbb{R}^d$ é a entrada.
- $U \in \mathbb{R}^{n \times d}$ são os pesos da primeira camada.
- $v \in \mathbb{R}^n$ são os pesos da segunda camada.
- $b \in \mathbb{R}^n$ é o bias da primeira camada.
- $c \in \mathbb{R}$ é o bias da segunda camada.
- $\sigma$ é a função de ativação.
- $w = (U, b, v, c) \in \mathbb{R}^{n(d+2)+1}$ é o vetor de todos os parâmetros da rede.

Os gradientes com respeito a esses parâmetros são dados por [^1]:
$$\
\nabla_U \Phi(x, w) = (v \odot \sigma'(Ux + b))x^T \in \mathbb{R}^{n \times d}
$$
$$\
\nabla_b \Phi(x, w) = v \odot \sigma'(Ux + b) \in \mathbb{R}^n
$$
$$\
\nabla_v \Phi(x, w) = \sigma(Ux + b) \in \mathbb{R}^n
$$
$$\
\nabla_c \Phi(x, w) = 1 \in \mathbb{R}
$$
onde $\odot$ denota o produto de Hadamard.

#### 11.5.2 Inicialização de LeCun
A inicialização de LeCun [^1] é uma técnica comum usada para melhorar a estabilidade e o desempenho do treinamento, definindo a variância dos pesos em cada camada como o inverso da dimensão de entrada da camada [^1]. Os parâmetros são inicializados aleatoriamente com distribuições específicas para os pesos e biases [^1]. Especificamente, os pesos $U$ e $v$ são inicializados com uma distribuição com média zero e variância $1/d$ e $1/n$, respectivamente, enquanto os biases $b$ e $c$ são inicializados como zero [^1]:
$$\
U_{0;ij} \sim \mathcal{D}(0, \frac{1}{d}), \quad v_{0;i} \sim \mathcal{D}(0, \frac{1}{n}), \quad b_{0;i}, c_0 = 0,
$$
onde $\mathcal{D}$ é uma distribuição com média zero e variância um, e $i = 1, ..., n$, $j = 1, ..., d$ [^1]. Assume-se que a distribuição $\mathcal{D}$ tenha expectativa zero, variância um e momentos finitos até a oitava ordem [^1]. Exemplos típicos para $\mathcal{D}(0, 1)$ são a distribuição normal padrão em $\mathbb{R}$ ou a distribuição uniforme em $[-\sqrt{3}, \sqrt{3}]$ [^1].

#### 11.5.3 Kernel Tangente Neural (NTK)
O kernel tangente neural (NTK) surge da linearização da rede neural em torno da inicialização [^1]. O kernel tangente empírico é dado por [^1]:
$$\
K_n(x, z) = \langle \nabla_w \Phi(x, w_0), \nabla_w \Phi(z, w_0) \rangle
$$
Sob a inicialização de LeCun, o kernel tangente empírico converge para um kernel específico, conhecido como kernel tangente neural (NTK) [^1]:
$$\
\lim_{n \to \infty} \frac{1}{n} K_n(x, z) = \mathbb{E}[\sigma'(u^T x) \sigma'(u^T z)] =: K^{LC}(x, z)
$$
onde $u \sim \mathcal{D}(0, 1/d)$ [^1].

#### 11.5.4 Dinâmica do Gradiente Descendente
O treinamento da rede neural envolve minimizar uma função de perda, como o erro quadrático médio [^1]:
$$\
f(w) = \sum_{i=1}^m (\Phi(x_i, w) - y_i)^2
$$
O gradiente descendente é usado para atualizar os parâmetros [^1]:
$$\
w_{k+1} = w_k - h \nabla f(w_k)
$$
onde $h$ é a taxa de aprendizado [^1]. Sob certas condições, o gradiente descendente converge para um minimizador global [^1].

#### 11.5.5 Proximidade ao Modelo Linearizado
A análise baseia-se na linearização $\Phi^{lin}$ que descreve o comportamento da rede completa numa vizinhança dos parâmetros iniciais.
Inicializando $w_0$ e definindo $p_0 = w_0$, o gradiente descendente computa as atualizações [^1]:
$$\
w_{k+1} = w_k - h \nabla_w f(w_k), \quad p_{k+1} = p_k - h \nabla_w f^{lin}(p_k)
$$
Escrevendo $\Phi(X, w) = (\Phi(x_i, w))_{i=1}^m \in \mathbb{R}^m$ e $\nabla_w \Phi(X, w) \in \mathbb{R}^{m \times n}$, tem-se [^1]:
$$\
\nabla_w f(w) = \nabla_w ||\Phi(X, w) - y||^2 = 2 \nabla_w \Phi(X, w)^T (\Phi(X, w) - y)
$$

### Conclusão

A inicialização de LeCun desempenha um papel crucial na estabilização e melhoria do desempenho do treinamento de redes neurais amplas [^1]. Ao definir a variância dos pesos como o inverso da dimensão de entrada, ela ajuda a normalizar a variância da saída através dos nós da rede [^1]. A análise do kernel tangente neural (NTK) fornece insights sobre o comportamento da rede durante o treinamento, mostrando que, sob certas condições, o gradiente descendente converge para um minimizador global [^1]. Além disso, a rede treinada permanece próxima ao modelo linearizado, permitindo uma análise mais fácil do comportamento da rede [^1]. Em resumo, a inicialização de LeCun, juntamente com a análise do NTK, oferece uma estrutura teórica para entender e treinar redes neurais amplas de forma eficaz [^1].

### Referências
[^1]: Capítulo 11 do texto fornecido.
<!-- END -->