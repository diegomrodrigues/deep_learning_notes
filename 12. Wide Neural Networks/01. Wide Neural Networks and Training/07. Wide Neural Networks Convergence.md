## 11.4 Convergência para Minimizadores Globais

### Introdução
Em continuidade ao estudo da dinâmica de treinamento de redes neurais de grande largura, este capítulo foca na convergência para minimizadores globais sob condições específicas. Como vimos anteriormente, redes neurais amplas tendem a exibir um comportamento linearizado [^1]. Esta seção investiga sob quais condições essa linearização é suficiente para garantir que o método do gradiente descendente encontre minimizadores globais da função objetivo original [^1]. A análise se baseia na suposição de que a função objetivo é "suficientemente próxima" de sua linearização, garantindo que o gradiente descendente ainda possa encontrar minimizadores globais [^8].

### Conceitos Fundamentais

A intuição por trás da convergência para minimizadores globais reside na ideia de que, se a função $w \rightarrow \Phi(x, w)$ não é linear, mas é "suficientemente próxima" de sua linearização $\Phi^{\text{lin}}$ definida em (11.3.1) [^7], então a função objetivo deve se comportar de maneira semelhante a uma função convexa. Isso permitiria que o gradiente descendente encontrasse minimizadores globais de (11.0.1b) [^7].

Para formalizar essa intuição, algumas premissas são necessárias. A **Assumption 11.12** [^8] estabelece condições sobre a função Φ e o ponto inicial $w_0$. Essas condições garantem que:
*   A matriz kernel do kernel tangente empírico (11.4.1) [^8] seja regular e seus autovalores pertençam a um intervalo específico $[\lambda_{\text{min}}, \lambda_{\text{max}}]$.
*   A norma do gradiente $\nabla_w \Phi(x_i, w)$ seja limitada por uma constante $U$ em uma bola $B_r(w_0)$ de raio $r$ centrada em $w_0$.
*   A função $w \rightarrow \Phi(x_i, w)$ seja $L$-suave na mesma bola $B_r(w_0)$.
*   A constante $L$ e o raio $r$ satisfaçam certas desigualdades relacionadas a $\lambda_{\text{min}}$, $U$ e $f(w_0)$.

O **Teorema 11.13** [^10] formaliza a convergência para minimizadores globais sob as condições estabelecidas na Assumption 11.12 [^8]. O teorema garante que, com uma taxa de aprendizado $h$ adequadamente escolhida (11.4.4) [^10], o gradiente descendente converge para um ponto próximo ao ponto inicial $w_0$ (11.4.6a) [^10] e que a função objetivo diminui exponencialmente (11.4.6b) [^10].

**Teorema 11.13.** Seja a Assumption 11.12 satisfeita e fixe uma taxa de aprendizado positiva
$$ h < \frac{1}{\lambda_{\text{min}} + \lambda_{\text{max}}} $$
Defina, para todo $k \in \mathbb{N}$,
$$ w_{k+1} = w_k - h \nabla f(w_k) $$
Então, vale para todo $k \in \mathbb{N}$:
$$ ||w_k - w_0|| < \frac{2 \sqrt{m} U}{\lambda_{\text{min}}} \sqrt{f(w_0)} $$
$$ f(w_k) \le (1 - h \lambda_{\text{min}})^{2k} f(w_0) $$
$\blacksquare$

A prova do Teorema 11.13 [^10] envolve o uso de indução matemática para mostrar que os iterados do gradiente descendente permanecem dentro da bola $B_r(w_0)$ e que a função objetivo diminui a cada iteração. A prova também utiliza o fato de que a matriz kernel tangente empírica é regular e que seus autovalores são limitados.

### Conclusão

Este capítulo apresentou um resultado abstrato que garante a convergência do gradiente descendente para minimizadores globais sob a condição de que a função objetivo seja suficientemente próxima de sua linearização. As Assumptions 11.12 [^8] e o Teorema 11.13 [^10] fornecem um framework para analisar a convergência de algoritmos de treinamento em redes neurais amplas. Nas próximas seções, será demonstrado que as condições da Assumption 11.12 [^8] são satisfeitas com alta probabilidade para certas redes neurais amplas [^9].

### Referências
[^1]: Capítulo 11, Wide neural networks, p. 139
[^2]: Capítulo 11, Wide neural networks, p. 139
[^3]: Capítulo 11, Wide neural networks, p. 139
[^4]: Capítulo 11, Wide neural networks, p. 139
[^5]: Capítulo 11, Wide neural networks, p. 139
[^6]: Capítulo 11, Wide neural networks, p. 139
[^7]: Capítulo 11, Wide neural networks, p. 146
[^8]: Capítulo 11, Wide neural networks, p. 146
[^9]: Capítulo 11, Wide neural networks, p. 147
[^10]: Capítulo 11, Wide neural networks, p. 148
[^11]: Capítulo 11, Wide neural networks, p. 148
[^12]: Capítulo 11, Wide neural networks, p. 148
[^13]: Capítulo 11, Wide neural networks, p. 149
[^14]: Capítulo 11, Wide neural networks, p. 149
[^15]: Capítulo 11, Wide neural networks, p. 149

<!-- END -->