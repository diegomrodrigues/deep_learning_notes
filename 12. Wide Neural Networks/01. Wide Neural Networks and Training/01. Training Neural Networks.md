## Treinamento de Redes Neurais e Otimização da Função de Perda

### Introdução
O treinamento de redes neurais envolve um processo iterativo de ajuste dos seus parâmetros, visando minimizar uma **função de perda**. Esta função quantifica a discrepância entre as previsões da rede e os valores reais dos dados [^1]. Este capítulo explora a dinâmica do treinamento de redes neurais, focando na minimização da função de perda através do método do **gradiente descendente**.

### Conceitos Fundamentais
O objetivo do treinamento é encontrar os parâmetros $w$ que minimizem a diferença entre a saída da rede neural $\Phi(x, w)$ e a saída verdadeira $y$ para um dado conjunto de pares de dados $(x_i, y_i)$ [^1]. A função de perda, comumente o **erro quadrático médio**, é definida como:

$$f(w) := \sum_{i=1}^{m} (\Phi(x_i, w) - y_i)^2$$

Esta função representa um múltiplo do risco empírico $R_S()$ para a amostra $S = (x_i, y_i)$ e o *square-loss* [^1].  O método do gradiente descendente, com um tamanho de passo constante $h$, é empregado para atualizar iterativamente os parâmetros da rede $w$ [^1].  A sequência de parâmetros $(w_k)_{k \in \mathbb{N}}$ é gerada através da seguinte iteração:

$$w_{k+1} := w_k - h \nabla f(w_k)$$

onde $\nabla f(w_k)$ representa o gradiente da função de perda em relação aos parâmetros $w$ [^1]. O objetivo principal é compreender como $\Phi(x, w_k)$ evolui à medida que $k$ aumenta, buscando parâmetros $w$ que minimizem a função de perda [^1].

Em continuidade ao capítulo anterior, no qual foi estabelecido que, para mapeamentos lineares $w \rightarrow \Phi(x, w)$, a função objetivo (11.0.1b) é convexa e o gradiente descendente converge para um minimizador global [^1], esta seção aborda o caso mais complexo de arquiteturas neurais típicas, onde $w \rightarrow \Phi(x, w)$ não é linear [^1].

Pesquisas recentes indicam que o comportamento de redes neurais tende a se linearizar nos parâmetros à medida que a largura da rede aumenta [^1].  Isto permite a aplicação de resultados e técnicas do caso linear ao treinamento de redes neurais [^1]. Em particular, será provado que o gradiente descendente pode encontrar minimizadores globais quando aplicado a (11.0.1b) para redes de largura muito grande [^1].

#### Linear Least-Squares
Um dos algoritmos de aprendizado de máquina mais simples é a regressão linear por mínimos quadrados. Dado o conjunto de dados (11.0.1a), a regressão linear busca ajustar uma função linear $\Phi(x,w) := x^T w$ em termos de $w$ minimizando $f(w)$ em (11.0.1b) [^1]. Com

$$A = \begin{pmatrix} x_1^T \\\\ \vdots \\\\ x_m^T \end{pmatrix} \in \mathbb{R}^{m \times d} \quad \text{e} \quad y = \begin{pmatrix} y_1 \\\\ \vdots \\\\ y_m \end{pmatrix} \in \mathbb{R}^m$$

tem-se

$$f(w) = ||Aw - y||^2$$

Mais geralmente, o *ansatz* $\Phi(x, (w,b)) := w^T x + b$ corresponde a

$$\Phi(x, (w, b)) = (1,x^T) \begin{pmatrix} w \\\\ b \end{pmatrix}.$$

Portanto, permitir adicionalmente um *bias* pode ser tratado analogamente [^1].

O modelo $\Phi(x, w) = x^T w$ é linear tanto em $x$ quanto em $w$. Em particular, $w \rightarrow f(w)$ é uma função convexa, e podemos aplicar os resultados de convergência do Capítulo 10 ao usar algoritmos baseados em gradiente [^1]. Se $A$ for invertível, então $f$ tem um minimizador único dado por $w^* = A^{-1}y$ [^1]. Se rank$(A) = d$, então $f$ é fortemente convexa e ainda existe um minimizador único. No entanto, se rank$(A) < d$, então ker$(A) \neq \\{0\\}$ e existem infinitos minimizadores de $f$ [^1]. Para garantir a unicidade, procuramos a solução de norma mínima (ou solução de 2-norma mínima)

$$w^* := \text{argmin}_{\\{w \in \mathbb{R}^d \mid f(w) \leq f(v) \forall v \in \mathbb{R}^d\\}} ||w||.$$

A proposição a seguir estabelece a unicidade de $w^*$ e demonstra que ele pode ser representado como uma superposição de $(x_i)_1^m$ [^1].

**Proposition 11.2.** Seja $A \in \mathbb{R}^{m \times d}$ e $y \in \mathbb{R}^m$ como em (11.1.1). Existe uma única solução de 2-norma mínima de (11.1.2). Denotando $H := \text{span}\\{x_1,...,x_m\\} \subseteq \mathbb{R}^d$, é o elemento único

$$w^* = \text{argmin}_{\tilde{w} \in H} f(\tilde{w}) \in H.$$

*Proof.* Começamos com a existência e unicidade. Seja $C \subseteq \mathbb{R}^m$ o espaço gerado pelas colunas de $A$ [^1]. Então $C$ é fechado e convexo, e portanto $y^* = \text{argmin}_{\tilde{y} \in C} ||y - \tilde{y}||$ existe e é único (esta é uma propriedade fundamental dos espaços de Hilbert, veja o Teorema B.14) [^1]. Em particular, o conjunto $M = \\{w \in \mathbb{R}^d \mid Aw = y^*\\} \subseteq \mathbb{R}^d$ de minimizadores de $f$ não está vazio [^1]. Claramente $M$ também é fechado e convexo [^1]. Pelo mesmo argumento de antes, $w^* = \text{argmin}_{w \in M} ||w^*||$ existe e é único [^1].

Resta mostrar (11.1.4). Denote por $w^*$ a solução de norma mínima e decomponha $w^* = \tilde{w} + \hat{w}$ com $\tilde{w} \in H$ e $\hat{w} \in H^\perp$ [^1]. Temos $Aw^* = A\tilde{w}$ e $||w^*||^2 = ||\tilde{w}||^2 + ||\hat{w}||^2$ [^1]. Como $w^*$ é a solução de norma mínima, deve valer $\hat{w} = 0$. Portanto, $w^* \in H$ [^1]. Finalmente, suponha que exista um minimizador $v$ de $f$ em $H$ diferente de $w^*$. Então $0 \neq w^* - v \in H$, e como $H$ é gerado pelas linhas de $A$, temos $A(w^* - v) \neq 0$ [^1]. Assim, $y^* = Aw^* \neq Av$, o que contradiz o fato de que $v$ minimiza $f$. $\blacksquare$

A condição de minimizar a 2-norma é uma forma de regularização [^1]. Curiosamente, o gradiente descendente converge para a solução de norma mínima para o objetivo quadrático (11.1.2), desde que $w_0$ seja inicializado dentro de $H = \text{span}\\{x_1,...,x_m\\}$ (e.g., $w_0 = 0$) [^1]. Portanto, ele não encontra um minimizador "arbitrário", mas regulariza implicitamente o problema neste sentido [^1]. Em seguida, $\sigma_{\text{max}}(A)$ denota o valor singular máximo de $A$ [^1].

**Theorem 11.3.** Seja $A \in \mathbb{R}^{m \times d}$ como em (11.1.1), seja $w_0 = \tilde{w}_0 + \hat{w}_0$ onde $\tilde{w}_0 \in H$ e $\hat{w}_0 \in H^\perp$ [^1]. Fixe $h \in (0,1/(2\sigma_{\text{max}}(A)^2))$ e defina

$$w_{k+1} := w_k - h\nabla f(w_k) \quad \text{para todo } k \in \mathbb{N}$$

com $f$ em (11.1.2). Então

$$\lim_{k \rightarrow \infty} w_k = w^* + \hat{w}_0.$$

Esboçamos o argumento no caso em que $w_0 \in H$ e deixamos a prova completa para o leitor, veja o Exercício 11.32 [^1]. Note que $H$ é o espaço gerado pelas linhas de $A$ (ou as colunas de $A^T$) [^1]. O gradiente da função objetivo é igual a

$$\nabla f(w) = 2A^T(Aw - y).$$

Portanto, se $w_0 \in H$, então as iterações do gradiente descendente nunca saem do subespaço $H$ [^1]. Pelo Exercício 10.34 e Teorema 10.11, para um tamanho de passo pequeno o suficiente, vale $f(w_k) \rightarrow 0$ [^1]. Pela Proposição 11.2, existe apenas um minimizador em $H$, correspondendo à solução de norma mínima [^1]. Assim, $w_k$ converge para a solução de norma mínima.

#### Kernel Least-Squares
Sejam novamente $(x_j, y_j) \in \mathbb{R}^d \times \mathbb{R}, j = 1,...,m$ [^1]. Em muitas aplicações, os modelos lineares são muito simplistas e não conseguem capturar a verdadeira relação entre $x$ e $y$ [^1]. Os métodos de *kernel* permitem superar este problema introduzindo não-linearidade em $x$, mas mantendo a linearidade no parâmetro $w$ [^1].

Seja $H$ um espaço de Hilbert com produto interno $\langle \cdot, \cdot \rangle_H$, que também é referido como o espaço de *features* [^1]. Para um mapa de *features* (tipicamente não-linear) $\phi : \mathbb{R}^d \rightarrow H$, considere o modelo

$$\Phi(x, w) = \langle \phi(x), w \rangle_H$$

com $w \in H$ [^1]. Se $H = \mathbb{R}^n$, os componentes de $\phi$ são referidos como *features* [^1]. Com a função objetivo

$$f(w) := \sum_{j=1}^m (\langle \phi(x), w \rangle - y_j)^2 \quad w \in H,$$

desejamos determinar um minimizador de $f$ [^1]. Para garantir a unicidade e regularizar o problema, novamente consideramos a solução de norma $H$ mínima

$$w^* := \text{argmin}_{\\{w \in H \mid f(w) \leq f(v) \forall v \in H\\}} ||w||_H.$$

Como veremos abaixo, $w^*$ está bem definido [^1]. Chamaremos $\Phi(x, w^*) = \langle \phi(x), w^* \rangle_H$ o estimador de mínimos quadrados do *kernel* [^1]. A não-linearidade do mapa de *features* permite modelos mais expressivos $x \rightarrow \Phi(x, w)$ capazes de capturar estruturas mais complicadas além da linearidade nos dados [^1].

**Remark 11.4 (Gradient descent).** Seja $H = \mathbb{R}^n$ equipado com o produto interno Euclidiano [^1]. Considere a sequência $(w_k)_{k \in \mathbb{N}_0} \subseteq \mathbb{R}^n$ gerada pelo gradiente descendente para minimizar (11.2.2) [^1]. Assumindo um tamanho de passo suficientemente pequeno, pelo Teorema 11.3 para $x \in \mathbb{R}^d$

$$\lim_{k \rightarrow \infty} \Phi(x, w_k) = \langle \phi(x), w^* \rangle + \langle \phi(x), \hat{w}_0 \rangle .$$

Aqui, $\hat{w}_0 \in \mathbb{R}^n$ denota a projeção ortogonal de $w_0 \in \mathbb{R}^n$ em $H^\perp$, onde $H := \text{span}\\{\phi(x_1),...,\phi(x_m)\\}$ [^1]. O gradiente descendente, portanto, fornece o estimador de mínimos quadrados do *kernel* mais $\langle \phi(x), \hat{w}_0 \rangle$. Notavelmente, no conjunto

$$\\{x \in \mathbb{R}^d \mid \phi(x) \in \text{span}\\{\phi(x_1),...,\phi(x_m)}\\}\\},$$

(11.2.3) coincide com o estimador de mínimos quadrados do *kernel* independente da inicialização $w_0$ [^1].

### Conclusão
Este capítulo estabeleceu os fundamentos para entender o treinamento de redes neurais como um processo de otimização da função de perda. Ao explorar o método do gradiente descendente e suas propriedades, tanto em contextos lineares quanto não lineares, e ao introduzir os conceitos de *linear least-squares* e *kernel least-squares*, fornecemos uma base sólida para análises mais avançadas. Os conceitos e resultados aqui apresentados são cruciais para a compreensão da dinâmica de treinamento de redes neurais de grande largura, que será explorada em seções subsequentes.

### Referências
[^1]: Capítulo 11, "Wide neural networks", página 139-141, do texto fornecido.
<!-- END -->