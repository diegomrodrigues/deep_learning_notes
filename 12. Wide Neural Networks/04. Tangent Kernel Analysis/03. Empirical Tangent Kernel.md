## Kernel Least Squares Regression e o Empirical Tangent Kernel

### Introdução
Este capítulo explora a relação entre a minimização do erro quadrático para um modelo linearizado e a regressão de mínimos quadrados do kernel (kernel least squares regression), com foco no **empirical tangent kernel**. A análise se baseia na linearização de modelos não lineares, uma técnica crucial para entender o comportamento de redes neurais amplas [^1]. A linearização permite aplicar resultados e técnicas do caso linear ao treinamento de redes neurais complexas.

### Conceitos Fundamentais
Considerando um modelo geral $\Phi(x, w)$, onde $x \in \mathbb{R}^d$ é a entrada e $w \in \mathbb{R}^n$ são os parâmetros, o objetivo é minimizar o erro quadrático [^1]:
$$\nf(w) = \sum_{i=1}^{m} (\Phi(x_i, w) - y_i)^2\n$$
onde $(x_i, y_i)$ são os pares de dados.

Para modelos não lineares, a função objetivo $f(w)$ geralmente não é convexa, tornando a otimização um desafio. A **linearização** do modelo $\Phi(x, w)$ em torno de um ponto inicial $w_0$ simplifica o problema [^1]:
$$\n\Phi^{lin}(x, w) = \Phi(x, w_0) + \nabla_w\Phi(x, w_0)^T (w - w_0)\n$$
onde $\nabla_w\Phi(x, w_0)$ é o gradiente de $\Phi(x, w)$ em relação a $w$, avaliado em $w_0$.

Minimizar o erro quadrático para o modelo linearizado $\Phi^{lin}(x, w)$ [^7]:
$$\nf^{lin}(w) = \sum_{i=1}^{m} (\Phi^{lin}(x_i, w) - y_i)^2 = \sum_{i=1}^{m} (\langle \nabla_w\Phi(x_i, w_0), w \rangle + \delta_i)^2\n$$
onde $\langle \cdot, \cdot \rangle$ denota o produto interno Euclidiano em $\mathbb{R}^n$ e $\delta_i = \Phi(x_i, w_0) - \nabla_w\Phi(x_i, w_0)^T w_0 - y_i$ [^7], corresponde a uma **kernel least squares regression** com um **feature map** [^7]:
$$\n\phi(x) = \nabla_w\Phi(x, w_0) \in \mathbb{R}^n\n$$
O kernel correspondente é o **empirical tangent kernel** [^7]:
$$\nK_n(x, x') = \langle \nabla_w\Phi(x, w_0), \nabla_w\Phi(x', w_0) \rangle\n$$
O empirical tangent kernel $K_n(x, x')$ é definido como o produto interno dos gradientes do modelo em relação aos parâmetros $w$, avaliados nos parâmetros iniciais $w_0$ [^7]. Ele surge da aproximação de primeira ordem (tangente) do modelo original $\Phi$ em torno da inicialização $w_0$ [^7]. É importante notar que o kernel depende da escolha de $w_0$ [^7].

**Em resumo:**
*   Minimizar o erro quadrático do modelo linearizado é equivalente a realizar uma regressão de mínimos quadrados do kernel.
*   O feature map é dado pelo gradiente do modelo original em relação aos parâmetros, avaliado no ponto inicial.
*   O kernel resultante é o empirical tangent kernel, que mede a similaridade entre as entradas $x$ e $x'$ com base na similaridade de seus gradientes em torno do ponto inicial $w_0$.

A escolha do ponto inicial $w_0$ é crucial, pois ele influencia o empirical tangent kernel e, consequentemente, a solução da regressão de mínimos quadrados. O modelo linearizado $\Phi^{lin}$ captura o comportamento de $\Phi$ apenas para parâmetros $w$ próximos a $w_0$ [^7].

### Conclusão
A análise do empirical tangent kernel fornece uma ferramenta para entender o comportamento de modelos não lineares, como redes neurais, através da linearização e da aplicação de técnicas de regressão de mínimos quadrados do kernel. A dependência do kernel em relação ao ponto inicial $w_0$ destaca a importância da escolha da inicialização no treinamento de modelos complexos.

### Referências
[^1]: Capítulo 11, Wide neural networks.
[^7]: Seção 11.3, Tangent kernel.
<!-- END -->