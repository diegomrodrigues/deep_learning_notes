## Linearização do Modelo e Kernel Tangente Empírico

### Introdução
Este capítulo se baseia nos conceitos de treinamento de redes neurais de grande largura e nos métodos de mínimos quadrados, explorados anteriormente [^1]. Em particular, expandimos a noção de linearização em torno dos parâmetros da rede, introduzida na Seção 11.5.2 [^1], para analisar o comportamento do kernel tangente. Nosso foco será na aproximação de primeira ordem de Taylor do modelo da rede neural e na derivação do kernel tangente empírico correspondente.

### Conceitos Fundamentais

Para simplificar a análise do treinamento de redes neurais, o modelo $\Phi(x, w)$ é linearizado em torno de um ponto de inicialização $w_0$ usando uma aproximação de Taylor de primeira ordem [^1]:
$$\
\Phi_{lin}(x, w) = \Phi(x, w_0) + \nabla_w\Phi(x, w_0)^T(w - w_0).
$$\
Aqui, $\Phi_{lin}(x, w)$ representa a aproximação linear do modelo, $x$ é a entrada, $w$ são os parâmetros da rede, $w_0$ é o ponto de inicialização, e $\nabla_w\Phi(x, w_0)$ é o gradiente de $\Phi$ em relação a $w$ avaliado em $w_0$ [^1]. Essa linearização permite transferir técnicas e resultados do caso linear para o treinamento de redes neurais, especialmente quando a largura da rede aumenta [^1].

Para facilitar a notação, define-se $\delta_i$ como [^1]:
$$\
\delta_i := \Phi(x_i, w_0) - \nabla_w\Phi(x_i, w_0)^T w_0 - y_i,
$$\
para todo $i = 1, ..., m$, onde $(x_i, y_i)$ são os pares de dados de treinamento [^1].

A função de perda do modelo linearizado é dada por [^1]:
$$\
f_{lin}(w) := \sum_{i=1}^m (\Phi_{lin}(x_i, w) - y_i)^2 = \sum_{i=1}^m (\langle \nabla_w\Phi(x_i, w_0), w \rangle + \delta_i)^2,
$$\
onde $\langle \cdot, \cdot \rangle$ representa o produto interno Euclidiano em $\mathbb{R}^n$ [^1]. Minimizar $f_{lin}$ corresponde a uma regressão de mínimos quadrados com o mapa de características $\phi(x) = \nabla_w\Phi(x, w_0) \in \mathbb{R}^n$ [^1].

O *kernel tangente empírico* $K_n(x, x')$ é definido como [^1]:
$$\
K_n(x, x') = \langle \nabla_w\Phi(x, w_0), \nabla_w\Phi(x', w_0) \rangle.
$$\
Este kernel surge da aproximação de Taylor de primeira ordem do modelo original em torno da inicialização $w_0$ [^1]. É importante notar que o kernel depende da escolha de $w_0$ [^1].

Treinar $\Phi_{lin}$ com descida do gradiente leva ao estimador de mínimos quadrados com o kernel $K_n$ mais um termo adicional dependendo de $w_0$ [^1]. O modelo linearizado $\Phi_{lin}$ captura o comportamento de $\Phi$ apenas para parâmetros $w$ próximos a $w_0$ [^1]. Se os parâmetros permanecerem próximos à inicialização durante o treinamento, o comportamento de $\Phi$ e $\Phi_{lin}$ será similar [^1].

### Conclusão

A linearização do modelo e a introdução do kernel tangente empírico fornecem uma estrutura para analisar o comportamento de redes neurais, especialmente em regimes de grande largura. A proximidade entre o modelo original e sua linearização é crucial para garantir que o treinamento do modelo linearizado seja representativo do treinamento do modelo original. Os resultados apresentados aqui pavimentam o caminho para entender a convergência para minimizadores globais sob certas condições, exploradas nas seções subsequentes [^1]. <!-- END -->