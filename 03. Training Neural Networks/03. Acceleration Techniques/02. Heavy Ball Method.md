## Heavy Ball Method: Uma Abordagem de Aceleração
### Introdução
Este capítulo aprofunda-se nas técnicas de aceleração no contexto do treinamento de redes neurais, com foco específico no **método da heavy ball** [^178]. Este método, introduzido por Polyak em 1964, busca acelerar a convergência, mitigando as oscilações comuns em paisagens de perda mal condicionadas. A ideia central é simular a dinâmica de uma bola pesada rolando pelo vale da superfície de perda [^10.4].

### Conceitos Fundamentais
O método da heavy ball atualiza os pesos usando uma **média exponencialmente ponderada de todos os gradientes passados** [^1]. Essa abordagem suaviza o caminho e reduz as oscilações, que são frequentes em cenários de perda mal condicionados [^1].

**Interpretação da Equação Diferencial Ordinária (ODE)**

O método da heavy ball pode ser interpretado como uma discretização de uma ODE de segunda ordem [^1]:
$$\nmw''(t) = -\nabla f(w(t)) - rw'(t)\n$$
onde:
*   $m$ representa a massa do ponto [^10.4]
*   $w(t)$ denota a posição do ponto no tempo $t$ [^111]
*   $\nabla f(w(t))$ é o gradiente da função objetivo [^111]
*   $r$ é o coeficiente de fricção [^10.4]

Nesta analogia [^10.4]:
*   O termo de momentum corresponde a uma **massa positiva** ($m > 0$) [^1]
*   O termo de fricção corresponde à **direção negativa da velocidade atual** ($r > 0$) [^1]
*   $-\nabla f(w(t))$ representa a força que empurra a bola para o mínimo [^10.4]
*   $-rw'(t)$ representa a fricção que diminui a velocidade [^10.4]

**Discretização do Método Heavy Ball**

A discretização da ODE resulta na seguinte regra de atualização [^10.4]:
$$\nw_{k+1} = w_k - \alpha \nabla f(w_k) + \beta (w_k - w_{k-1})\n$$
onde:
*   $w_{k+1}$ é o peso atualizado [^10.4]
*   $w_k$ é o peso atual [^10.4]
*   $\nabla f(w_k)$ é o gradiente no ponto $w_k$ [^10.4]
*   $w_{k-1}$ é o peso anterior [^10.4]
*   $\alpha$ é a taxa de aprendizado [^10.4]
*   $\beta$ é o coeficiente de momentum [^10.4]

**Análise e Ajuste de Hiperparâmetros**

Os hiperparâmetros $\alpha$ e $\beta$ desempenham um papel crucial no desempenho do método. O ajuste fino desses parâmetros é essencial para equilibrar a força do gradiente e o termo de momentum [^10.4].

O método da heavy ball, ao contrário do gradient descent, pode não convergir sob certas condições [^10.4]. No entanto, quando ajustado corretamente, pode levar a uma convergência mais rápida do que o gradient descent padrão.

**Exemplo Quadrático Bidimensional**
Para ilustrar a eficácia do método heavy ball, considere a função objetivo quadrática em duas dimensões [^10.4]:
$$\nf(w) = \frac{1}{2} w^T D w\n$$
onde $D = \begin{pmatrix} \lambda_1 & 0 \\\\ 0 & \lambda_2 \end{pmatrix}$ e $\lambda_1 > \lambda_2 > 0$ [^10.4].

Para essa função, o gradient descent com taxa de aprendizado constante $h$ resulta em convergência lenta [^10.4]:
$$\nw_{k+1} = w_k - hDw_k\n$$
O método heavy ball, por outro lado, pode atingir uma taxa de convergência que depende da raiz quadrada do número de condição $\kappa = \lambda_1 / \lambda_2$ [^10.4].

### Conclusão
O método da heavy ball oferece uma abordagem promissora para acelerar o treinamento de redes neurais, especialmente em paisagens de perda mal condicionadas [^1]. Ao introduzir um termo de momentum, o método suaviza o caminho da otimização e reduz as oscilações, resultando em uma convergência mais rápida [^1]. Embora exija um ajuste cuidadoso dos hiperparâmetros, o método da heavy ball pode superar o gradient descent padrão em muitos cenários práticos [^10.4]. Como veremos na próxima seção, o método de Nesterov oferece uma melhora em relação ao método da heavy ball [^10.4].

### Referências
[^1]: Capítulo fornecido.
[^178]: Polyak, B. T. *Some methods of speeding up the convergence of iteration methods*. USSR Computational Mathematics and Mathematical Physics, 4(5), 1–17, 1964.
[^10.4]: Seção 10.4 do capítulo fornecido.
[^111]: A ODE descreve o movimento de uma partícula $w(t)$, cuja velocidade no tempo $t > 0$ é igual a $-\nabla f(w(t))$ - o vetor de descida mais acentuada.
[^10.4]: A interpretação como uma discretização de ODE de segunda ordem fornece uma visão sobre o comportamento do método heavy ball.
<!-- END -->