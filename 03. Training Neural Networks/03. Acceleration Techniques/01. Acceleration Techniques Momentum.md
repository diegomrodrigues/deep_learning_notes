## Técnicas de Aceleração em Otimização Convexa

### Introdução
Como discutido anteriormente, o método do gradiente descendente é uma ferramenta fundamental para minimizar funções objetivas, especialmente em contextos de aprendizado de máquina e redes neurais [^1]. No entanto, sua taxa de convergência pode ser lenta, especialmente para funções mal condicionadas [^1]. Para mitigar essa limitação, várias técnicas de aceleração foram desenvolvidas, visando melhorar a velocidade com que o algoritmo se aproxima do mínimo da função objetivo. Este capítulo se concentrará em técnicas de aceleração, como o método da bola pesada e a aceleração de Nesterov, que incorporam termos de *momentum* para acelerar a convergência [^1].

### Conceitos Fundamentais

#### Método da Bola Pesada (Heavy Ball Method)
O método da bola pesada, introduzido por Polyak em 1964 [^1], [^178], é inspirado na dinâmica de uma bola pesada rolando pela superfície da função de perda. A ideia central é adicionar um termo de *momentum* à atualização do gradiente descendente, que leva em consideração as atualizações anteriores. A formulação do método é dada por:

$$
w_{k+1} = w_k - \alpha \nabla f(w_k) + \beta (w_k - w_{k-1}) \text{, [^1] [^10.4.5]}
$$

onde:
- $w_k$ representa a iteração atual dos pesos
- $\alpha > 0$ é a taxa de aprendizado
- $\nabla f(w_k)$ é o gradiente da função objetivo no ponto $w_k$
- $\beta \in (0, 1)$ é o parâmetro de *momentum* que controla a influência das atualizações anteriores [^1]
- $w_k - w_{k-1}$ representa a diferença entre a iteração atual e a anterior, incorporando o *momentum* [^1]

O termo $\beta(w_k - w_{k-1})$ atua como uma "inércia", ajudando a superar vales rasos e oscilações, suavizando o caminho para o mínimo [^1].  Ao expandir iterativamente a equação [^10.4.5], obtemos:

$$
w_{k+1} = w_0 - \alpha \sum_{j=0}^{k} \beta^j \nabla f(w_{k-j}) \text{, [^1] [^10.4.6]}
$$

Essa equação mostra que a atualização de $w_k$ é uma média ponderada exponencialmente de todos os gradientes passados. O parâmetro $\beta$ determina o balanço entre o impacto dos gradientes recentes e passados [^1].

**Interpretação Física:** O método da bola pesada pode ser interpretado como uma discretização da seguinte equação diferencial de segunda ordem [^1]:

$$
m w''(t) = -\nabla f(w(t)) - r w'(t) \text{, [^1] [^10.4.7]}
$$

onde:
- $m$ representa a massa da bola
- $\nabla f(w(t))$ é a força que atua sobre a bola, correspondendo ao gradiente negativo da função objetivo [^1]
- $r$ é o coeficiente de atrito, representando a resistência ao movimento [^1]

A discretização dessa equação leva à formulação do método da bola pesada. O termo de *momentum* está relacionado à massa da bola, enquanto o gradiente corresponde à força que a impulsiona em direção ao mínimo [^1].

#### Aceleração de Nesterov (Nesterov Accelerated Gradient - NAG)
A aceleração de Nesterov, proposta por Nesterov [^1], [^158], é uma refinamento do método da bola pesada que frequentemente oferece uma convergência ainda mais rápida. A principal diferença é que o gradiente é avaliado em um ponto "projetado para o futuro", em vez do ponto atual [^1]. O método NAG é formulado como um processo de dois passos [^1]:

$$
v_{k+1} = w_k - \alpha \nabla f(w_k) \text{, [^1] [^10.4.10a]}
$$
$$
w_{k+1} = v_{k+1} + \beta (v_{k+1} - v_k) \text{, [^1] [^10.4.10b]}
$$

onde:
- $v_k$ é um ponto intermediário
- $w_k = v_k + \beta(v_k - v_{k-1})$ é uma estimativa da posição na próxima iteração, e o gradiente é avaliado nesse ponto [^1]

A chave é que o gradiente não é avaliado na posição atual $v_k$, mas sim em $w_k$, que é uma estimativa da posição na próxima iteração [^1].

**Reescrita da Formulação:**  Para análise, a atualização de Nesterov pode ser reescrita como uma sequência de três passos [^1]:

$$\nu_0 = \frac{(1 + \tau)w_0 - v_0}{\tau} \text{, [^1]}
$$
$$
w_k = \frac{\tau}{1 + \tau} u_k + \frac{1}{1 + \tau}v_k \text{, [^1] [^10.4.11a]}
$$
$$
v_{k+1} = w_k - \alpha \nabla f(w_k) \text{, [^1] [^10.4.11b]}
$$
$$
\nu_{k+1} = u_k + \tau (w_k - u_k) - \frac{1}{L} \nabla f(w_k) \text{, [^1] [^10.4.11c]}
$$

onde $\tau = \sqrt{\mu/L}$, $\alpha = 1/L$, e $\beta = (1-\tau)/(1+\tau)$ [^1].

**Convergência:** Para funções $L$-suaves e $\mu$-fortemente convexas, a aceleração de Nesterov atinge uma taxa de convergência de $1 - \sqrt{\mu/L}$, que é mais rápida do que a taxa de $1 - \mu/L$ do gradiente descendente [^1].

### Conclusão

As técnicas de aceleração, como o método da bola pesada e a aceleração de Nesterov, representam avanços significativos na otimização de funções convexas, especialmente em contextos onde a convergência rápida é essencial. Ao incorporar termos de *momentum*, esses métodos conseguem mitigar as limitações do gradiente descendente padrão, oferecendo uma convergência mais eficiente e robusta, especialmente para funções mal condicionadas. A escolha entre esses métodos e outros algoritmos de otimização depende das características específicas do problema em questão, incluindo a suavidade, a convexidade e o condicionamento da função objetivo.

### Referências

[^1]: Capítulo 10 do texto fornecido.
[^178]: Polyak, B. T. "Some methods of speeding up the convergence of iteration methods." *USSR Computational Mathematics and Mathematical Physics* 4.5 (1964): 1-17.
[^158]: Nesterov, Yu. "A method of solving a convex programming problem with convergence rate O(1/k^2)." *Soviet Mathematics Doklady*. Vol. 27. No. 2. 1983.
<!-- END -->