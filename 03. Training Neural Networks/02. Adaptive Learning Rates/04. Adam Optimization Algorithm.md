## Adam: Combinação de Taxas de Aprendizagem Adaptativas com Momentum
### Introdução
Este capítulo explora o algoritmo **Adam (Adaptive Moment Estimation)**, um método de otimização popular em aprendizado profundo que combina as vantagens de taxas de aprendizado adaptativas, semelhantes ao RMSProp, com o momentum. Adam calcula taxas de aprendizado adaptativas individuais para diferentes parâmetros, utilizando estimativas dos primeiros e segundos momentos dos gradientes [^114].

### Conceitos Fundamentais
O algoritmo Adam [^114] é uma evolução dos métodos de otimização que utilizam taxas de aprendizado adaptativas, como o RMSProp, e incorpora o conceito de momentum para acelerar a convergência. A atualização dos parâmetros em Adam é dada por [^1]:
$$
W_{k+1} = W_k - a m_{k+1} \oslash \sqrt{v_{k+1} + \varepsilon}
$$
onde:

*   $W_k$ representa os pesos no passo $k$.
*   $m_{k+1}$ é uma estimativa do primeiro momento (média) do gradiente.
*   $v_{k+1}$ é uma estimativa do segundo momento (variância não centralizada) do gradiente.
*   $\alpha$ é a taxa de aprendizado base.
*   $\varepsilon$ é uma constante pequena para evitar divisão por zero.
*   $\oslash$ denota a divisão component-wise.

As estimativas dos momentos são calculadas exponencialmente como:
$$
m_{k+1} = \beta_1 m_k + (1 - \beta_1)\nabla f(W_k)
$$
$$
v_{k+1} = \gamma_1 v_k + (1 - \gamma_1)\nabla f(W_k) \odot \nabla f(W_k)
$$
onde:

*   $\beta_1$ é o fator de decaimento exponencial para a estimativa do primeiro momento.
*   $\gamma_1$ é o fator de decaimento exponencial para a estimativa do segundo momento.
*   $\nabla f(W_k)$ é o gradiente da função objetivo no passo $k$.
*   $\odot$ denota a multiplicação component-wise (Hadamard product).

No contexto do Adam, os parâmetros $\beta_2$ e $\gamma_2$ são definidos como [^1]:
$$
\beta_2 = 1 - \beta_1 \in (0, 1)
$$
$$
\gamma_2 = 1 - \gamma_1 \in (0, 1)
$$
A taxa de aprendizado adaptativa $\alpha_k$ é ajustada da seguinte forma [^1]:
$$
\alpha_k = \alpha \frac{\sqrt{1 - \gamma_1^{k+1}}}{1 - \beta_1^{k+1}}
$$
Essa correção é importante porque as estimativas dos momentos são inicializadas em zero, o que pode levar a um viés no início do treinamento.

As etapas do algoritmo Adam podem ser resumidas como [^1]:

1.  Inicializar os parâmetros: $m_0 = 0$, $v_0 = 0$, $W_0$.
2.  Para $k = 0, 1, 2, ...$ até a convergência:
    a. Calcular o gradiente: $g_k = \nabla f(W_k)$.
    b. Atualizar as estimativas dos momentos:
    $$
    m_{k+1} = \beta_1 m_k + (1 - \beta_1)g_k
    $$
    $$
    v_{k+1} = \gamma_1 v_k + (1 - \gamma_1)g_k \odot g_k
    $$
    c. Corrigir o viés:
    $$
    \hat{m}_{k+1} = \frac{m_{k+1}}{1 - \beta_1^{k+1}}
    $$
    $$
    \hat{v}_{k+1} = \frac{v_{k+1}}{1 - \gamma_1^{k+1}}
    $$
    d. Atualizar os parâmetros:
    $$
    W_{k+1} = W_k - \alpha \frac{\hat{m}_{k+1}}{\sqrt{\hat{v}_{k+1}} + \varepsilon}
    $$

Os valores padrão recomendados para os hiperparâmetros são $\alpha = 0.001$, $\beta_1 = 0.9$, $\gamma_1 = 0.999$ e $\varepsilon = 10^{-8}$ [^114]. No entanto, a escolha ideal desses parâmetros pode depender do problema específico.

### Conclusão
Adam é um algoritmo de otimização poderoso e amplamente utilizado em aprendizado profundo devido à sua capacidade de adaptar as taxas de aprendizado para diferentes parâmetros e incorporar o momentum. A combinação dessas características permite que Adam convirja de forma eficiente em uma variedade de problemas de otimização, embora existam exemplos de funções convexas para as quais Adam não converge para um minimizador [^188]. Modificações como o AMSGrad [^188] visam mitigar esse problema.

### Referências
[^1]: [114] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.
[^2]: [188] Reddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of adam and beyond. *arXiv preprint arXiv:1904.09237*.
[^3]: [1] (Informações extraídas diretamente do contexto fornecido.)

<!-- END -->