## Métodos Adaptativos de Taxa de Aprendizagem

### Introdução
Como vimos anteriormente [^110], o treinamento de redes neurais envolve a minimização de uma função objetivo $f: \mathbb{R}^n \rightarrow \mathbb{R}$, onde $n$ representa o número de pesos e biases na rede neural. O método de **gradient descent** [^110] é uma abordagem iterativa para encontrar um minimizador $w^* \in \mathbb{R}^n$ tal que $f(w^*) \leq f(w)$ para todo $w \in \mathbb{R}^n$. No entanto, a escolha da **learning rate** $h_k$ [^110] é crucial para o sucesso do algoritmo. Uma learning rate muito alta pode levar a *overshooting* [^111], enquanto uma learning rate muito baixa pode resultar em *slow convergence* [^111].

Neste capítulo, exploraremos métodos adaptativos de taxa de aprendizagem que ajustam dinamicamente as learning rates durante a otimização, potencialmente melhorando o desempenho. Focaremos em **AdaGrad**, **RMSProp** e **Adam**, que empregam mini-batches, aceleração e tamanhos de passo adaptativos [^24]. Esses métodos são casos especiais da atualização geral [^24]:

$$
m_{k+1} = \beta_1 m_k + \beta_2 \nabla f(w_k) \\
v_{k+1} = \gamma_1 v_k + \gamma_2 \nabla f(w_k) \odot \nabla f(w_k) \\
w_{k+1} = w_k - \alpha_k m_{k+1} \oslash \sqrt{v_{k+1} + \epsilon}
$$

onde $\odot$ e $\oslash$ denotam multiplicação e divisão *component-wise*, respectivamente [^24], e $\epsilon$ é um termo de estabilidade para evitar divisão por zero [^24].

### Conceitos Fundamentais

#### AdaGrad
O algoritmo **AdaGrad** (Adaptive Gradient Algorithm) [^133] ajusta dinamicamente as learning rates [^24] durante a otimização [^133], utilizando learning rates individuais para cada componente [^133]. Ele corresponde a um caso específico de (10.5.1) [^24] com $\beta_1 = 0, \gamma_1 = 0, \beta_2 = \gamma_2 = 1$ e $\alpha_k = \alpha$ para todo $k \in \mathbb{N}_0$ [^133]. Assim, restam os hiperparâmetros $\epsilon > 0$ e $\alpha > 0$ [^133]. O AdaGrad realiza a seguinte atualização [^133]:

$$
v_{k+1} = v_k + \nabla f(w_k) \odot \nabla f(w_k) \\
w_{k+1} = w_k - \alpha \nabla f(w_k) \oslash \sqrt{v_{k+1} + \epsilon}
$$

Como [^134]
$$
v_{k+1} = \sum_{j=0}^{k} \nabla f(w_j) \odot \nabla f(w_j)
$$

o algoritmo escala o gradiente $\nabla f(w_k)$ na atualização *component-wise* pela raiz quadrada inversa da soma de todos os gradientes quadrados passados mais $\epsilon$ [^134]. O fator de escala $(v_{k+1,i} + \epsilon)^{-1/2}$ para o componente $i$ será grande se os gradientes anteriores para esse componente forem pequenos, e vice-versa [^134]. Conforme os autores de [56]: *"nossos procedimentos dão taxas de aprendizado muito baixas para características que ocorrem frequentemente e taxas de aprendizado altas para características infrequentes"* [^134].

**Destaque:** A principal vantagem do AdaGrad é a sua capacidade de adaptar as learning rates individualmente para cada parâmetro, permitindo uma convergência mais rápida em problemas com características esparsas [^134].

#### RMSProp
O algoritmo **RMSProp** (Root Mean Square Propagation) [^134] busca retificar a rápida redução das learning rates, que pode ocorrer com AdaGrad, ajustando as learning rates usando uma média exponencialmente ponderada dos gradientes passados [^134]. O RMSProp corresponde a (10.5.1) [^24] com $\beta_1 = 0, \beta_2 = 1, \alpha_k = \alpha$ para todo $k \in \mathbb{N}_0$ e $\gamma_2 = 1 - \gamma_1 \in (0,1)$ [^134], efetivamente deixando os hiperparâmetros $\epsilon > 0, \gamma_1 \in (0,1)$ e $\alpha > 0$ [^134]. Os valores padrão recomendados são $\epsilon = 10^{-8}, \alpha = 0.01$ e $\gamma_1 = 0.9$ [^134]. O algoritmo é dado por [^134]:

$$
v_{k+1} = \gamma_1 v_k + (1 - \gamma_1) \nabla f(w_k) \odot \nabla f(w_k) \\
w_{k+1} = w_k - \alpha \nabla f(w_k) \oslash \sqrt{v_{k+1} + \epsilon}
$$

Note que [^134]
$$
v_{k+1} = (1 - \gamma_1) \sum_{j=0}^{k} \gamma_1^j \nabla f(w_{k-j}) \odot \nabla f(w_{k-j})
$$

de modo que, ao contrário do AdaGrad (10.5.2) [^24], a influência do gradiente $\nabla f(w_{k-j})$ no peso $v_{k+1}$ decai exponencialmente em $j$ [^134].

**Destaque:** O RMSProp utiliza uma média móvel exponencial dos quadrados dos gradientes para ajustar as learning rates, o que permite que o algoritmo se adapte a diferentes escalas de gradientes ao longo do tempo, evitando o problema de *vanishing gradients* [^134].

#### Adam
O algoritmo **Adam** (Adaptive Moment Estimation) [^135] combina taxas de aprendizado adaptativas baseadas em médias exponencialmente ponderadas, como no RMSProp, com o *momentum* de heavy ball [^135]. Ao contrário do AdaGrad e do RMSProp, ele usa um valor $\beta_1 > 0$ [^135]. Mais precisamente, o Adam corresponde a (10.5.1) [^24] com $\beta_2 = 1 - \beta_1 \in (0,1), \gamma_2 = 1 - \gamma_1 \in (0,1)$ e $\alpha_k = \alpha \frac{\sqrt{1 - \gamma_1^{k+1}}}{1 - \beta_1^{k+1}}$ para todo $k \in \mathbb{N}_0$, para algum $\alpha > 0$ [^135]. Os valores padrão para os parâmetros restantes recomendados em [114] são $\epsilon = 10^{-8}, \alpha = 0.001, \beta_1 = 0.9$ e $\gamma_1 = 0.999$ [^135]. A atualização pode ser formulada como [^135]:

$$
m_{k+1} = \beta_1 m_k + (1 - \beta_1) \nabla f(w_k), \quad \hat{m}_{k+1} = \frac{m_{k+1}}{1 - \beta_1^{k+1}} \\
v_{k+1} = \gamma_1 v_k + (1 - \gamma_1) \nabla f(w_k) \odot \nabla f(w_k), \quad \hat{v}_{k+1} = \frac{v_{k+1}}{1 - \gamma_1^{k+1}} \\
w_{k+1} = w_k - \alpha \hat{m}_{k+1} \oslash \sqrt{\hat{v}_{k+1} + \epsilon}
$$

Note que $m_{k+1}$ é igual a [^135]
$$
m_{k+1} = (1 - \beta_1) \sum_{j=0}^{k} \beta_1^j \nabla f(w_{k-j})
$$

e, portanto, corresponde ao *momentum* de heavy ball com parâmetro de momentum $\beta = \beta_1$, veja (10.4.6) [^128]. As versões normalizadas $\hat{m}_{k+1}$ e $\hat{v}_{k+1}$ são introduzidas para contabilizar o bias em direção a $0$, decorrente da inicialização $m_0 = 0$ [^135].

**Destaque:** Adam combina os benefícios do RMSProp (taxas de aprendizado adaptativas) com o *momentum* (aceleração), resultando em um algoritmo robusto e eficiente para uma ampla gama de problemas de otimização em deep learning [^135].

### Conclusão

Os métodos adaptativos de taxa de aprendizagem, como AdaGrad, RMSProp e Adam, oferecem vantagens significativas em relação ao gradient descent tradicional, ajustando dinamicamente as learning rates durante a otimização [^24]. AdaGrad adapta as learning rates individualmente para cada parâmetro, RMSProp utiliza médias móveis exponenciais dos quadrados dos gradientes, e Adam combina ambos com o *momentum* [^135]. A escolha do método mais adequado depende do problema específico e pode exigir experimentação e ajuste dos hiperparâmetros [^135].

### Referências
[^110]: Capítulo 10, página 110
[^111]: Capítulo 10, página 111
[^128]: Capítulo 10, página 128
[^133]: Capítulo 10, página 133
[^134]: Capítulo 10, página 134
[^135]: Capítulo 10, página 135
[^24]: Capítulo 10, página 133

<!-- END -->