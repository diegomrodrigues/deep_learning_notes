## RMSProp: Ajuste Adaptativo da Taxa de Aprendizagem via Médias Exponenciais

### Introdução
O método **RMSProp** (Root Mean Square Propagation) é um algoritmo de otimização que ajusta as taxas de aprendizagem adaptativamente, com o objetivo de melhorar a convergência em problemas de otimização de aprendizado profundo. Ele se baseia na ideia de utilizar uma média móvel exponencial dos gradientes passados para escalar a taxa de aprendizagem, abordando as limitações do AdaGrad, que pode sofrer de taxas de aprendizagem excessivamente pequenas ao longo do tempo [^1].

### Conceitos Fundamentais
O RMSProp, como mencionado, busca mitigar o problema da rápida diminuição das taxas de aprendizagem observada no AdaGrad [^1]. No AdaGrad, a taxa de aprendizagem é dividida pela raiz quadrada da soma acumulada dos quadrados dos gradientes passados, o que pode levar a uma diminuição drástica e prematura da taxa de aprendizagem, especialmente em cenários onde os gradientes iniciais são grandes.

O RMSProp suaviza esse efeito, utilizando uma média ponderada exponencial dos quadrados dos gradientes passados [^1]. A atualização do RMSProp pode ser expressa pelas seguintes equações:

$$
V_{k+1} = \gamma_1 V_k + (1 - \gamma_1) \nabla f(w_k) \odot \nabla f(w_k)
$$

$$
W_{k+1} = W_k - \alpha \nabla f(w_k) \oslash \sqrt{V_{k+1} + \epsilon}
$$

Onde:
*   $V_{k+1}$ representa a média móvel exponencial dos quadrados dos gradientes no passo *k+1*.
*   $\gamma_1$ é um hiperparâmetro que controla a taxa de decaimento da média móvel exponencial, tipicamente um valor próximo de 1 (e.g., 0.9) [^1].
*   $\nabla f(w_k)$ é o gradiente da função objetivo *f* no ponto *wk*.
*   $\odot$ denota a multiplicação elemento a elemento (Hadamard product).
*   $W_{k+1}$ é o vetor de pesos atualizado.
*   $\alpha$ é a taxa de aprendizagem global.
*   $\oslash$ denota a divisão elemento a elemento.
*   $\epsilon$ é um termo de suavização para evitar a divisão por zero [^1].

No contexto da equação geral (10.5.1) presente no contexto, os parâmetros do RMSProp correspondem a: $\beta_1 = 0$, $\beta_2 = 1$, $\gamma_2 = 1 - \gamma_1 \in (0, 1)$, e $\alpha_k = \alpha$ [^1].

A principal diferença entre o RMSProp e o AdaGrad reside na forma como os gradientes passados são agregados. Enquanto o AdaGrad acumula todos os gradientes passados, o RMSProp usa uma média móvel exponencial, dando mais peso aos gradientes mais recentes. Isso permite que o RMSProp se adapte melhor a mudanças na paisagem da função objetivo e evite o problema da diminuição excessiva da taxa de aprendizagem.

A influência do gradiente $\nabla f(w_{k-j})$ no peso $V_{k+1}$ decai exponencialmente em *j* [^1]. Isso significa que os gradientes mais recentes têm um impacto maior na atualização dos pesos do que os gradientes mais antigos.

### Conclusão
O RMSProp representa uma melhoria significativa em relação ao AdaGrad, ao introduzir uma média móvel exponencial dos gradientes passados. Essa abordagem permite um ajuste mais dinâmico das taxas de aprendizagem, resultando em uma convergência mais rápida e estável, especialmente em problemas de aprendizado profundo complexos. Ao evitar o acúmulo excessivo de gradientes passados, o RMSProp consegue manter uma taxa de aprendizagem razoável ao longo do tempo, facilitando a exploração da paisagem da função objetivo e a identificação de soluções ótimas.

### Referências
[^1]: Chapter 10 of the provided text.
<!-- END -->