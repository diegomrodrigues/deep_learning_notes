## Gradient Descent: Uma Abordagem Iterativa para Otimização em Redes Neurais

### Introdução
O treinamento de redes neurais envolve a minimização de uma função objetivo, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, que representa o erro entre as previsões da rede e os valores reais dos dados [^1]. O algoritmo de **Gradient Descent** (GD) é uma técnica iterativa fundamental para encontrar (aproximadamente) um minimizador $w^* \in \mathbb{R}^n$ tal que $f(w^*) \leq f(w)$ para todo $w \in \mathbb{R}^n$ [^1]. Este capítulo detalha o funcionamento do Gradient Descent, suas variantes e as condições para sua convergência, com foco em aplicações no treinamento de redes neurais.

### Conceitos Fundamentais

O Gradient Descent é um algoritmo iterativo que se inicia com uma estimativa inicial $w_0 \in \mathbb{R}^n$ e atualiza sequencialmente os pesos movendo-se na direção do declive mais acentuado da função objetivo [^1]. A regra de atualização é dada por:

$$w_{k+1} := w_k - h_k \nabla f(w_k),$$

onde $h_k > 0$ é o tamanho do passo ou taxa de aprendizado [^1]. Essa regra pode ser interpretada como uma discretização de Euler do fluxo gradiente:

$$w'(t) = -\nabla f(w(t)), \quad t \in [0, \infty),$$

que descreve o movimento de uma partícula $w(t)$ com velocidade no tempo $t > 0$ igual ao gradiente negativo $-\nabla f(w(t))$, garantindo que o valor da função objetivo diminua ao longo de seu percurso [^1].

**Interpretação Geométrica:**

A equação (10.1.1) [^1],

$$f(w_k + v) = f(w_k) + v^T \nabla f(w_k) + O(||v||^2) \quad \text{para } ||v||^2 \rightarrow 0,$$

mostra que a mudança em $f$ ao redor de $w_k$ é localmente descrita pelo gradiente $\nabla f(w_k)$. Para pequenos $v$, a contribuição do termo de segunda ordem é negligenciável, e a direção $v$ ao longo da qual a diminuição do risco é maximizada equivale ao gradiente negativo $-\nabla f(w_k)$. Assim, $-\nabla f(w_k)$ é também chamada de direção de maior declive [^1].

**Escolha da Taxa de Aprendizado ($h_k$):**

A escolha apropriada da taxa de aprendizado é crucial para o desempenho do Gradient Descent [^1]. A taxa de aprendizado deve ser suficientemente pequena para que o termo de segunda ordem em (10.1.1) não domine, garantindo que a atualização (10.1.2) [^1],

$$w_{k+1} := w_k - h_k \nabla f(w_k),$$

diminua a função objetivo [^1]. No entanto, $h_k$ também deve ser grande o suficiente para garantir uma diminuição significativa da função objetivo, facilitando uma convergência mais rápida [^1].

Uma taxa de aprendizado muito alta pode ultrapassar o mínimo, enquanto uma taxa muito baixa resulta em uma convergência lenta [^1]. Estratégias comuns incluem taxas de aprendizado constantes ($h_k = h$ para todo $k \in \mathbb{N}_0$), esquemas de taxa de aprendizado decrescentes ($h_k \searrow 0$ quando $k \rightarrow \infty$) e métodos adaptativos [^1]. Métodos adaptativos ajustam dinamicamente $h_k$ com base nos valores de $f(w_j)$ ou $\nabla f(w_j)$ para $j \leq k$ [^1].

**L-Suavidade:**

Uma suposição chave para analisar a convergência do Gradient Descent é a continuidade de Lipschitz do gradiente $\nabla f$ [^1].

**Definição 10.2:** A função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ é chamada de *L-suave* se $f \in C^1(\mathbb{R}^n)$ e

$$||\nabla f(w) - \nabla f(v)|| \leq L||w - v|| \quad \text{para todo } w, v \in \mathbb{R}^n,$$

onde $L > 0$ [^1].

Para $f$ *L-suave*, o seguinte lema fornece um limite superior quadrático para $f$ [^1]:

**Lemma 10.3:** Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ *L-suave*. Então,

$$f(v) \leq f(w) + \langle \nabla f(w), v - w \rangle + \frac{L}{2} ||v - w||^2 \quad \text{para todo } w, v \in \mathbb{R}^n.$$

**Prova:** Para todo $w, v \in \mathbb{R}^n$,

$$f(v) = f(w) + \int_0^1 \langle \nabla f(w + t(v - w)), v - w \rangle dt$$
$$= f(w) + \langle \nabla f(w), v - w \rangle + \int_0^1 \langle \nabla f(w + t(v - w)) - \nabla f(w), v - w \rangle dt.$$

Assim,

$$f(v) - f(w) - \langle \nabla f(w), v - w \rangle \leq \int_0^1 ||\nabla f(w + t(v - w)) - \nabla f(w)|| ||v - w|| dt$$
$$\leq \int_0^1 L||t(v - w)|| ||v - w|| dt = \frac{L}{2} ||v - w||^2.$$

$\blacksquare$

**Convergência com Taxa de Aprendizado Constante:**

O seguinte lema demonstra uma propriedade de decaimento para as iteradas do Gradient Descent sob a condição de *L-suavidade* [^1].

**Lemma 10.5:** Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ *L-suave*. Seja $(h_k)_{k=1}^\infty$ uma sequência de números positivos e $(w_k)_{k=0}^\infty \subset \mathbb{R}^n$ definida por (10.1.2) [^1]. Então, para todo $k \in \mathbb{N}$,

$$f(w_{k+1}) \leq f(w_k) - \left(h_k - \frac{Lh_k^2}{2}\right) ||\nabla f(w_k)||^2.$$

**Prova:** Utilizando o Lemma 10.3 com $v = w_{k+1}$ e $w = w_k$, obtemos

$$f(w_{k+1}) \leq f(w_k) + \langle \nabla f(w_k), w_{k+1} - w_k \rangle + \frac{L}{2} ||w_{k+1} - w_k||^2$$
$$= f(w_k) + \langle \nabla f(w_k), -h_k \nabla f(w_k) \rangle + \frac{L}{2} ||-h_k \nabla f(w_k)||^2$$
$$= f(w_k) - h_k ||\nabla f(w_k)||^2 + \frac{Lh_k^2}{2} ||\nabla f(w_k)||^2$$
$$= f(w_k) - \left(h_k - \frac{Lh_k^2}{2}\right) ||\nabla f(w_k)||^2.$$

$\blacksquare$

**Convexidade:**

A convexidade da função objetivo desempenha um papel crucial na garantia da convergência do Gradient Descent [^1].

**Definição 10.8:** Uma função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ é chamada de *convexa* se, para todo $w, v \in \mathbb{R}^n$ e $\lambda \in [0, 1]$,

$$f(\lambda w + (1 - \lambda)v) \leq \lambda f(w) + (1 - \lambda)f(v).$$

Se $f \in C^1(\mathbb{R}^n)$, então $f$ é convexa se e somente se

$$f(w) + \langle \nabla f(w), v - w \rangle \leq f(v) \quad \text{para todo } w, v \in \mathbb{R}^n.$$

**Lema 10.9:** Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ *L-suave* e convexa. Seja $h_k \in (0, 2/L)$ para todo $k \in \mathbb{N}_0$, e seja $(w_k)_{k=0}^\infty \subset \mathbb{R}^n$ definido por (10.1.2) [^1]. Suponha que $w^*$ seja um minimizador de $f$. Então, para todo $k \in \mathbb{N}_0$,

$$||w_{k+1} - w^*||^2 \leq ||w_k - w^*||^2 - h_k \left(2 - \frac{h_k}{L}\right) ||\nabla f(w_k)||^2.$$

**Convexidade Forte:**

Para obter taxas de convergência mais rápidas e garantir a existência de minimizadores únicos, introduzimos a noção de convexidade forte [^1].

**Definição 10.13:** Uma função $f \in C^1(\mathbb{R}^n)$ é chamada de *$\mu$-fortemente convexa* se, para todo $w, v \in \mathbb{R}^n$,

$$f(v) \geq f(w) + \langle \nabla f(w), v - w \rangle + \frac{\mu}{2} ||v - w||^2,$$

onde $\mu > 0$ [^1].

**Teorema 10.14:** Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ *L-suave* e $\mu$-fortemente convexa. Seja $h_k = h \in (0, 1/L]$ para todo $k \in \mathbb{N}_0$, e seja $(w_k)_{k=0}^\infty \subset \mathbb{R}^n$ definido por (10.1.2) [^1], e seja $w^*$ o minimizador único de $f$. Então, $f(w_k) \rightarrow f(w^*)$ e $w_k \rightarrow w^*$ convergem linearmente quando $k \rightarrow \infty$. Para a escolha específica $h = 1/L$,

$$||w_k - w^*||^2 \leq \left(1 - \frac{\mu}{L}\right)^k ||w_0 - w^*||^2,$$
$$f(w_k) - f(w^*) \leq \frac{L}{2} \left(1 - \frac{\mu}{L}\right)^k ||w_0 - w^*||^2.$$

### Conclusão

O Gradient Descent é um algoritmo fundamental para o treinamento de redes neurais, permitindo a otimização iterativa dos pesos e biases da rede para minimizar a função objetivo [^1]. A escolha adequada da taxa de aprendizado, juntamente com propriedades como a *L-suavidade*, convexidade e convexidade forte da função objetivo, desempenha um papel crucial na garantia da convergência e eficiência do algoritmo [^1]. As variantes do Gradient Descent, como o Stochastic Gradient Descent (SGD) e os métodos adaptativos, oferecem melhorias adicionais em termos de custo computacional e taxa de convergência [^1].

### Referências
[^1]: Chapter 10, Training of neural networks, (OCR text provided).

<!-- END -->