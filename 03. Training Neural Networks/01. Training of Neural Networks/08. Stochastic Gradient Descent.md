## Stochastic Gradient Descent para Treinamento de Redes Neurais

### Introdução
Este capítulo aprofunda o conceito de **Stochastic Gradient Descent (SGD)** no contexto do treinamento de redes neurais, expandindo sobre as bases estabelecidas em seções anteriores [^1]. O SGD é uma ferramenta essencial para lidar com a complexidade computacional inerente ao treinamento de modelos profundos, e sua compreensão detalhada é fundamental para qualquer profissional da área.

### Conceitos Fundamentais
O **SGD** é um algoritmo iterativo de otimização que busca aproximar o gradiente da função objetivo utilizando um subconjunto aleatório dos dados [^1]. Em vez de calcular o gradiente sobre todo o conjunto de dados a cada iteração, o SGD utiliza uma amostra aleatória, reduzindo drasticamente o custo computacional. A regra de atualização dos pesos é dada por:

$$
w_{k+1} := w_k - h_k G_k,
$$

onde $G_k$ é um estimador não viesado do gradiente $\nabla f(w_k)$ tal que $E[G_k|w_k] = \nabla f(w_k)$, e $h_k > 0$ é o tamanho do passo (step size) ou taxa de aprendizado (learning rate) [^1].

No contexto da minimização do risco empírico, o gradiente $\nabla f(w)$ é frequentemente substituído por um estimador $G$ para diminuir a complexidade computacional [^1]. Este estimador é calculado como:

$$
G := \frac{1}{m_b} \sum_{j \in J} (\phi(x_j, w) - y_j) \nabla_w \phi(x_j, w),
$$

onde $J$ é um subconjunto aleatório de $\{1, ..., m\}$ com cardinalidade $m_b$, $\phi(x_j, w)$ é a saída da rede neural para a entrada $x_j$ com pesos $w$, e $y_j$ é o rótulo correspondente [^1].

As declarações de convergência para o SGD são *estocásticas*, focando na convergência em expectativa [^1]. Sob certas condições, como *L-smoothness*, $\mu$-forte convexidade e um limite uniforme na variância do estimador, a distância quadrática esperada ao minimizador e a diferença esperada entre o valor da função e o mínimo decaem a uma taxa de $O(k^{-1})$ [^1].

O texto [^119] menciona que a ideia de SGD remonta a Robbins e Monro [189], onde o gradiente $\nabla f(w_k)$ em (10.1.2) é substituído por uma variável aleatória $G_k$. Esta variável é interpretada como uma aproximação de $\nabla f(w_k)$, sendo $G_k$ um estimador não viesado, ou seja, $E[G_k|w_k] = \nabla f(w_k)$ [^120].

Após escolher um valor inicial $w_0 \in \mathbb{R}^n$, a regra de atualização se torna [^120]:
$$w_{k+1} := w_k - h_k G_k$$
onde $h_k > 0$ denota o tamanho do passo, dependendo de $k$ [^120]. A iteração cria uma Cadeia de Markov $(w_0, w_1, ...)$, onde o estado de $w_k$ depende apenas de $w_{k-1}$ [^120].

A principal razão para substituir o gradiente real por um estimador não é melhorar a precisão ou a taxa de convergência, mas sim diminuir o custo computacional e os requisitos de armazenamento do algoritmo [^120]. A suposição subjacente é que $G_{k-1}$ pode ser computado a uma fração do custo necessário para a computação de $\nabla f(w_{k-1})$ [^120].

**Exemplo 10.18 (Minimização de Risco Empírico)** [^120]: Suponha que temos dados $S := \{(x_j, y_j)\}_{j=1}^m$, onde $y_j \in \mathbb{R}$ é o rótulo correspondente ao ponto de dados $x_j \in \mathbb{R}^d$. Usando a perda quadrática, deseja-se ajustar uma rede neural $\Phi(\cdot, w) : \mathbb{R}^d \rightarrow \mathbb{R}$ dependendo de parâmetros (pesos e vieses) $w \in \mathbb{R}^n$, de forma que o risco empírico seja minimizado [^120]:
$$f(w) := R_S(w) = \frac{1}{2m} \sum_{j=1}^m (\Phi(x_j, w) - y_j)^2$$
Realizar um passo de gradiente descendente requer a computação de [^120]:
$$\nabla f(w) = \frac{1}{m} \sum_{j=1}^m (\Phi(x_j, w) - y_j) \nabla_w \Phi(x_j, w)$$
A computação de $m$ gradientes da rede neural $\Phi$ pode ser inviável para $m$ grande [^120]. Para diminuir a complexidade computacional, o gradiente completo é substituído por [^120]:
$$G := (\Phi(x_j, w) - y_j) \nabla_w \Phi(x_j, w)$$
onde $j \sim uniform(1, ..., m)$ é uma variável aleatória com distribuição uniforme no conjunto discreto $\{1, ..., m\}$ [^120]. Então:
$$E[G] = \frac{1}{m} \sum_{j=1}^m (\Phi(x_j, w) - y_j) \nabla_w \Phi(x_j, w) = \nabla f(w)$$
A avaliação de $G$ requer apenas a computação de um único gradiente da rede neural [^120].

Mais geralmente, pode-se escolher um tamanho de mini-lote $m_b$ (onde $m_b \ll m$) e deixar $G = \frac{1}{m_b} \sum_{j \in J} (\Phi(x_j, w) - y_j) \nabla_w \Phi(x_j, w)$, onde $J$ é um subconjunto aleatório de $\{1, ..., m\}$ de cardinalidade $m_b$ [^120].

**Remark 10.19** [^121]: Uma variante comum na prática é definir $m_{bk} = m$ para $m_b, k, m \in \mathbb{N}$, ou seja, o número de pontos de dados $m$ é um múltiplo $k$-fold do tamanho do mini-lote $m_b$. Em cada época, uma partição aleatória $\bigcup_{i=1}^k J_i = \{1, ..., m\}$ é determinada. Para cada $i = 1, ..., k$, os pesos são atualizados com a estimativa do gradiente:
$$\frac{1}{m_b} \sum_{j \in J_i} (\Phi(x_j, w) - y_j) \nabla_w \Phi(x_j, w)$$
Em uma época (que corresponde a $k$ atualizações dos pesos da rede neural), o algoritmo varre todo o conjunto de dados [^121].

#### Ajuste da Taxa de Aprendizagem
A escolha adequada da taxa de aprendizado ($h_k$) é crucial para o sucesso do SGD. Uma taxa muito alta pode levar a *overshooting* do mínimo, enquanto uma taxa muito baixa pode resultar em convergência lenta [^111]. Estratégias comuns incluem taxas de aprendizado constantes ($h_k = h$ para todo $k$), *learning rate schedules* (taxas decrescentes $h_k \searrow 0$ quando $k \rightarrow \infty$) e métodos adaptativos [^111]. Métodos adaptativos ajustam dinamicamente $h_k$ baseados nos valores de $f(w_j)$ ou $\nabla f(w_j)$ para $j \leq k$ [^111].

#### L-smoothness
Uma suposição chave para analisar a convergência de (10.1.2) é a continuidade de Lipschitz de $\nabla f$ [^112].

**Definition 10.2** [^112]: Seja $n \in \mathbb{N}$ e $L > 0$. A função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ é dita *L-smooth* se $f \in C^1(\mathbb{R}^n)$ e:
$$||\nabla f(w) - \nabla f(v)|| \leq L||w - v||$$
para todo $w, v \in \mathbb{R}^n$.

#### Stochasticidade e Convergência
Devido à natureza estocástica do SGD, as declarações de convergência são geralmente probabilísticas [^122]. Em vez de garantir a convergência para um ponto específico, o SGD converge em expectativa ou com alta probabilidade [^122].

### Conclusão
O SGD é uma ferramenta poderosa para o treinamento de redes neurais, permitindo lidar com grandes conjuntos de dados e modelos complexos [^1]. No entanto, sua aplicação bem-sucedida requer uma compreensão cuidadosa dos seus parâmetros, especialmente a taxa de aprendizado, e das condições de convergência [^111]. Métodos adaptativos e *learning rate schedules* são frequentemente utilizados para otimizar o processo de treinamento e garantir a convergência [^111].

### Referências
[^1]: Capítulo 10, Training of neural networks.
[^111]: Page 111, Training of neural networks.
[^112]: Page 112, Training of neural networks.
[^119]: Page 119, Training of neural networks.
[^120]: Page 120, Training of neural networks.
[^121]: Page 121, Training of neural networks.
[^122]: Page 122, Training of neural networks.
<!-- END -->