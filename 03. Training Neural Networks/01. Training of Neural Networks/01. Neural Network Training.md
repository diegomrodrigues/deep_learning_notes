## O Treinamento de Redes Neurais via Otimização por Gradiente

### Introdução
O treinamento de redes neurais é fundamentalmente um problema de otimização [^1]. Como vimos anteriormente, o objetivo é minimizar uma função objetivo $f: \mathbb{R}^n \rightarrow \mathbb{R}$ que representa o erro entre as previsões da rede e os dados reais [^1]. Esta função $f$ depende dos pesos e biases da rede, que são coletados em um vetor $w \in \mathbb{R}^n$ [^1]. O objetivo é encontrar um minimizador $w^* \in \mathbb{R}^n$ tal que $f(w^*) \leq f(w)$ para todo $w \in \mathbb{R}^n$ [^1]. Este capítulo detalha como algoritmos de otimização iterativos, como o método do gradiente descendente e suas variantes, são utilizados para alcançar esse objetivo [^1].

### Conceitos Fundamentais

#### Gradiente Descendente
O método do **gradiente descendente** é um algoritmo iterativo que busca o mínimo de uma função movendo-se na direção oposta ao gradiente [^1]. A ideia geral é iniciar com um ponto inicial $w_0 \in \mathbb{R}^n$ e, em seguida, aplicar atualizações sequenciais movendo-se na direção de maior declive da função objetivo [^1]. Assumindo que $f \in C^2(\mathbb{R}^n)$, a *k*-ésima iteração é denotada por $w_k$ [^1]. A expansão de Taylor de primeira ordem de $f$ em torno de $w_k$ é dada por:

$$
f(w_k + v) = f(w_k) + v^T\nabla f(w_k) + O(||v||^2) \quad \text{para } ||v|| \rightarrow 0 \text{ [^1]}
$$

Esta equação mostra que a variação em $f$ em torno de $w_k$ é localmente descrita pelo gradiente $\nabla f(w_k)$ [^1]. Para pequenos $v$, a contribuição do termo de segunda ordem é desprezível, e a direção $v$ ao longo da qual a diminuição do risco é maximizada é igual ao gradiente negativo $-\nabla f(w_k)$ [^1]. Assim, $-\nabla f(w_k)$ também é chamado de **direção de maior declive** [^1]. Isso leva a uma atualização da forma:

$$
w_{k+1} := w_k - h_k \nabla f(w_k) \text{ [^1]}
$$

onde $h_k > 0$ é referido como o **tamanho do passo** ou **taxa de aprendizado** [^1]. Este algoritmo iterativo é conhecido como gradiente descendente [^1].

A escolha do tamanho do passo $h_k$ é crucial [^1]. Ele precisa ser suficientemente pequeno para que o termo de segunda ordem na expansão de Taylor não domine, garantindo que a atualização diminua o valor da função objetivo [^1]. Por outro lado, $h_k$ deve ser grande o suficiente para garantir uma diminuição significativa da função objetivo, facilitando uma convergência mais rápida [^1]. Uma taxa de aprendizado muito alta pode ultrapassar o mínimo, enquanto uma taxa muito baixa resulta em uma convergência lenta [^1].

Estratégias comuns incluem taxas de aprendizado constantes ($h_k = h$ para todo $k \in \mathbb{N}_0$), programações de taxa de aprendizado, como taxas de aprendizado decrescentes ($h_k \searrow 0$ quando $k \rightarrow \infty$), e métodos adaptativos [^1]. Nos métodos adaptativos, o algoritmo ajusta dinamicamente $h_k$ com base nos valores de $f(w_j)$ ou $\nabla f(w_j)$ para $j \leq k$ [^1].

É instrutivo interpretar a atualização do gradiente descendente como uma discretização de Euler do "fluxo do gradiente":

$$
w(0) = w_0, \quad w'(t) = -\nabla f(w(t)) \quad \text{para } t \in [0, \infty) \text{ [^1]}
$$

Esta ODE descreve o movimento de uma partícula $w(t)$, cuja velocidade no tempo $t > 0$ é igual a $-\nabla f(w(t))$, o vetor de maior declive [^1]. Note que:

$$
\frac{d}{dt}f(w(t)) = \langle \nabla f(w(t)), w'(t) \rangle = -||\nabla f(w(t))||^2 \text{ [^1]}
$$

e, portanto, a dinâmica necessariamente diminui o valor da função objetivo ao longo de seu caminho, desde que $\nabla f(w(t)) \neq 0$ [^1].

#### L-Smoothness

Uma suposição chave para analisar a convergência do gradiente descendente é a continuidade de Lipschitz do gradiente $\nabla f$ [^1].

**Definição 10.2.** Seja $n \in \mathbb{N}$ e $L > 0$. A função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ é chamada de *L-smooth* se $f \in C^1(\mathbb{R}^n)$ e

$$
||\nabla f(w) - \nabla f(v)|| \leq L||w - v|| \quad \text{para todo } w, v \in \mathbb{R}^n \text{ [^1]}
$$

Para $w$ fixo, a *L-smoothness* implica o limite de crescimento linear

$$
||\nabla f(w + v)|| \leq ||\nabla f(w)|| + L||v|| \text{ [^1]}
$$

Integrando o gradiente ao longo de linhas em $\mathbb{R}^n$, mostra-se que $f$ é limitado superiormente por uma função quadrática tocando o gráfico de $f$ em $w$ [^1].

**Lemma 10.3.** Seja $n \in \mathbb{N}$ e $L > 0$. Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ ser *L-smooth*. Então

$$
f(v) \leq f(w) + \langle \nabla f(w), v - w \rangle + \frac{L}{2}||v - w||^2 \quad \text{para todo } w, v \in \mathbb{R}^n \text{ [^1]}
$$

*Prova.* Temos para todo $w, v \in \mathbb{R}^n$

$$
f(v) = f(w) + \int_0^1 \langle \nabla f(w + t(v - w)), v - w \rangle dt \text{ [^1]}
$$
$$
= f(w) + \langle \nabla f(w), v - w \rangle + \int_0^1 \langle \nabla f(w + t(v - w)) - \nabla f(w), v - w \rangle dt \text{ [^1]}
$$

Assim,

$$
f(v) - f(w) - \langle \nabla f(w), v - w \rangle \leq \int_0^1 L||t(v - w)||||v - w|| dt = \frac{L}{2}||v - w||^2 \text{ [^1]}
$$

o que mostra (10.1.4) [^1]. $\blacksquare$

O argumento na prova do Lemma 10.3 também fornece o limite inferior

$$
f(v) \geq f(w) + \langle \nabla f(w), v - w \rangle - \frac{L}{2}||w - v||^2 \quad \text{para todo } w, v \in \mathbb{R}^n \text{ [^1]}
$$

O lema anterior permite mostrar uma propriedade de decaimento para as iterações do gradiente descendente [^1]. Especificamente, os valores de $f$ necessariamente diminuem em cada iteração, desde que o tamanho do passo $h_k$ seja pequeno o suficiente e $\nabla f(w_k) \neq 0$ [^1].

**Lemma 10.5.** Seja $n \in \mathbb{N}$ e $L > 0$. Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ ser *L-smooth*. Além disso, seja $(h_k)_{k=1}^\infty$ números positivos e seja $(w_k)_{k=0}^\infty \subset \mathbb{R}^n$ definido por (10.1.2) [^1].
Então, para todo $k \in \mathbb{N}$

$$
f(w_{k+1}) \leq f(w_k) - \left(h_k - \frac{Lh_k^2}{2}\right) ||\nabla f(w_k)||^2 \text{ [^1]}
$$

*Prova.* O Lemma 10.3 com $v = w_{k+1}$ e $w = w_k$ fornece

$$
f(w_{k+1}) \leq f(w_k) + \langle \nabla f(w_k), -h_k \nabla f(w_k) \rangle + \frac{L}{2} ||h_k \nabla f(w_k)||^2 \text{ [^1]}
$$

que corresponde a (10.1.6) [^1]. $\blacksquare$

O lado direito em (10.1.6) é minimizado para o tamanho do passo $h_k = 1/L$, caso em que (10.1.6) lê

$$
f(w_{k+1}) \leq f(w_k) - \frac{1}{2L} ||\nabla f(w_k)||^2 \text{ [^1]}
$$

#### Convexity

Embora a *L-smoothness* acarrete algumas propriedades interessantes do gradiente descendente, ela não tem implicações diretas sobre a existência ou unicidade de minimizadores [^1]. Para mostrar a convergência de $f(w_k)$ em direção a $\inf_w f(w)$ para $k \rightarrow \infty$ (assumindo que este mínimo exista), é comum assumir que $f$ é uma **função convexa** [^1].

**Definição 10.8.** Seja $n \in \mathbb{N}$. Uma função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ é chamada de **convexa** se e somente se

$$
f(\lambda w + (1 - \lambda)v) \leq \lambda f(w) + (1 - \lambda)f(v) \text{ [^1]}
$$

para todo $w, v \in \mathbb{R}^n$, $\lambda \in (0, 1)$ [^1].

Se $f \in C^1(\mathbb{R}^n)$, então $f$ é convexa se e somente se

$$
f(w) + \langle \nabla f(w), v - w \rangle \leq f(v) \text{ [^1]}
$$

para todo $w, v \in \mathbb{R}^n$ [^1]. Assim, $f \in C^1(\mathbb{R}^n)$ é convexa se e somente se o gráfico de $f$ está acima de cada uma de suas tangentes [^1].

Para uma função convexa $f$, um minimizador não precisa existir (por exemplo, $f(w) = w$ para $w \in \mathbb{R}$) nem ser único (por exemplo, $f(w) = 0$ para $w \in \mathbb{R}^n$) [^1]. No entanto, se $w^*$ e $v^*$ são dois minimizadores, então toda combinação convexa $\lambda w^* + (1 - \lambda)v^*$, $\lambda \in [0, 1]$, também é um minimizador [^1]. Assim, o conjunto de todos os minimizadores é convexo [^1]. Em particular, uma função objetivo convexa tem zero, um ou infinitamente muitos minimizadores [^1]. Além disso, se $f \in C^1(\mathbb{R}^n)$, então $\nabla f(w) = 0$ implica

$$
f(w) = f(w) + \langle \nabla f(w), v - w \rangle \leq f(v) \text{ [^1]}
$$

para todo $v \in \mathbb{R}^n$ [^1]. Assim, $w$ é um minimizador de $f$ se e somente se $\nabla f(w) = 0$ [^1].

Pelo Lemma 10.5, a pequenez dos tamanhos de passo e a *L-smoothness* são suficientes para mostrar uma propriedade de decaimento para a função objetivo $f$ [^1]. Sob a suposição adicional de convexidade, também se obtém uma propriedade de decaimento para a distância de $w_k$ a qualquer minimizador $w^*$ [^1].

**Lemma 10.9.** Seja $n \in \mathbb{N}$ e $L > 0$. Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ ser *L-smooth* e convexa [^1]. Além disso, seja $h_k \in (0, 2/L)$ para todo $k \in \mathbb{N}_0$, e $(w_k)_{k=0}^\infty \subset \mathbb{R}^n$ definido por (10.1.2) [^1]. Suponha que $w^*$ é um minimizador de $f$ [^1]. Então, para todo $k \in \mathbb{N}_0$

$$
||w_{k+1} - w^*||^2 \leq ||w_k - w^*||^2 - h_k \left(1 - \frac{h_k L}{2}\right) ||\nabla f(w_k)||^2 \text{ [^1]}
$$

#### Strong Convexity

Para obter uma convergência mais rápida e garantir a existência de minimizadores únicos, introduz-se a noção de **strong convexity** [^1]. Como a terminologia sugere, a strong convexity implica a convexidade; especificamente, enquanto a convexidade requer que $f$ seja limitada inferiormente pela linearização em torno de cada ponto, as funções fortemente convexas são limitadas inferiormente pela linearização mais um termo quadrático positivo [^1].

**Definição 10.13.** Seja $n \in \mathbb{N}$ e $\mu > 0$. Uma função $f \in C^1(\mathbb{R}^n)$ é chamada de **μ-strongly convex** se

$$
f(v) \geq f(w) + \langle \nabla f(w), v - w \rangle + \frac{\mu}{2} ||v - w||^2 \text{ [^1]}
$$

para todo $w, v \in \mathbb{R}^n$ [^1].

Note que (10.1.15) é o oposto do limite (10.1.4) implícito pela *L-smoothness* [^1].

Toda função μ-strongly convex tem um minimizador único [^1]. Para ver isso, note primeiro que (10.1.15) implica que $f$ seja limitada inferiormente por uma função quadrática convexa, de modo que exista pelo menos um minimizador $w^*$, e $\nabla f(w^*) = 0$ [^1]. Por (10.1.15), então tem-se $f(v) > f(w^*)$ para todo $v \neq w^*$ [^1].

### Conclusão

Este capítulo apresentou os fundamentos do treinamento de redes neurais como um problema de otimização, com foco no método do gradiente descendente e suas propriedades de convergência sob diferentes condições, como *L-smoothness* e convexidade [^1]. A análise detalhada da *L-smoothness*, convexidade e strong convexity fornece insights sobre o comportamento do gradiente descendente e a escolha apropriada de tamanhos de passo para garantir a convergência [^1]. Os conceitos e resultados aqui apresentados são cruciais para entender e melhorar o desempenho dos algoritmos de treinamento de redes neurais [^1].

### Referências
[^1]: Chapter 10 Training of neural networks.
<!-- END -->