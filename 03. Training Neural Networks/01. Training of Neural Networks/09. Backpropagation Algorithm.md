## Backpropagation: Cálculo Eficiente de Gradientes em Redes Neurais

### Introdução
Este capítulo aprofunda o algoritmo de *backpropagation*, uma ferramenta essencial para o treinamento de redes neurais [^1]. Como vimos anteriormente, o treinamento de redes neurais envolve a minimização de uma função objetivo, geralmente o risco empírico ($R_S$ em (1.2.3) [^1]), que quantifica o erro entre as previsões da rede e os valores reais. O *backpropagation* fornece um meio eficiente de calcular os gradientes dessa função objetivo em relação aos parâmetros da rede (pesos e biases), permitindo o uso de otimizadores baseados em gradientes, como o *gradient descent* [^1].

### Conceitos Fundamentais

O *backpropagation* é um algoritmo que combina uma passagem *forward* para calcular as ativações da rede e uma passagem *backward* para calcular os gradientes [^1]. O algoritmo se baseia na regra da cadeia para calcular as derivadas da função de perda em relação aos pesos e *biases* da rede [^1].

**Passagem Forward:**

A passagem *forward* envolve a propagação da entrada através das camadas da rede neural, calculando as ativações de cada neurônio. Para uma rede neural com $L$ camadas, a passagem *forward* pode ser descrita pelas seguintes equações [^15]:

$$
x^{(1)} := W^{(0)}x + b^{(0)} \quad (10.3.3a)
$$

$$
x^{(l+1)} := W^{(l)}\sigma(x^{(l)}) + b^{(l)} \quad \text{para } l \in \{1, ..., L\} \quad (10.3.3b)
$$

onde:

*   $x$ é a entrada da rede.
*   $W^{(l)}$ são as matrizes de peso da camada $l$.
*   $b^{(l)}$ são os vetores de *bias* da camada $l$.
*   $\sigma$ é a função de ativação, aplicada *component-wise*.
*   $x^{(l)}$ representa as pré-ativações na camada $l$.
*   $x^{(L+1)} = \Phi(x, w)$ representa a saída da rede, onde $w$ são os parâmetros da rede.

As pré-ativações $x^{(l)}$ são por vezes chamadas de ativações [^15].

**Passagem Backward:**

O *backpropagation* introduz variáveis intermediárias $\alpha^{(l)}$, que representam o gradiente da função de perda $\mathcal{L}$ em relação às pré-ativações na camada $l$ [^1]. Essas variáveis são calculadas recursivamente durante a passagem *backward*, começando pela camada de saída e retropropagando até a primeira camada [^1].

$$
\alpha^{(l)} := \nabla_{x^{(l)}} \mathcal{L} \in \mathbb{R}^{d_l} \quad \text{para } l = 1, ..., L+1
$$

O cálculo recursivo de $\alpha^{(l)}$ é dado por [^16]:

$$
\alpha^{(L+1)} = \nabla_{x^{(L+1)}} \mathcal{L}(x^{(L+1)}, y) \quad (10.3.5)
$$

$$
\alpha^{(l)} = \sigma'(x^{(l)}) \odot (W^{(l)})^T \alpha^{(l+1)} \quad \text{para } l = L, ..., 1
$$

onde:

*   $\mathcal{L}(x^{(L+1)}, y)$ é a função de perda, que compara a saída da rede $x^{(L+1)}$ com o valor real $y$.
*   $\sigma'(x^{(l)})$ é a derivada da função de ativação, avaliada nas pré-ativações da camada $l$.
*   $\odot$ denota o produto de Hadamard (produto *component-wise*).

**Cálculo dos Gradientes:**

O *backpropagation* fornece fórmulas explícitas para calcular os gradientes da função de perda em relação aos pesos e *biases* [^1]. Esses gradientes são expressos em termos das variáveis $\alpha^{(l)}$ e das ativações $x^{(l)}$ [^1].

$$
\nabla_{b^{(l)}} \mathcal{L} = \alpha^{(l+1)} \quad \text{para } l = 0, ..., L
$$

$$
\nabla_{W^{(0)}} \mathcal{L} = \alpha^{(1)} x^T
$$

$$
\nabla_{W^{(l)}} \mathcal{L} = \alpha^{(l+1)} \sigma(x^{(l)})^T \quad \text{para } l = 1, ..., L
$$

Essas fórmulas permitem calcular os gradientes de forma eficiente, evitando o cálculo direto das derivadas, que seria computacionalmente inviável para redes neurais profundas.

**Algoritmo Backpropagation:**

O *backpropagation* pode ser resumido nos seguintes passos [^17]:

1.  **Passagem Forward:** Calcular as ativações $x^{(l)}$ para todas as camadas $l = 1, ..., L+1$.
2.  **Passagem Backward:** Calcular as variáveis $\alpha^{(l)}$ para todas as camadas $l = L+1, ..., 1$.
3.  **Cálculo dos Gradientes:** Calcular os gradientes $\nabla_{W^{(l)}} \mathcal{L}$ e $\nabla_{b^{(l)}} \mathcal{L}$ para todas as camadas $l = 0, ..., L$.

### Conclusão

O *backpropagation* é um algoritmo fundamental para o treinamento de redes neurais, permitindo o cálculo eficiente dos gradientes da função de perda em relação aos parâmetros da rede [^1]. Ao combinar uma passagem *forward* para calcular as ativações e uma passagem *backward* para calcular os gradientes, o *backpropagation* possibilita o uso de otimizadores baseados em gradientes, como o *gradient descent*, para ajustar os pesos e *biases* da rede e minimizar o erro de previsão [^1]. Embora a função objetivo associada a redes neurais tipicamente não seja convexa [^17], o *backpropagation* fornece uma ferramenta poderosa para encontrar soluções aproximadas e treinar redes neurais eficazes.

### Referências
[^1]: Página 110, "Training of neural networks".
[^15]: Página 124, "Training of neural networks".
[^16]: Página 125, "Training of neural networks".
[^17]: Página 127, "Training of neural networks".
<!-- END -->