## Convexidade e Otimização em Treinamento de Redes Neurais

### Introdução
A convexidade desempenha um papel crucial na otimização de redes neurais, garantindo que qualquer minimizador local seja também um minimizador global. Este capítulo explora a convexidade no contexto do treinamento de redes neurais, focando em suas propriedades e implicações para os algoritmos de otimização. Como vimos anteriormente, o treinamento de redes neurais envolve a minimização de uma função objetivo [^1], geralmente o risco empírico $R_S$ [^1].

### Conceitos Fundamentais
**Definição de Convexidade:** Uma função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ é **convexa** se, para todos os $w, v \in \mathbb{R}^n$ e $\lambda \in (0, 1)$, a seguinte desigualdade se mantém [^1]:
$$f(\lambda w + (1 - \lambda)v) \leq \lambda f(w) + (1 - \lambda)f(v)$$
Geometricamente, isso significa que qualquer ponto no segmento de reta que conecta dois pontos no gráfico da função está acima ou no gráfico da função [^5].

**Convexidade e Minimizadores:** Uma propriedade fundamental das funções convexas é que qualquer **minimizador local** é também um **minimizador global** [^1]. Isso simplifica significativamente o problema de otimização, pois não precisamos nos preocupar em ficar presos em mínimos locais subótimos.

**Critério de Convexidade para Funções Diferenciáveis:** Se $f \in C^1(\mathbb{R}^n)$, então $f$ é convexa se e somente se [^1]:
$$f(w) + \langle \nabla f(w), v - w \rangle \leq f(v) \quad \text{para todos } w, v \in \mathbb{R}^n$$
Essa desigualdade afirma que o gráfico de $f$ está sempre acima de seus planos tangentes [^5].

**Convexidade e Combinações Convexas de Minimizadores:** Para uma função objetivo convexa $f$, se $w^*$ e $v^*$ são dois minimizadores, então toda combinação convexa $\lambda w^* + (1 - \lambda)v^*$, com $\lambda \in [0, 1]$, também é um minimizador [^1]. Isso implica que o conjunto de todos os minimizadores de uma função convexa é um conjunto convexo [^5].

**Condição de Otimalidade para Funções Convexas Diferenciáveis:** Se $f \in C^1(\mathbb{R}^n)$, então $\nabla f(w) = 0$ implica que $w$ é um minimizador de $f$ [^1]. Em outras palavras, um ponto estacionário de uma função convexa diferenciável é necessariamente um minimizador global [^5].

**Lema 10.8:** Uma função $f: \mathbb{R}^n \rightarrow \mathbb{R}$ é convexa se e somente se [^5]:
$$f(\lambda w + (1 - \lambda)v) \leq \lambda f(w) + (1 - \lambda)f(v), \quad \forall w, v \in \mathbb{R}^n, \lambda \in (0, 1)$$

**Lema 10.9:** Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ uma função L-smooth e convexa. Seja $h_k \in (0, 2/L)$ para todo $k \in \mathbb{N}_0$, e seja $(w_k)_{k=0} \subseteq \mathbb{R}^n$ definido por (10.1.2) [^5]. Suponha que $w^*$ seja um minimizador de $f$. Então, para todo $k \in \mathbb{N}_0$:
$$||w_{k+1} - w^*||^2 \leq ||w_k - w^*||^2 - h_k \left(1 - \frac{h_k L}{2}\right) ||\nabla f(w_k)||^2$$

**Lema 10.10:** Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ uma função L-smooth e convexa. Então [^6]:
$$\frac{1}{L} ||\nabla f(w) - \nabla f(v)||^2 \leq \langle \nabla f(w) - \nabla f(v), w - v \rangle, \quad \forall w, v \in \mathbb{R}^n$$

**Teorema 10.11:** Seja $f: \mathbb{R}^n \rightarrow \mathbb{R}$ uma função L-smooth e convexa. Seja $h_k = h \in (0, 2/L)$ para todo $k \in \mathbb{N}_0$, e seja $(w_k)_{k=0} \subseteq \mathbb{R}^n$ definido por (10.1.2) [^6]. Suponha que $w^*$ seja um minimizador de $f$. Então, $f(w_k) - f(w^*) = O(k^{-1})$ para $k \rightarrow \infty$, e para a escolha específica $h = 1/L$:
$$f(w_k) - f(w^*) \leq \frac{2L}{4 + k} ||w_0 - w^*||^2, \quad \forall k \in \mathbb{N}_0$$

### Conclusão
A convexidade oferece garantias importantes para a otimização de redes neurais, particularmente em relação à existência e unicidade de minimizadores [^5]. Embora as funções objetivo em redes neurais nem sempre sejam convexas [^17], entender os princípios da otimização convexa fornece uma base sólida para o desenvolvimento e análise de algoritmos de treinamento mais eficazes. O estudo da convexidade, juntamente com técnicas como L-smoothness e a PL-inequality, contribui para a criação de métodos de otimização que convergem de forma mais eficiente e estável [^10].

### Referências
[^1]: Chapter 10, Training of neural networks, page 110
[^5]: Chapter 10, Training of neural networks, page 114
[^6]: Chapter 10, Training of neural networks, page 115
[^10]: Chapter 10, Training of neural networks, page 119
[^17]: Chapter 10, Training of neural networks, page 126
<!-- END -->